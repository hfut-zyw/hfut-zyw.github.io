<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第一章 矩阵 | Homepage | 张有为</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="简要介绍Pytorch中张量的基本创建，运算，自动求导等">
<meta property="og:type" content="article">
<meta property="og:title" content="第一章 矩阵">
<meta property="og:url" content="http://example.com/2022/09/03/pytorch/chapter1-tensor/index.html">
<meta property="og:site_name" content="Homepage | 张有为">
<meta property="og:description" content="简要介绍Pytorch中张量的基本创建，运算，自动求导等">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-09-03T12:27:57.000Z">
<meta property="article:modified_time" content="2022-09-03T12:46:19.730Z">
<meta property="article:author" content="宝可梦训练师">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="Homepage | 张有为" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/icon.gif">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>


  
<meta name="generator" content="Hexo 6.2.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/face.png" class="js-avatar show" style="width: 100%;height: 100%;opacity: 1;">
		</a>

		<hgroup>
			<h1 class="header-author"><a href="/">宝可梦训练师</a></h1>
		</hgroup>

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/2022/08/09/me/aboutme">个人简介</a></li>
				        
							<li><a href="/categories/notes">随写</a></li>
				        
							<li><a href="/categories/analysis">实变泛函</a></li>
				        
							<li><a href="/categories/opt">优化笔记</a></li>
				        
							<li><a href="/categories/pytorch">Pytorch</a></li>
				        
							<li><a href="/categories/nndl">nndl案例与实践</a></li>
				        
						</ul>
					</nav>
					<nav class="half-header-menu">
						<a class="hide">Home</a>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
					        
						</div>
						<!-- music -->
						
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/face.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/2022/08/09/me/aboutme">个人简介</a></li>
		        
					<li><a href="/categories/notes">随写</a></li>
		        
					<li><a href="/categories/analysis">实变泛函</a></li>
		        
					<li><a href="/categories/opt">优化笔记</a></li>
		        
					<li><a href="/categories/pytorch">Pytorch</a></li>
		        
					<li><a href="/categories/nndl">nndl案例与实践</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-pytorch/chapter1-tensor" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/09/03/pytorch/chapter1-tensor/" class="article-date">
  	<time datetime="2022-09-03T12:27:57.000Z" itemprop="datePublished">2022-09-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第一章 矩阵
      
          <span class="title-pop-out"></a>
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/pytorch/">pytorch</a>
	</div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>简要介绍Pytorch中张量的基本创建，运算，自动求导等</p>
</blockquote>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="1-矩阵类型"><a href="#1-矩阵类型" class="headerlink" title="1.矩阵类型"></a>1.矩阵类型</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">数据类型</th>
<th style="text-align:center">dytpe</th>
<th style="text-align:center">CPU</th>
<th style="text-align:center">GPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">16位浮点型</td>
<td style="text-align:center">//</td>
<td style="text-align:center">//</td>
<td style="text-align:center">//</td>
</tr>
<tr>
<td style="text-align:center">32位浮点型</td>
<td style="text-align:center">torch.float32或torch.float</td>
<td style="text-align:center">torch.FloatTensor</td>
<td style="text-align:center">torch.cuda.FloatTensor</td>
</tr>
<tr>
<td style="text-align:center">64位浮点型</td>
<td style="text-align:center">torch.float64或torch.double</td>
<td style="text-align:center">torch.DoubleTensor</td>
<td style="text-align:center">torch.cuda.DoubleTensor</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">8位无符号整型</td>
<td style="text-align:center">torch.uint8</td>
<td style="text-align:center">//</td>
<td style="text-align:center">//</td>
</tr>
<tr>
<td style="text-align:center">8位有符号整型</td>
<td style="text-align:center">torch.int8</td>
<td style="text-align:center">//</td>
<td style="text-align:center">//</td>
</tr>
<tr>
<td style="text-align:center">16位有符号整型</td>
<td style="text-align:center">torch.int16</td>
<td style="text-align:center">//</td>
<td style="text-align:center">//</td>
</tr>
<tr>
<td style="text-align:center">32位有符号整型</td>
<td style="text-align:center">torch.int32</td>
<td style="text-align:center">//</td>
<td style="text-align:center">//</td>
</tr>
<tr>
<td style="text-align:center">64位有符号整型</td>
<td style="text-align:center">torch.int64</td>
<td style="text-align:center">//</td>
<td style="text-align:center">//</td>
</tr>
</tbody>
</table>
</div>
<p>Pytorch中定义了8种CPU张量类型和对应的GPU张量类型，相同数据类型的tensor才能做运算</p>
<h3 id="1-1-全局矩阵类型设置"><a href="#1-1-全局矩阵类型设置" class="headerlink" title="1.1 全局矩阵类型设置"></a>1.1 全局矩阵类型设置</h3><p><code>torch.set_default_tensor_type(torch.FloatTensor)</code>  </p>
<h3 id="1-2-数据类型转换"><a href="#1-2-数据类型转换" class="headerlink" title="1.2 数据类型转换"></a>1.2 数据类型转换</h3><p><code>1.Tensor.long() ,  .int() ,  .float() ,  .double()</code>   </p>
<p><code>2.Tensor.to()</code>   </p>
<p><code>3.dtype=&#39;torch.float32&#39;</code>  </p>
<h3 id="1-3-与numpy数据类型转换"><a href="#1-3-与numpy数据类型转换" class="headerlink" title="1.3 与numpy数据类型转换"></a>1.3 与numpy数据类型转换</h3><p><code>Tensor--&gt; Numpy ：Tensor.numpy()</code>  </p>
<p><code>Numpy --&gt; Tensor ：torch.from_numpy()</code>  </p>
<h3 id="1-4-与Python数据类型转换"><a href="#1-4-与Python数据类型转换" class="headerlink" title="1.4 与Python数据类型转换"></a>1.4 与Python数据类型转换</h3><p><code>Tensor --&gt; list：data.tolist()</code>  </p>
<hr>
<h2 id="2-矩阵创建"><a href="#2-矩阵创建" class="headerlink" title="2.矩阵创建"></a>2.矩阵创建</h2><h3 id="2-1-从list-numpy创建"><a href="#2-1-从list-numpy创建" class="headerlink" title="2.1 从list,numpy创建"></a>2.1 从list,numpy创建</h3><p><code>torch.tensor()</code>  </p>
<p><code>torch.from_numpy()</code>  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]],dtype=torch.int32)</span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]],dtype=torch.float32)</span><br><span class="line">z=torch.from_numpy(np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]]))</span><br><span class="line">x,y,z</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[1, 2, 3, 4],
         [2, 3, 4, 5]], dtype=torch.int32),
 tensor([[1., 2., 3., 4.],
         [2., 3., 4., 5.]]),
 tensor([[1, 2, 3, 4],
         [2, 3, 4, 5]], dtype=torch.int32))
</code></pre><h3 id="2-2-从常见函数创建"><a href="#2-2-从常见函数创建" class="headerlink" title="2.2 从常见函数创建"></a>2.2 从常见函数创建</h3><p><code>torch.empty(尺寸) torch.full(尺寸,值)</code>  </p>
<p><code>torch.zeros(尺寸) torch.ones(尺寸) torch.eye(维数)</code>  </p>
<p><code>torch.zeros_like(另一个矩阵) torch.ones_like(另一个矩阵)</code>   </p>
<p>note：尺寸可以是一维的，也可以是多维的，一般用列表框起来  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a1=torch.empty(<span class="number">3</span>)</span><br><span class="line">a2=torch.empty(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">a3=torch.eye(<span class="number">3</span>)   </span><br><span class="line">a4=torch.zeros(<span class="number">3</span>)</span><br><span class="line">a5=torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">a6=torch.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a7=torch.full([<span class="number">2</span>,<span class="number">3</span>],<span class="number">6</span>)</span><br><span class="line">a8=torch.zeros_like(a1)</span><br><span class="line">a1,a2,a3,a4,a5,a6,a7,a8</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([0., 0., 0.]),
 tensor([[-3., -2.],
         [-1.,  1.],
         [ 2.,  0.]]),
 tensor([[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]),
 tensor([0., 0., 0.]),
 tensor([[0., 0., 0.],
         [0., 0., 0.]]),
 tensor([[1., 1., 1.],
         [1., 1., 1.]]),
 tensor([[6, 6, 6],
         [6, 6, 6]]),
 tensor([0., 0., 0.]))
</code></pre><h3 id="2-3-区间线性采样"><a href="#2-3-区间线性采样" class="headerlink" title="2.3 区间线性采样"></a>2.3 区间线性采样</h3><p><code>torch.arange(首，尾，可选步长)</code>  </p>
<p>note：不包括尾巴  </p>
<p><code>torch.linspace(首，尾，数量)</code>  </p>
<p>note：包括尾巴，步长=(尾-首)/(n-1),因为starts+(n-1)step=end  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a=torch.arange(<span class="number">5</span>,<span class="number">8</span>)</span><br><span class="line">b=torch.arange(<span class="number">5</span>,<span class="number">8</span>,<span class="number">2</span>)</span><br><span class="line">c=torch.linspace(<span class="number">5</span>,<span class="number">8</span>,<span class="number">1</span>)</span><br><span class="line">d=torch.linspace(<span class="number">5</span>,<span class="number">8</span>,<span class="number">10</span>)</span><br><span class="line">a,b,c,d</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([5, 6, 7]),
 tensor([5, 7]),
 tensor([5.]),
 tensor([5.0000, 5.3333, 5.6667, 6.0000, 6.3333, 6.6667, 7.0000, 7.3333, 7.6667,
         8.0000]))
</code></pre><h3 id="2-4-一些常见随机矩阵生成"><a href="#2-4-一些常见随机矩阵生成" class="headerlink" title="2.4 一些常见随机矩阵生成"></a>2.4 一些常见随机矩阵生成</h3><ul>
<li>均匀分布$U(0,1)$  </li>
</ul>
<p><code>torch.rand(尺寸)</code>  </p>
<ul>
<li>均匀分布$U(low,high)$  </li>
</ul>
<p><code>torch.randint(low,high,尺寸)</code>   </p>
<ul>
<li>正态分布$N(0,1)$  </li>
</ul>
<p><code>torch.randn(尺寸)</code>  </p>
<ul>
<li>正态分布$N(u,\sigma^2)$   </li>
</ul>
<p><code>torch.normal(均值，方差，尺寸)</code>  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.rand(<span class="number">2</span>,<span class="number">3</span>)  </span><br><span class="line">y=torch.randint(<span class="number">1</span>,<span class="number">9</span>,[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a=torch.randn([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">b=torch.normal(<span class="number">10</span>,<span class="number">3</span>,[<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">x,y,a,b</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[0.9641, 0.8441, 0.5508],
         [0.9394, 0.7539, 0.3244]]),
 tensor([[8, 4, 5],
         [8, 4, 1]]),
 tensor([[ 1.0820, -0.0542,  0.4432,  0.5290],
         [-0.4088, -0.1003, -0.1843, -1.2680],
         [-0.5264,  0.5502,  0.9714,  0.2716]]),
 tensor([[ 7.3727,  6.3348, 12.0752, 12.0296],
         [ 5.6573,  8.8038,  9.2094,  7.3360],
         [10.9025,  7.7881, 13.1615,  3.4490]]))
</code></pre><hr>
<h2 id="3-矩阵切片"><a href="#3-矩阵切片" class="headerlink" title="3 矩阵切片"></a>3 矩阵切片</h2><ul>
<li>逗号   <ul>
<li>逗号前表示行，逗号后表示列        </li>
</ul>
</li>
<li>冒号 <ul>
<li>一个冒号&nbsp;start ：end  &nbsp;&nbsp;&nbsp;  </li>
<li>两个冒号&nbsp;start ：end ：step  &nbsp;&nbsp;&nbsp;  </li>
</ul>
</li>
</ul>
<h3 id="3-1-一维矩阵的切片"><a href="#3-1-一维矩阵的切片" class="headerlink" title="3.1 一维矩阵的切片"></a>3.1 一维矩阵的切片</h3><ul>
<li>同列表切片  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.random.seed()</span><br><span class="line">a=torch.rand(<span class="number">10</span>) </span><br><span class="line">a,a[<span class="number">5</span>],a[<span class="number">0</span>:<span class="number">3</span>],a[:<span class="number">6</span>],a[:-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([0.4498, 0.0418, 0.9517, 0.5108, 0.6561, 0.7067, 0.4083, 0.4138, 0.2637,
         0.3813]),
 tensor(0.7067),
 tensor([0.4498, 0.0418, 0.9517]),
 tensor([0.4498, 0.0418, 0.9517, 0.5108, 0.6561, 0.7067]),
 tensor([0.4498, 0.0418, 0.9517, 0.5108, 0.6561, 0.7067, 0.4083, 0.4138, 0.2637]))
</code></pre><h3 id="3-2-二维矩阵的切片"><a href="#3-2-二维矩阵的切片" class="headerlink" title="3.2 二维矩阵的切片"></a>3.2 二维矩阵的切片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand([<span class="number">6</span>,<span class="number">4</span>])</span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.6021, 0.2580, 0.3276, 0.8740],
        [0.5805, 0.2504, 0.5213, 0.5771],
        [0.7685, 0.2554, 0.2083, 0.2814],
        [0.9462, 0.5307, 0.3409, 0.4292],
        [0.3421, 0.2437, 0.1801, 0.3460],
        [0.6368, 0.6607, 0.4203, 0.9574]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>],a[<span class="number">0</span>:<span class="number">2</span>],a[<span class="number">0</span>:-<span class="number">1</span>:<span class="number">2</span>],a[[<span class="number">0</span>,<span class="number">2</span>]]  <span class="comment">#取指定行</span></span><br></pre></td></tr></table></figure>
<pre><code>(tensor([0.6021, 0.2580, 0.3276, 0.8740]),
 tensor([[0.6021, 0.2580, 0.3276, 0.8740],
         [0.5805, 0.2504, 0.5213, 0.5771]]),
 tensor([[0.6021, 0.2580, 0.3276, 0.8740],
         [0.7685, 0.2554, 0.2083, 0.2814],
         [0.3421, 0.2437, 0.1801, 0.3460]]),
 tensor([[0.6021, 0.2580, 0.3276, 0.8740],
         [0.7685, 0.2554, 0.2083, 0.2814]]))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[:,<span class="number">0</span>]  <span class="comment">#取指定列</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([0.6021, 0.5805, 0.7685, 0.9462, 0.3421, 0.6368])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>:<span class="number">2</span>,-<span class="number">1</span>]  <span class="comment">#取任意元素</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([0.8740, 0.5771])
</code></pre><hr>
<h3 id="4-矩阵运算"><a href="#4-矩阵运算" class="headerlink" title="4.矩阵运算"></a>4.矩阵运算</h3><ul>
<li>加减乘除：按元素   <ul>
<li>$a+b=torch.add(a,b)$  </li>
<li>$a-b=torch.sub(a,b)$  </li>
<li>$a*b=torch.multiply(a,b)$  </li>
<li>$a/b=torch.div(a,b)$<br>&nbsp;   </li>
</ul>
</li>
<li>加法的广播机制  <ul>
<li>维数一致：如shape=[3,5,8]和shape=[1,5,1],运算的结果为shape=[3,5,8]。  </li>
<li>维数不一致：如shape=[3,5,8和shape=[1,8],两个矩阵右对齐，然后维数少的左边补齐，[1,8]—&gt;[1,1,8]，然后运算，结果shape=[3,5,8]  </li>
<li>对比相应维数的长度时，如果不等，必须有一个为1才能进行广播，否则出错。如shape=[3,6]和shape=[2,6]无法运算<br>&nbsp; </li>
</ul>
</li>
<li>矩阵乘法  <ul>
<li>x是一维行向量，y是一维行向量  <ul>
<li>$torch.matmul(x,y)=xy^T$  </li>
<li>$torch.matmul(y,x)=yx^T$</li>
</ul>
</li>
<li>x是一维行向量，W是二维矩阵  <ul>
<li>$torch.matmul(W,x)=(Wx^T)^T$  </li>
<li>$torch.matmul(x,W)=xW$  </li>
</ul>
</li>
<li>A是矩阵，B是矩阵  <ul>
<li>$torch.matmul(A,B)=AB$  </li>
<li>$torch.multiply(A,B)=A\cdot B$   </li>
</ul>
</li>
<li>M.shape=[j,1,n,m]，N.shape=[k,m,p]：最后两个维度作矩阵乘法，其他维度进行广播机制  <ul>
<li>[j,1]广播[k],就是[j,k],然后[n,m]与[m,p]作矩阵乘法，就是[n,p]，最终结果shape=[j,k,n,p]  </li>
<li>实际情况，N有k个shape=[m,p]的矩阵，将M也变成k个shape=[n,m]的矩阵，作运算，最后复制为j份存到dim=0这一维<br>&nbsp;  </li>
</ul>
</li>
</ul>
</li>
<li>按批次运算1：若神经网络输入为一个D维的行向量x，输出为K维的行向量y：如 FNN，RNN   <ul>
<li>x—&gt;A，torch.matmul(x,W)—&gt;torch.matmul(A,W)  </li>
<li>输入输均由一个行向量，变成N个行向量，且乘法依然符合矩阵乘法<br>&nbsp;  </li>
</ul>
</li>
<li>按批次运算2：若神经网络输入为一个LxD维的矩阵X，输出为LxD的矩阵O：如 Transformer   <ul>
<li>X—&gt; M，torch.matmul(X,W)—&gt;torch.matmul(M,W)  </li>
<li>输入输均由一排矩阵，变成N排矩阵，矩阵乘法为最后两维的矩阵乘法  </li>
<li>注意，CNN输入虽然也是矩阵，但是其运算是卷积，以点积形式实现，而非矩阵乘法  </li>
</ul>
</li>
</ul>
<h3 id="4-1-两个一维行向量相乘"><a href="#4-1-两个一维行向量相乘" class="headerlink" title="4.1 两个一维行向量相乘"></a>4.1 两个一维行向量相乘</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">y=torch.tensor([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">a=torch.matmul(y,x)</span><br><span class="line">b=torch.matmul(x.t(),y)</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>
<pre><code>(tensor(11), tensor(11))
</code></pre><h3 id="4-2-一维行向量左乘矩阵"><a href="#4-2-一维行向量左乘矩阵" class="headerlink" title="4.2 一维行向量左乘矩阵"></a>4.2 一维行向量左乘矩阵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">a=torch.matmul(x,y)</span><br><span class="line">b=torch.matmul(x.t(),y)</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([11, 16]), tensor([11, 16]))
</code></pre><h3 id="4-3-一维行向量右乘矩阵"><a href="#4-3-一维行向量右乘矩阵" class="headerlink" title="4.3 一维行向量右乘矩阵"></a>4.3 一维行向量右乘矩阵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">a=torch.matmul(y,x)</span><br><span class="line">b=torch.matmul(y,x.t())</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([ 8, 18]), tensor([ 8, 18]))
</code></pre><h3 id="4-4-矩阵相乘"><a href="#4-4-矩阵相乘" class="headerlink" title="4.4 矩阵相乘"></a>4.4 矩阵相乘</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">8</span>],[<span class="number">33</span>,<span class="number">21</span>]])    <span class="comment">#shape=[4,2]</span></span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])              <span class="comment">#shape=[2,3]</span></span><br><span class="line">a=torch.matmul(x,y)                            <span class="comment">#shape=[4,3]</span></span><br><span class="line">a</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 14,  19,  24],
        [  5,   7,   9],
        [ 35,  46,  57],
        [117, 171, 225]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">8</span>],[<span class="number">33</span>,<span class="number">21</span>]],[[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]]])    <span class="comment">#shape=[2,4,2]</span></span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])                                          <span class="comment">#shape=[2,3]</span></span><br><span class="line">a=torch.matmul(x,y)                                                        <span class="comment">#shape=[2,4,3]，实际就是将x的每排shape=[4,2]的矩阵分别与矩阵y相乘</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[ 14,  19,  24],
         [  5,   7,   9],
         [ 35,  46,  57],
         [117, 171, 225]],

        [[  5,   7,   9],
         [  5,   7,   9],
         [  5,   7,   9],
         [  5,   7,   9]]])
</code></pre><hr>
<h2 id="5-矩阵变形"><a href="#5-矩阵变形" class="headerlink" title="5.矩阵变形"></a>5.矩阵变形</h2><h3 id="5-1-转置-改变尺寸"><a href="#5-1-转置-改变尺寸" class="headerlink" title="5.1 转置,改变尺寸"></a>5.1 转置,改变尺寸</h3><p><code>torch.reshape(尺寸)</code>    </p>
<p>将矩阵拉平后，变成想要的尺寸  </p>
<p><code>torch.flatten()</code>  </p>
<p>将矩阵拉平  </p>
<p><code>torch.transpose(dim,dim)</code> </p>
<p>指定维数进行转置，没指定的看成整体</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]]])</span><br><span class="line">x.shape,x.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(torch.Size([1, 2, 3]), 2)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.flatten()</span><br></pre></td></tr></table></figure>
<pre><code>tensor([1, 2, 3, 6, 7, 8])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.reshape(<span class="number">1</span>,<span class="number">3</span>,-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[1, 2],
         [3, 6],
         [7, 8]]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[1, 6],
         [2, 7],
         [3, 8]]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[1, 2, 3]],

        [[6, 7, 8]]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).t()</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1, 3],
        [2, 4]])
</code></pre><h3 id="5-2-分割：chunk"><a href="#5-2-分割：chunk" class="headerlink" title="5.2 分割：chunk()"></a>5.2 分割：chunk()</h3><p><code>torch.chunk(块，dim)</code>   </p>
<p>在指定维数将矩阵分成相应的块数 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]).chunk(<span class="number">3</span>,<span class="number">0</span>)                 <span class="comment">#包含9个元素，分成3块</span></span><br></pre></td></tr></table></figure>
<pre><code>(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],[[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">3</span>,<span class="number">7</span>,<span class="number">0</span>]]])        <span class="comment">#第0维包含2个元素(矩阵)，第1维包含2行向量，第2维包含3个元素 </span></span><br><span class="line">x.chunk(<span class="number">2</span>,<span class="number">0</span>)                                                 <span class="comment">#把第0维的两个矩阵分开，获得两个矩阵</span></span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[[1, 2, 3],
          [4, 5, 6]]]),
 tensor([[[7, 8, 9],
          [3, 7, 0]]]))
</code></pre><h3 id="5-3-拼接：cat"><a href="#5-3-拼接：cat" class="headerlink" title="5.3 拼接：cat()"></a>5.3 拼接：cat()</h3><p><code>torch.cat((a,b),dim)</code>  </p>
<ul>
<li>dim=0，分别将第0维的元素看成整体，将它们拉平横向拼在一块 </li>
<li>dim=1，分别将第1维的元素看成整体，将它们拉平横向拼在一块 </li>
<li>dim=2，分别将第2维的元素看成整体，将它们拉平横向拼在一块 </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a,b=x.chunk(<span class="number">2</span>,<span class="number">0</span>)                                           <span class="comment">#将x分成两块，用a，b接收   </span></span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[[1, 2, 3],
          [4, 5, 6]]]),
 tensor([[[7, 8, 9],
          [3, 7, 0]]]))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat((a,b),<span class="number">0</span>)                                         <span class="comment">#按第0维拼接，第0维的元素是矩阵，所以获得所有矩阵的拼接</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[1, 2, 3],
         [4, 5, 6]],

        [[7, 8, 9],
         [3, 7, 0]]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat((a,b),<span class="number">1</span>)                                          <span class="comment">#按第1维拼接，第1维是行向量，所以获得所有行向量的拼接</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[1, 2, 3],
         [4, 5, 6],
         [7, 8, 9],
         [3, 7, 0]]])
</code></pre><h3 id="5-4-堆叠：stack"><a href="#5-4-堆叠：stack" class="headerlink" title="5.4 堆叠：stack()"></a>5.4 堆叠：stack()</h3><p><code>torch.stack((a,b),dim)</code>  </p>
<ul>
<li>首先把每个矩阵最外层扩展一个维度，也就是加个方括号  </li>
<li>dim=0，分别将第0维的元素看成整体，将它们拉平并罗列放置  </li>
<li>dim=1，分别将第1维的元素看成整体，将它们拉平并罗列放置  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        	      [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">B = torch.tensor([[<span class="number">12</span>, <span class="number">22</span>, <span class="number">33</span>],</span><br><span class="line">        	      [<span class="number">44</span>, <span class="number">55</span>, <span class="number">66</span>],</span><br><span class="line">                  [<span class="number">77</span>, <span class="number">88</span>,<span class="number">99</span>]])</span><br><span class="line">A,B</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[1, 2, 3],
         [4, 5, 6],
         [7, 8, 9]]),
 tensor([[12, 22, 33],
         [44, 55, 66],
         [77, 88, 99]]))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack((A,B))                            <span class="comment">#第一步，最外围加个方括号，增加一维；第二步，按dim=0堆叠，也就是将多个矩阵堆叠，获得多排矩阵</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[ 1,  2,  3],
         [ 4,  5,  6],
         [ 7,  8,  9]],

        [[12, 22, 33],
         [44, 55, 66],
         [77, 88, 99]]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack((A,B),dim=<span class="number">1</span>)                      <span class="comment">#第一步，最外围加个方括号，增加一维；第二步，按dim=1堆叠，分别将行向量堆叠，分别获得一个矩阵，再把这些矩阵罗列放在一个列表中</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[ 1,  2,  3],
         [12, 22, 33]],

        [[ 4,  5,  6],
         [44, 55, 66]],

        [[ 7,  8,  9],
         [77, 88, 99]]])
</code></pre><h3 id="5-5-维数增减：squeeze-与unsqueeze"><a href="#5-5-维数增减：squeeze-与unsqueeze" class="headerlink" title="5.5 维数增减：squeeze()与unsqueeze()"></a>5.5 维数增减：squeeze()与unsqueeze()</h3><p><code>torch.squeeze()</code>  </p>
<ul>
<li>删除所有维数为1的维度    </li>
</ul>
<p><code>torch.squeeze(dim)</code>  </p>
<ul>
<li>删除指定维度为1的维度，其实就是去掉指定维数的括号    </li>
</ul>
<p><code>torch.unsqueeze(dim)</code>  </p>
<ul>
<li>在指定的地方增加一个维数，假如原本是个二维矩阵  </li>
<li>dim=0,   &nbsp;最外层加一个括号  </li>
<li>dim=1,   &nbsp;对第1层每个元素，也就是每个行向量，分别加个括号  </li>
<li>dim=2,   &nbsp;对第2层每个元素，也就是每个数值，分别加个括号  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]],[[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]]])</span><br><span class="line">x.squeeze(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y=x.squeeze(<span class="number">2</span>)</span><br><span class="line">y,y.unsqueeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[1, 2, 3],
         [4, 5, 6]]),
 tensor([[[1, 2, 3]],

         [[4, 5, 6]]]))
</code></pre><h3 id="5-6-关于维度的小结"><a href="#5-6-关于维度的小结" class="headerlink" title="5.6 关于维度的小结"></a>5.6 关于维度的小结</h3><ul>
<li>[[[ &nbsp;分别看作第0，1，2维的墙   </li>
<li>如果对第0维进行操作，比如sum，mean，min的操作，那么进入第0维房间，把里面的元素看成整体，进行操作，操作后只改变第0维的维数  </li>
<li>如果是增加维数（墙面），按指定的位置增加墙面即可，如squeeze和unsqueeze  </li>
</ul>
<hr>
<h2 id="6-求和均值方差"><a href="#6-求和均值方差" class="headerlink" title="6.求和均值方差"></a>6.求和均值方差</h2><p><code>torch.sum() , torch.mean() , torch.var() , torch.std()</code>  </p>
<ul>
<li>对全部元素求和,均值，方差，标准差  </li>
</ul>
<p><code>torch.sum(dim) , torch.mean(dim) , torch.var(dim) , torch.std(dim)</code>  </p>
<ul>
<li>进入指定的维数房间，对里面的元素进行求和，均值，方差，标准差  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">x,x.<span class="built_in">sum</span>(dim=<span class="number">0</span>),x.<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[1, 2, 3],
         [4, 5, 6]]),
 tensor([5, 7, 9]),
 tensor([ 6, 15]))
</code></pre><hr>
<h2 id="7-自动求导"><a href="#7-自动求导" class="headerlink" title="7.自动求导"></a>7.自动求导</h2><p><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/2528424">PaddlePaddle</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=R_m4kanPy6Q">Youtube1</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=MswxJw-8PvE">Youtube2</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ALMKFS_QFm4">Youtube3</a>  </p>
<h3 id="7-1-基本概念"><a href="#7-1-基本概念" class="headerlink" title="7.1 基本概念"></a>7.1 基本概念</h3><ul>
<li>这里仅讨论标量对参数的求导，梯度与参数矩阵形状一致，表示每个参数动一动，标量的变化   </li>
<li>被求导的变量，需要设置requires_grad=True，dtype=torch.float32   </li>
<li>求导的梯度，在参数的grad属性中保存  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=torch.ones([<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=a.<span class="built_in">sum</span>()</span><br><span class="line">b.backward()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
</code></pre><h3 id="7-2-梯度的累加"><a href="#7-2-梯度的累加" class="headerlink" title="7.2 梯度的累加"></a>7.2 梯度的累加</h3><ul>
<li>梯度每次计算都会被累加在grad属性中    </li>
<li>使用torch.grad.zero_()方法可以进行梯度归零    </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a=torch.ones([<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    b=a.<span class="built_in">sum</span>()</span><br><span class="line">    b.backward()</span><br><span class="line">    <span class="built_in">print</span>(a.grad)</span><br><span class="line">a.grad.zero_()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[2., 2., 2.],
        [2., 2., 2.]])
tensor([[3., 3., 3.],
        [3., 3., 3.]])
tensor([[0., 0., 0.],
        [0., 0., 0.]])
</code></pre><h3 id="7-3-计算图的构建与销毁"><a href="#7-3-计算图的构建与销毁" class="headerlink" title="7.3 计算图的构建与销毁"></a>7.3 计算图的构建与销毁</h3><ul>
<li>每次进行前向计算时都会自动构建计算图，调用backward后自动销毁   </li>
<li>在调用backward方法的时候设置retain_graph=True，计算图可保留，不用再次前向计算  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=torch.ones([<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=a.<span class="built_in">sum</span>()</span><br><span class="line">b.backward(retain_graph=<span class="literal">True</span>) <span class="comment">#计算图保留，相当于对计算图什么也没做，就求了一次梯度</span></span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line">b.backward() <span class="comment">#调用后自动销毁计算图</span></span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[2., 2., 2.],
        [2., 2., 2.]])
</code></pre><h3 id="7-4-不构建计算图"><a href="#7-4-不构建计算图" class="headerlink" title="7.4 不构建计算图"></a>7.4 不构建计算图</h3><ul>
<li>前向计算时，构建正向计算图的同时，会通过回溯的方式，构建反向算子与反向计算图    </li>
<li>可以通过查看grad_fn属性查看有没有构建反向算子  </li>
<li>with torch.no_grad(): 后面的计算，不会构建计算图    </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">2.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">f=<span class="number">2</span>*a                                  <span class="comment">#f参与构建计算图</span></span><br><span class="line">g=<span class="number">3</span>*b                                  <span class="comment">#g参与构建计算图</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    f=<span class="number">3</span>*a                              <span class="comment">#f重新定义，不构建计算图</span></span><br><span class="line"><span class="built_in">print</span>(f.grad_fn,g.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>None &lt;MulBackward0 object at 0x000001D68C876DC0&gt;
</code></pre><h3 id="7-5-将变量变成常量"><a href="#7-5-将变量变成常量" class="headerlink" title="7.5 将变量变成常量"></a>7.5 将变量变成常量</h3><ul>
<li>a.detach()返回一个张量，data区就是a的data区，requires_grad=False    </li>
<li>可以用out来接收，输入到下一个网络中，并且作为常量传入的，不参与梯度计算    </li>
<li>由于是新创建的张量，不影响原来的计算图 ；但是不要用同一个变量名接收这个张量，否则计算图中的那个变量名就没了   </li>
<li>固定网络A的参数，更新网络B的参数的方法    <ul>
<li>方法一：将网络A的输入detach一下，创建新的标量张量作为网络B的输入，这时候只会构建B网络的计算图    </li>
<li>方法二：用for遍历A网络的参数，将requires_grad属性全部设为False    </li>
</ul>
</li>
<li>一个不常用的torch.detach_()操作，原地修改计算图，有点复杂，实际上所有的tensor操作后加个下划线都是原地操作   </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">2.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=a.detach()</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>
<pre><code>(tensor(2., requires_grad=True), tensor(2.))
</code></pre><hr>
<h2 id="8-GPU的使用"><a href="#8-GPU的使用" class="headerlink" title="8.GPU的使用"></a>8.GPU的使用</h2><ul>
<li>这里仅使用一块gpu  </li>
<li>查看设备是否可用：torch.cuda.is_available()查看设备  </li>
<li>指定设备：device = torch.device(“cuda:0”)或device = torch.device( “cpu”)或device = torch.device(“cuda:0”  if torch.cuda.is_available() else “cpu”)  </li>
<li>使用tensor.to(device)和model.to(tensor)把输入张量和模型参数送进gpu，这样计算图便在gpu中构建了；使用.device可以查看设备属性  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available(),torch.tensor(<span class="number">2.0</span>).device</span><br></pre></td></tr></table></figure>
<pre><code>(False, device(type=&#39;cpu&#39;))
</code></pre><hr>
<h2 id="9-综合案例"><a href="#9-综合案例" class="headerlink" title="9.综合案例"></a>9.综合案例</h2><h3 id="9-1-综合案例1"><a href="#9-1-综合案例1" class="headerlink" title="9.1 综合案例1"></a>9.1 综合案例1</h3><p>采集两个圆上的点数据，x作为一个特征，y作为一个特征，每个圆各采集5个样本，最后合并成10个样本，每个样本有2个特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">n_samples = <span class="number">10</span></span><br><span class="line">n_samples_out = n_samples // <span class="number">2</span></span><br><span class="line">n_samples_in = n_samples - n_samples_out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采集第1类数据，特征为(x,y)</span></span><br><span class="line"><span class="comment"># 使用&#x27;paddle.linspace&#x27;在0到pi上均匀取n_samples_out个值</span></span><br><span class="line"><span class="comment"># 使用&#x27;paddle.cos&#x27;计算上述取值的余弦值作为特征1，使用&#x27;paddle.sin&#x27;计算上述取值的正弦值作为特征2</span></span><br><span class="line">outer_circ_x = paddle.cos(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_out))</span><br><span class="line">outer_circ_y = paddle.sin(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_out))</span><br><span class="line"></span><br><span class="line">inner_circ_x = <span class="number">1</span> - paddle.cos(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_in))</span><br><span class="line">inner_circ_y = <span class="number">0.5</span> - paddle.sin(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_in))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;outer_circ_x.shape:&#x27;</span>, outer_circ_x.shape,<span class="string">&#x27;\n&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;outer_circ_y.shape:&#x27;</span>, outer_circ_y.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;inner_circ_x.shape:&#x27;</span>, inner_circ_x.shape,<span class="string">&#x27;\n&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;inner_circ_y.shape:&#x27;</span>, inner_circ_y.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;outer_circ_x:&#x27;</span>, outer_circ_x,<span class="string">&#x27;\n&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;outer_circ_y:&#x27;</span>, outer_circ_y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;inner_circ_x:&#x27;</span>, inner_circ_x,<span class="string">&#x27;\n&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;inner_circ_y:&#x27;</span>, inner_circ_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用&#x27;paddle.concat&#x27;将两类数据的特征1和特征2分别延维度0拼接在一起，得到全部特征1和特征2</span></span><br><span class="line"><span class="comment"># 使用&#x27;paddle.stack&#x27;将两类特征延维度1堆叠在一起</span></span><br><span class="line">X = paddle.stack(</span><br><span class="line">    [paddle.concat([outer_circ_x, inner_circ_x]),</span><br><span class="line">     paddle.concat([outer_circ_y, inner_circ_y])],</span><br><span class="line">    axis=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;after concat shape:&#x27;</span>, paddle.concat(</span><br><span class="line">    [outer_circ_x, inner_circ_x]).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X shape:&#x27;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;after concat :&#x27;</span>, paddle.concat(</span><br><span class="line">    [outer_circ_x, inner_circ_x]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X :&#x27;</span>, X)</span><br><span class="line"><span class="comment"># 使用&#x27;paddle. zeros&#x27;将第一类数据的标签全部设置为0</span></span><br><span class="line"><span class="comment"># 使用&#x27;paddle. ones&#x27;将第一类数据的标签全部设置为1</span></span><br><span class="line">y = paddle.concat(</span><br><span class="line">    [paddle.zeros(shape=[n_samples_out]),</span><br><span class="line">     paddle.ones(shape=[n_samples_in])]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y shape:&#x27;</span>, y.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y :&#x27;</span>, y)</span><br></pre></td></tr></table></figure>
<pre><code>outer_circ_x.shape: [5] 
 outer_circ_y.shape: [5]
inner_circ_x.shape: [5] 
 inner_circ_y.shape: [5]
outer_circ_x: Tensor(shape=[5], dtype=float32, place=Place(cpu), stop_gradient=True,
       [ 1.        ,  0.70710677, -0.00000004, -0.70710683, -1.        ]) 
 outer_circ_y: Tensor(shape=[5], dtype=float32, place=Place(cpu), stop_gradient=True,
       [ 0.        ,  0.70710683,  1.        ,  0.70710683, -0.00000009])
inner_circ_x: Tensor(shape=[5], dtype=float32, place=Place(cpu), stop_gradient=True,
       [0.        , 0.29289323, 1.        , 1.70710683, 2.        ]) 
 inner_circ_y: Tensor(shape=[5], dtype=float32, place=Place(cpu), stop_gradient=True,
       [ 0.50000000, -0.20710683, -0.50000000, -0.20710683,  0.50000006])
after concat shape: [10]
X shape: [10, 2]
after concat : Tensor(shape=[10], dtype=float32, place=Place(cpu), stop_gradient=True,
       [ 1.        ,  0.70710677, -0.00000004, -0.70710683, -1.        ,
         0.        ,  0.29289323,  1.        ,  1.70710683,  2.        ])
X : Tensor(shape=[10, 2], dtype=float32, place=Place(cpu), stop_gradient=True,
       [[ 1.        ,  0.        ],
        [ 0.70710677,  0.70710683],
        [-0.00000004,  1.        ],
        [-0.70710683,  0.70710683],
        [-1.        , -0.00000009],
        [ 0.        ,  0.50000000],
        [ 0.29289323, -0.20710683],
        [ 1.        , -0.50000000],
        [ 1.70710683, -0.20710683],
        [ 2.        ,  0.50000006]])
y shape: [10]
y : Tensor(shape=[10], dtype=float32, place=Place(cpu), stop_gradient=True,
       [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])
</code></pre><hr>
<h3 id="9-2-综合案例2：分类任务的准确率计算"><a href="#9-2-综合案例2：分类任务的准确率计算" class="headerlink" title="9.2 综合案例2：分类任务的准确率计算"></a>9.2 综合案例2：分类任务的准确率计算</h3><ul>
<li>二分类任务imputs为NxD，labels为Nx1，outputs为Nx1    </li>
<li>多分类任务imputs为NxD，labels为Nx1，outputs为NxC  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">preds, labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - preds：预测值，二分类时，shape=[N, 1]，N为样本数量，多分类时，shape=[N, C]，C为类别数量</span></span><br><span class="line"><span class="string">        - labels：真实标签，shape=[N, 1]</span></span><br><span class="line"><span class="string">    输出：</span></span><br><span class="line"><span class="string">        - 准确率：shape=[1]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 判断是二分类任务还是多分类任务，preds.shape[1]=1时为二分类任务，preds.shape[1]&gt;1时为多分类任务</span></span><br><span class="line">    <span class="keyword">if</span> preds.shape[<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 二分类时，判断每个概率值是否大于0.5，当大于0.5时，类别为1，否则类别为0</span></span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.cast&#x27;将preds的数据类型转换为float32类型</span></span><br><span class="line">        preds = paddle.cast((preds&gt;=<span class="number">0.5</span>),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 多分类时，使用&#x27;paddle.argmax&#x27;计算最大元素索引作为类别</span></span><br><span class="line">        preds = paddle.argmax(preds,axis=<span class="number">1</span>, dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> paddle.mean(paddle.cast(paddle.equal(preds, labels),dtype=<span class="string">&#x27;float32&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设模型的预测值为[[0.],[1.],[1.],[0.]]，真实类别为[[1.],[1.],[0.],[0.]]，计算准确率</span></span><br><span class="line">preds = paddle.to_tensor([[<span class="number">0.</span>],[<span class="number">1.</span>],[<span class="number">1.</span>],[<span class="number">0.</span>]])</span><br><span class="line">labels = paddle.to_tensor([[<span class="number">1.</span>],[<span class="number">1.</span>],[<span class="number">0.</span>],[<span class="number">0.</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuracy is:&quot;</span>, accuracy(preds, labels))</span><br></pre></td></tr></table></figure>
<pre><code>accuracy is: Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
       [0.50000000])
</code></pre><hr>
<h3 id="9-3-综合案例3：Simple-RNN（了解矩阵行向量左乘矩阵以及一个批次的行向量左乘矩阵）"><a href="#9-3-综合案例3：Simple-RNN（了解矩阵行向量左乘矩阵以及一个批次的行向量左乘矩阵）" class="headerlink" title="9.3 综合案例3：Simple RNN（了解矩阵行向量左乘矩阵以及一个批次的行向量左乘矩阵）"></a>9.3 综合案例3：Simple RNN（了解矩阵行向量左乘矩阵以及一个批次的行向量左乘矩阵）</h3><h4 id="输入与输出分析"><a href="#输入与输出分析" class="headerlink" title="输入与输出分析"></a>输入与输出分析</h4><ul>
<li>inputs：3维矩阵，如shape=[3，4，8]，表示3个batch（3个句子），每个句子包含4个单词，每个单词是一个8维向量    </li>
<li>中间层计算（1）：先分析一个句子的情况，并且忽略记忆单元。第一个词进去，出来一个词，第二个词进去，出来一个词，依次进行到最后一个词进去，出来一个词，把最后出来的词作为输出    </li>
<li>中间层计算（2）：由于神经网络一次性并行计算一个batch的样本，如一个batch包含5句话，那么将5句话的第一个词是一个矩阵，每一行就是每句话的第一个词，那么每层输出都是一个矩阵，表示这5句话的第j个词对应的输出   </li>
<li>隐藏层：就是中间层的输出    </li>
<li>outputs：就是最后一个词和隐藏层丢进网络的输出    </li>
<li>单次计算的本质：处理单个词，输出单个词；对于一个batch，输入多个并行的词，输出多个并行的词，并行的词来自不同的句子；对于一个句子的多个词，采用串行依次处理  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SRN模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SRN</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size,  hidden_size, W_attr=<span class="literal">None</span>, U_attr=<span class="literal">None</span>, b_attr=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SRN, self).__init__()</span><br><span class="line">        <span class="comment"># 嵌入向量的维度</span></span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        <span class="comment"># 隐状态的维度</span></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        <span class="comment"># 定义模型参数W，其shape为 input_size x hidden_size</span></span><br><span class="line">        self.W = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="string">&quot;float32&quot;</span>, attr=W_attr)</span><br><span class="line">        <span class="comment"># 定义模型参数U，其shape为hidden_size x hidden_size</span></span><br><span class="line">        self.U = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="string">&quot;float32&quot;</span>,attr=U_attr)</span><br><span class="line">        <span class="comment"># 定义模型参数b，其shape为 1 x hidden_size</span></span><br><span class="line">        self.b = paddle.create_parameter(shape=[<span class="number">1</span>, hidden_size], dtype=<span class="string">&quot;float32&quot;</span>, attr=b_attr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化向量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, batch_size</span>):</span><br><span class="line">        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> hidden_state</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, hidden_state=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># inputs: 输入数据, 其shape为batch_size x seq_len x input_size</span></span><br><span class="line">        batch_size, seq_len, input_size = inputs.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化起始状态的隐向量, 其shape为 batch_size x hidden_size</span></span><br><span class="line">        <span class="keyword">if</span> hidden_state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            hidden_state = self.init_state(batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 循环执行RNN计算</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">            <span class="comment"># 获取当前时刻的输入数据step_input, 其shape为 batch_size x input_size</span></span><br><span class="line">            step_input = inputs[:, step, :]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;中间层的x:<span class="subst">&#123;step_input&#125;</span>&#x27;</span>,<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            <span class="comment"># 获取当前时刻的隐状态向量hidden_state, 其shape为 batch_size x hidden_size</span></span><br><span class="line">            hidden_state = F.tanh(paddle.matmul(step_input, self.W) + paddle.matmul(hidden_state, self.U) + self.b)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;中间层的h:<span class="subst">&#123;hidden_state&#125;</span>&#x27;</span>,<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> hidden_state</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">W_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="number">0.1</span>, <span class="number">0.2</span>], [<span class="number">0.1</span>,<span class="number">0.2</span>]]))</span><br><span class="line">U_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="number">0.0</span>, <span class="number">0.1</span>], [<span class="number">0.1</span>,<span class="number">0.0</span>]]))</span><br><span class="line">b_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="number">0.1</span>, <span class="number">0.1</span>]]))</span><br><span class="line"></span><br><span class="line">srn = SRN(<span class="number">2</span>, <span class="number">2</span>, W_attr=W_attr, U_attr=U_attr, b_attr=b_attr)</span><br><span class="line"></span><br><span class="line">inputs = paddle.to_tensor([[[<span class="number">1</span>, <span class="number">0</span>],[<span class="number">0</span>, <span class="number">2</span>]],[[<span class="number">2</span>, <span class="number">4</span>],[<span class="number">1</span>, <span class="number">3</span>]]], dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">hidden_state = srn(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hidden_state&quot;</span>, hidden_state)</span><br></pre></td></tr></table></figure>
<pre><code>中间层的x:Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,
       [[1., 0.],
        [2., 4.]]) 

中间层的h:Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,
       [[0.19737528, 0.29131261],
        [0.60436779, 0.86172318]]) 

中间层的x:Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,
       [[0., 2.],
        [1., 3.]]) 

中间层的h:Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,
       [[0.31773996, 0.47749743],
        [0.52713710, 0.74447167]]) 

hidden_state Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,
       [[0.31773996, 0.47749743],
        [0.52713710, 0.74447167]])
</code></pre><hr>
<p><a href="#Top">To the top</a></p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/09/03/opt/KT-conditions/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          KT条件
        
      </div>
    </a>
  
  
    <a href="/2022/08/13/pytorch/Pytorch/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Pytorch 简介</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2022 宝可梦训练师
      </div>
        <div class="footer-right">
          <a href="https://space.bilibili.com/782159" target="_blank">访问我的哔哩哔哩</a> 
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: ,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":170,"height":340},"mobile":{"show":false},"log":false});</script></body>
</html>