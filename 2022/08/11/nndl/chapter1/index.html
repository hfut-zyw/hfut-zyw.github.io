<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第1章 实践基础 | Homepage | 张有为</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="第1章 实践基础深度学习在很多领域中都有非常出色的表现，在图像识别、语音识别、自然语言处理、机器人、广告投放、医学诊断和金融等领域都有广泛应用。而目前深度学习的模型还主要是各种各样的神经网络。">
<meta property="og:type" content="article">
<meta property="og:title" content="第1章 实践基础">
<meta property="og:url" content="http://example.com/2022/08/11/nndl/chapter1/index.html">
<meta property="og:site_name" content="Homepage | 张有为">
<meta property="og:description" content="第1章 实践基础深度学习在很多领域中都有非常出色的表现，在图像识别、语音识别、自然语言处理、机器人、广告投放、医学诊断和金融等领域都有广泛应用。而目前深度学习的模型还主要是各种各样的神经网络。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-08-11T13:05:57.000Z">
<meta property="article:modified_time" content="2022-09-03T22:12:12.424Z">
<meta property="article:author" content="Pokemon Master">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="Homepage | 张有为" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/icon.gif">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>


  
<meta name="generator" content="Hexo 6.2.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/face.png" class="js-avatar show" style="width: 100%;height: 100%;opacity: 1;">
		</a>

		<hgroup>
			<h1 class="header-author"><a href="/">Pokemon Master</a></h1>
		</hgroup>

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/2022/08/09/me/aboutme">个人简介</a></li>
				        
							<li><a href="/categories/notes">随写</a></li>
				        
							<li><a href="/categories/analysis">实变泛函</a></li>
				        
							<li><a href="/categories/opt">优化笔记</a></li>
				        
							<li><a href="/categories/pytorch">Pytorch</a></li>
				        
							<li><a href="/categories/nndl">nndl案例与实践</a></li>
				        
						</ul>
					</nav>
					<nav class="half-header-menu">
						<a class="hide">Home</a>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
					        
						</div>
						<!-- music -->
						
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/face.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/2022/08/09/me/aboutme">个人简介</a></li>
		        
					<li><a href="/categories/notes">随写</a></li>
		        
					<li><a href="/categories/analysis">实变泛函</a></li>
		        
					<li><a href="/categories/opt">优化笔记</a></li>
		        
					<li><a href="/categories/pytorch">Pytorch</a></li>
		        
					<li><a href="/categories/nndl">nndl案例与实践</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-nndl/chapter1" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/08/11/nndl/chapter1/" class="article-date">
  	<time datetime="2022-08-11T13:05:57.000Z" itemprop="datePublished">2022-08-11</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第1章 实践基础
      
          <span class="title-pop-out"></a>
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/nndl/">nndl</a>
	</div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第1章-实践基础"><a href="#第1章-实践基础" class="headerlink" title="第1章 实践基础"></a>第1章 实践基础</h1><p>深度学习在很多领域中都有非常出色的表现，在图像识别、语音识别、自然语言处理、机器人、广告投放、医学诊断和金融等领域都有广泛应用。而目前深度学习的模型还主要是各种各样的神经网络。<span id="more"></span>随着网络越来越复杂，从底层开始一步步实现深度学习系统变得非常低效，其中涉及模型搭建、梯度求解、并行计算、代码实现等多个环节。每一个环节都需要进行精心实现和检查，需要耗费开发人员很多的精力。为此，深度学习框架（也常称为机器学习框架）应运而生，它有助于研发人员聚焦任务和模型设计本身，省去大量而烦琐的代码编写工作，其优势主要表现在如下两个方面：</p>
<ul>
<li>实现简单：深度学习框架屏蔽了底层实现，用户只需关注模型的逻辑结构，同时简化了计算逻辑，降低了深度学习入门门槛。</li>
<li>使用高效：深度学习框架具备灵活的移植性，在不同设备（CPU、GPU或移动端）之间无缝迁移，使得深度学习框架会使模型训练以及部署更高效。</li>
</ul>
<p>本书使用飞桨框架作为实践的基础框架。飞桨（PaddlePaddle）框架是一套面向深度学习的基础训练和推理框架。飞桨于2016年正式开源，是主流深度学习开源框架中一款完全国产化的产品。目前，飞桨框架已经非常成熟并且易用，可以很好地支持本书的实践设计。</p>
<p>在讲解本书主要内容之前，本章先对实践环节的基础知识进行介绍，主要介绍以下内容：</p>
<ul>
<li>张量（Tensor）：深度学习中表示和存储数据的主要形式。在动手实践机器学习之前，需要熟悉张量的概念、性质和运算规则，以及了解飞桨中张量的各种API。</li>
<li>算子（Operator）：构建神经网络模型的基础组件。每个算子有前向和反向计算过程，前向计算对应一个数学函数，而反向计算对应这个数学函数的梯度计算。有了算子，我们就可以很方便地通过算子来搭建复杂的神经网络模型，而不需要手工计算梯度。</li>
</ul>
<p>此外，本章还汇总了在本书中自定义的一些算术、数据集以及轻量级训练框架Runner类。</p>
<h2 id="1-1-如何运行本书的代码"><a href="#1-1-如何运行本书的代码" class="headerlink" title="1.1 如何运行本书的代码"></a>1.1 如何运行本书的代码</h2><hr>
<p><strong>笔记</strong></p>
<p>本书涉及大量代码实践，通过运行代码理解如何构建模型及训练网络。本书中代码有两种运行方式：本地运行AI Studio运行。下面我们分别介绍两种运行方式的环境准备及操作方法。</p>
<hr>
<h3 id="1-1-1-本地运行"><a href="#1-1-1-本地运行" class="headerlink" title="1.1.1 本地运行"></a>1.1.1 本地运行</h3><h4 id="1-1-1-1-环境准备"><a href="#1-1-1-1-环境准备" class="headerlink" title="1.1.1.1 环境准备"></a>1.1.1.1 环境准备</h4><p>本书代码基于Python语言与飞桨框架开发，如选择在本地运行请首先确认本机的操作系统、Python及pip版本是否满足飞桨支持的环境。目前飞桨支持的环境如下：</p>
<ul>
<li>Linux 版本（64 bit）<ul>
<li>CentOS 7（GPU版本支持CUDA 10.1/10.2/11.0/11.1/11.2）<ul>
<li>Ubuntu 16.04（GPU版本支持CUDA 10.1/10.2/11.0/11.1/11.2）</li>
<li>Ubuntu 18.04（GPU版本支持CUDA 10.1/10.2/11.0/11.1/11.2）</li>
<li>Ubuntu 20.04（GPU版本支持CUDA 10.1/10.2/11.0/11.1/11.2）</li>
</ul>
</li>
</ul>
</li>
<li>Python版本3.6/3.7/3.8/3.9（64 bit）</li>
<li>pip或pip3版本20.2.2或更高版本（64 bit）</li>
</ul>
<p>可以使用如下命令查看本机的操作系统和位数信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -m &amp;&amp; cat /etc/*release</span><br></pre></td></tr></table></figure>
<p>使用如下命令确认Python版本是否为3.6/3.7/3.8/3.9:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br></pre></td></tr></table></figure>
<p>使用如下命令确认pip版本是否满足要求：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m ensurepip</span><br><span class="line">python -m pip --version</span><br></pre></td></tr></table></figure>
<p>确认Python和pip是64bit版本，且处理器架构是x86_64（或称作x64、Intel 64、AMD64）架构。需要注意，目前飞桨不支持arm64架构。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c &quot;import platform;print(platform.architecture()[0]);print(platform.machine())&quot;</span><br></pre></td></tr></table></figure>
<p>该命令第一行输出为“64bit”，第二行输出为“x86_64”“x64”或“AMD64”即符合要求。</p>
<h4 id="1-1-1-2-快速安装"><a href="#1-1-1-2-快速安装" class="headerlink" title="1.1.1.2 快速安装"></a>1.1.1.2 快速安装</h4><p>本书第1章内容使用CPU即可完成，无须其他硬件设备。但从第2章开始，建议使用支持CUDA的GPU，书中代码默认在32G RAM的Tesla V100上运行，如使用其他配置的GPU可适当调整模型训练参数或直接通过AI Studio平台运行代码。</p>
<p><strong>笔记</strong><br>在GPU上运行模型训练代码可以大幅缩短模型训练时间，但使用GPU并不是必需的。如果电脑没有GPU硬件设备，本书内代码在仅使用CPU的情况下仍可以跑通，只是模型训练所需时间会增加。在这种情况下，可以使用AI Studio平台的免费GPU算力运行代码。使用方法详见第1.1.2节。</p>
<p>目前推荐使用飞桨开源框架v2.2版本，后续可在飞桨官网查看最新稳定版本。通过如下命令安装CPU版本：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install paddlepaddle==2.2.2 -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure>
<p>通过如下命令安装GPU版本：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install paddlepaddle-gpu==2.2.2 -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure>
<p>默认GPU环境为CUDA 10.2，如需安装基于其他CUDA版本的飞桨框架，可在2.2.2后面加入版本后缀，比如CUDA 10.1版本的飞桨框架对应为paddlepaddle-gpu==2.2.2.post101。  </p>
<hr>
<p><strong>动手练习1.1</strong>  </p>
<ul>
<li>使用python命令进入python解释器，输入import paddle，验证安装是否成功。</li>
<li>输入paddle.<strong>version</strong>验证版本安装是否正确。</li>
<li>输入paddle.utils.run_check()，如出现“PaddlePaddle is installed successfully!”，则说明已正确安装。</li>
</ul>
<hr>
<h3 id="1-1-2-AI-Studio运行"><a href="#1-1-2-AI-Studio运行" class="headerlink" title="1.1.2 AI Studio运行"></a>1.1.2 AI Studio运行</h3><p>AI Studio是基于飞桨的人工智能学习与实训社区，提供免费的算力支持，本书的内容在AI Studio上提供配套的BML Codelab项目。BML Codelab是面向个人和企业开发者的AI开发工具，基于Jupyter提供了在线的交互式开发环境。</p>
<p>BML Codelab目前默认使用飞桨2.2.2版本，无须额外安装。如图1.1所示，通过选择“启动环境”$\rightarrow$“基础版”即可在CPU环境下运行，选择“至尊版GPU”即可在32G RAM的Tesla V100上运行代码，至尊版GPU每天有8小时的免费使用时间。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/d92624b43ee741d1b381c8cbe75a7c5a80281be8c7204f46acde16805d068566' width='400'></center>
<center>图1.1 AI Studio项目运行环境选择</center>

<p><br></br></p>
<p>选择环境进入项目后，项目整体布局如图1.2所示。项目页面（Notebook）由侧边栏、菜单栏、快捷工具栏、状态监控栏和代码编辑区组成。这里我们重点介绍代码编辑区和快捷工具栏，其余边栏的使用方法可参见<a target="_blank" rel="noopener" href="https://ai.baidu.com/ai-doc/AISTUDIO/Gktuwqf1x">BML Codelab环境使用说明</a>。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/7490de9b3c254e54b9b56cf86b7866f42af8c495951341278c7f71dddede43f5' width='600'></center>
<center>图1.2 BML Codelab项目布局</center>

<p><br></br></p>
<p>代码编辑区主要由代码编写单元(Code Cell)组成，在代码编写单元内编写Python代码或shell命令，点击“运行”按钮，代码或命令将在云端执行，并将结果返回到代码编写单元，直接显示在项目页面中。图1.3中我们简单定义两行代码，通过运行代码编写单元，得到输出结果。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/3ab4878737ac4de09e5526fbe46621ada9050a06f11a4823ae2265654dcbeb42' width='600'></center>
<center>图1.3 代码编写单元交互式运行</center>

<p><br></br></p>
<p>快捷工具栏内工具如图1.4所示，工具功能为：</p>
<ul>
<li>运行：运行当前选中的代码编写单元。</li>
<li>停止运行：停止Notebook的运行状态。</li>
<li>重启内核：重启代码内核，清空环境中的环境变量、缓存变量、输出结果等.</li>
<li>保存：保存Notebook项目文件.</li>
<li>插入：添加指定类型的单元，支持Code和Markdown两种类型。</li>
<li>定位：定位到正在执行的单元。</li>
</ul>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/9d1c2a78c08f410598406e28765e0e1c0893f49f22324f699aaf6c10871dd0f1' width='600'></center>
<center>图1.4 快捷工具栏</center>

<p><br></br></p>
<h2 id="1-2-张量"><a href="#1-2-张量" class="headerlink" title="1.2 张量"></a>1.2 张量</h2><p>在深度学习的实践中，我们通常使用向量或矩阵运算来提高计算效率。比如$w_1x_1 + w_2 x_2 +\cdots +w_N x_N$的计算可以用$\bm w^\top \bm x$来代替（其中$\bm w=[w_1 w_2 \cdots w_N]^\top，\bm x=[x_1 x_2 \cdots x_N]^\top$），这样可以充分利用计算机的并行计算能力，特别是利用GPU来实现高效矩阵运算。</p>
<p>在深度学习框架中，数据经常用张量(Tensor)的形式来存储。张量是矩阵的扩展与延伸，可以认为是高阶的矩阵。1阶张量为向量，2阶张量为矩阵。如果你对Numpy熟悉，那么张量是类似于Numpy的多维数组(ndarray)的概念，可以具有任意多的维度。</p>
<hr>
<p><strong>笔记</strong></p>
<p>注意：这里的“维度”是“阶”的概念，和线性代数中向量的“维度”含义不同。</p>
<hr>
<p>张量的大小可以用形状（shape）来描述。比如一个三维张量的形状是 $[2, 2, 5]$，表示每一维（也称为轴（axis））的元素的数量，即第0轴上元素数量是2，第1轴上元素数量是2，第2轴上的元素数量为5。</p>
<p>图1.5给出了3种纬度的张量可视化表示。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/fcdd500471b842a4811bd7ab3f724ab4b9226fc94bf446818904e59ce1bb6e00' width='600'></center>
<center><br>图1.5 不同维度的张量可视化表示</br></center>

<p><br></br></p>
<p>张量中元素的类型可以是布尔型数据、整数、浮点数或者复数，但同一张量中所有元素的数据类型均相同。因此我们可以给张量定义一个数据类型(dtype)来表示其元素的类型。</p>
<h3 id="1-2-1-创建张量"><a href="#1-2-1-创建张量" class="headerlink" title="1.2.1 创建张量"></a>1.2.1 创建张量</h3><p>创建一个张量可以有多种方式，如：指定数据创建、指定形状创建、指定区间创建等。</p>
<h4 id="1-2-1-1-指定数据创建张量"><a href="#1-2-1-1-指定数据创建张量" class="headerlink" title="1.2.1.1 指定数据创建张量"></a>1.2.1.1 指定数据创建张量</h4><p>通过给定Python列表数据，可以创建任意维度的张量。</p>
<p>（1）通过指定的Python列表数据[2.0, 3.0, 4.0]，创建一个一维张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入PaddlePaddle</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="comment"># 创建一维Tensor</span></span><br><span class="line">ndim_1_Tensor = paddle.to_tensor([<span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line"><span class="built_in">print</span>(ndim_1_Tensor)</span><br></pre></td></tr></table></figure>
<pre><code>Tensor(shape=[3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [2., 3., 4.])
</code></pre><p>（2）通过指定的Python列表数据来创建类似矩阵（matrix）的二维张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建二维Tensor</span></span><br><span class="line">ndim_2_Tensor = paddle.to_tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>],</span><br><span class="line">                                  [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line"><span class="built_in">print</span>(ndim_2_Tensor)</span><br></pre></td></tr></table></figure>
<pre><code>Tensor(shape=[2, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[1., 2., 3.],
        [4., 5., 6.]])
</code></pre><p>（3）同样地，还可以创建维度为3、4…N等更复杂的多维张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建多维Tensor</span></span><br><span class="line">ndim_3_Tensor = paddle.to_tensor([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">                                   [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]],</span><br><span class="line">                                  [[<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">                                   [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>]]])</span><br><span class="line"><span class="built_in">print</span>(ndim_3_Tensor)</span><br></pre></td></tr></table></figure>
<pre><code>Tensor(shape=[2, 2, 5], dtype=int64, place=CPUPlace, stop_gradient=True,
       [[[1 , 2 , 3 , 4 , 5 ],
         [6 , 7 , 8 , 9 , 10]],

        [[11, 12, 13, 14, 15],
         [16, 17, 18, 19, 20]]])
</code></pre><p>需要注意的是，张量在任何一个维度上的元素数量必须相等。下面尝试定义一个在同一维度上元素数量不等的张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 尝试定义在不同维度上元素数量不等的Tensor</span></span><br><span class="line">ndim_2_Tensor = paddle.to_tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>],</span><br><span class="line">                                  [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ValueError:</span><br><span class="line">        Faild to convert input data to a regular ndarray :</span><br><span class="line">         - Usually this means the input data contains nested lists with different lengths.</span><br></pre></td></tr></table></figure>
<p>从输出结果看，这种定义情况会抛出异常，提示在任何维度上的元素数量必须相等。</p>
<h4 id="1-2-1-2-指定形状创建"><a href="#1-2-1-2-指定形状创建" class="headerlink" title="1.2.1.2 指定形状创建"></a>1.2.1.2 指定形状创建</h4><p>如果要创建一个指定形状、元素数据相同的张量，可以使用<code>paddle.zeros</code>、<code>paddle.ones</code>、<code>paddle.full</code>等API。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">m, n = <span class="number">2</span>, <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用paddle.zeros创建数据全为0，形状为[m, n]的Tensor</span></span><br><span class="line">zeros_Tensor = paddle.zeros([m, n])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用paddle.ones创建数据全为1，形状为[m, n]的Tensor</span></span><br><span class="line">ones_Tensor = paddle.ones([m, n])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用paddle.full创建数据全为指定值，形状为[m, n]的Tensor，这里我们指定数据为10</span></span><br><span class="line">full_Tensor = paddle.full([m, n], <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;zeros Tensor: &#x27;</span>, zeros_Tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ones Tensor: &#x27;</span>, ones_Tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;full Tensor: &#x27;</span>, full_Tensor)</span><br></pre></td></tr></table></figure>
<pre><code>zeros Tensor:  Tensor(shape=[2, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[0., 0., 0.],
        [0., 0., 0.]])
ones Tensor:  Tensor(shape=[2, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[1., 1., 1.],
        [1., 1., 1.]])
full Tensor:  Tensor(shape=[2, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[10., 10., 10.],
        [10., 10., 10.]])
</code></pre><h4 id="1-2-1-3-指定区间创建"><a href="#1-2-1-3-指定区间创建" class="headerlink" title="1.2.1.3 指定区间创建"></a>1.2.1.3 指定区间创建</h4><p>如果要在指定区间内创建张量，可以使用<code>paddle.arange</code>、<code>paddle.linspace</code>等API。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用paddle.arange创建以步长step均匀分隔数值区间[start, end)的一维Tensor</span></span><br><span class="line">arange_Tensor = paddle.arange(start=<span class="number">1</span>, end=<span class="number">5</span>, step=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用paddle.linspace创建以元素个数num均匀分隔数值区间[start, stop]的Tensor</span></span><br><span class="line">linspace_Tensor = paddle.linspace(start=<span class="number">1</span>, stop=<span class="number">5</span>, num=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;arange Tensor: &#x27;</span>, arange_Tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;linspace Tensor: &#x27;</span>, linspace_Tensor)</span><br></pre></td></tr></table></figure>
<pre><code>arange Tensor:  Tensor(shape=[4], dtype=int64, place=CPUPlace, stop_gradient=True,
       [1, 2, 3, 4])
linspace Tensor:  Tensor(shape=[5], dtype=float32, place=CPUPlace, stop_gradient=True,
       [1., 2., 3., 4., 5.])
</code></pre><h3 id="1-2-2-张量的属性"><a href="#1-2-2-张量的属性" class="headerlink" title="1.2.2 张量的属性"></a>1.2.2 张量的属性</h3><h4 id="1-2-2-1-张量的形状"><a href="#1-2-2-1-张量的形状" class="headerlink" title="1.2.2.1 张量的形状"></a>1.2.2.1 张量的形状</h4><p>张量具有如下形状属性：</p>
<ul>
<li><code>Tensor.ndim</code>：张量的维度，例如向量的维度为1，矩阵的维度为2。</li>
<li><code>Tensor.shape</code>： 张量每个维度上元素的数量。</li>
<li><code>Tensor.shape[n]</code>：张量第$n$维的大小。第$n$维也称为轴（axis）。</li>
<li><code>Tensor.size</code>：张量中全部元素的个数。</li>
</ul>
<p>为了更好地理解ndim、shape、axis、size四种属性间的区别，创建一个如图1.6所示的四维张量。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/d8461ef0994549a98c1b253f2a31fe60edb7bb200b964b43a9c25818f22a31c6' width='500'></center>

<center>图1.6 形状为[2, 3, 4, 5]的四维张量</center>

<p><br></br></p>
<p>创建一个四维张量，并打印出<code>shape</code>、<code>ndim</code>、<code>shape[n]</code>、<code>size</code>属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ndim_4_Tensor = paddle.ones([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of dimensions:&quot;</span>, ndim_4_Tensor.ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape of Tensor:&quot;</span>, ndim_4_Tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Elements number along axis 0 of Tensor:&quot;</span>, ndim_4_Tensor.shape[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Elements number along the last axis of Tensor:&quot;</span>, ndim_4_Tensor.shape[-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of elements in Tensor: &#x27;</span>, ndim_4_Tensor.size)</span><br></pre></td></tr></table></figure>
<pre><code>Number of dimensions: 4
Shape of Tensor: [2, 3, 4, 5]
Elements number along axis 0 of Tensor: 2
Elements number along the last axis of Tensor: 5
Number of elements in Tensor:  120
</code></pre><h4 id="1-2-2-2-形状的改变"><a href="#1-2-2-2-形状的改变" class="headerlink" title="1.2.2.2 形状的改变"></a>1.2.2.2 形状的改变</h4><p>除了查看张量的形状外，重新设置张量的在实际编程中也具有重要意义，飞桨提供了<code>paddle.reshape</code>接口来改变张量的形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个shape为[3,2,5]的三维Tensor</span></span><br><span class="line">ndim_3_Tensor = paddle.to_tensor([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">                                   [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]],</span><br><span class="line">                                  [[<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">                                   [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>]],</span><br><span class="line">                                  [[<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>],</span><br><span class="line">                                   [<span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>]]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;the shape of ndim_3_Tensor:&quot;</span>, ndim_3_Tensor.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># paddle.reshape 可以保持在输入数据不变的情况下，改变数据形状。这里我们设置reshape为[2,5,3]</span></span><br><span class="line">reshape_Tensor = paddle.reshape(ndim_3_Tensor, [<span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After reshape:&quot;</span>, reshape_Tensor)</span><br></pre></td></tr></table></figure>
<pre><code>the shape of ndim_3_Tensor: [3, 2, 5]
After reshape: Tensor(shape=[2, 5, 3], dtype=int64, place=CPUPlace, stop_gradient=True,
       [[[1 , 2 , 3 ],
         [4 , 5 , 6 ],
         [7 , 8 , 9 ],
         [10, 11, 12],
         [13, 14, 15]],

        [[16, 17, 18],
         [19, 20, 21],
         [22, 23, 24],
         [25, 26, 27],
         [28, 29, 30]]])
</code></pre><p>从输出结果看，将张量从[3, 2, 5]的形状reshape为[2, 5, 3]的形状时，张量内的数据不会发生改变，元素顺序也没有发生改变，只有数据形状发生了改变。</p>
<hr>
<p><strong>笔记</strong></p>
<p>使用reshape时存在一些技巧，比如：</p>
<ul>
<li>-1表示这个维度的值是从张量的元素总数和剩余维度推断出来的。因此，有且只有一个维度可以被设置为-1。</li>
<li>0表示实际的维数是从张量的对应维数中复制出来的，因此shape中0所对应的索引值不能超过张量的总维度。</li>
</ul>
<hr>
<p>分别对上文定义的ndim_3_Tensor进行reshape为[-1]和reshape为[0, 5, 2]两种操作，观察新张量的形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">new_Tensor1 = ndim_3_Tensor.reshape([-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;new Tensor 1 shape: &#x27;</span>, new_Tensor1.shape)</span><br><span class="line">new_Tensor2 = ndim_3_Tensor.reshape([<span class="number">0</span>, <span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;new Tensor 2 shape: &#x27;</span>, new_Tensor2.shape)</span><br></pre></td></tr></table></figure>
<pre><code>new Tensor 1 shape:  [30]
new Tensor 2 shape:  [3, 5, 2]
</code></pre><p>从输出结果看，第一行代码中的第一个reshape操作将张量<code>reshape</code>为元素数量为30的一维向量；第四行代码中的第二个<code>reshape</code>操作中，0对应的维度的元素个数与原张量在该维度上的元素个数相同。</p>
<p>除使用<code>paddle.reshape</code>进行张量形状的改变外，还可以通过<code>paddle.unsqueeze</code>将张量中的一个或多个维度中插入尺寸为1的维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ones_Tensor = paddle.ones([<span class="number">5</span>, <span class="number">10</span>])</span><br><span class="line">new_Tensor1 = paddle.unsqueeze(ones_Tensor, axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;new Tensor 1 shape: &#x27;</span>, new_Tensor1.shape)</span><br><span class="line">new_Tensor2 = paddle.unsqueeze(ones_Tensor, axis=[<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;new Tensor 2 shape: &#x27;</span>, new_Tensor2.shape)</span><br></pre></td></tr></table></figure>
<pre><code>new Tensor 1 shape:  [1, 5, 10]
new Tensor 2 shape:  [5, 1, 1, 10]
</code></pre><h4 id="1-2-2-3-张量的数据类型"><a href="#1-2-2-3-张量的数据类型" class="headerlink" title="1.2.2.3 张量的数据类型"></a>1.2.2.3 张量的数据类型</h4><p>飞桨中可以通过<code>Tensor.dtype</code>来查看张量的数据类型，类型支持bool、float16、float32、float64、uint8、int8、int16、int32、int64和复数类型数据。</p>
<p>1）通过Python元素创建的张量，可以通过dtype来指定数据类型，如果未指定：</p>
<ul>
<li>对于Python整型数据，则会创建int64型张量。</li>
<li>对于Python浮点型数据，默认会创建float32型张量。</li>
</ul>
<p>2）通过Numpy数组创建的张量，则与其原来的数据类型保持相同。通过<code>paddle.to_tensor()</code>函数可以将Numpy数组转化为张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用paddle.to_tensor通过已知数据来创建一个Tensor</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor dtype from Python integers:&quot;</span>, paddle.to_tensor(<span class="number">1</span>).dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor dtype from Python floating point:&quot;</span>, paddle.to_tensor(<span class="number">1.0</span>).dtype)</span><br></pre></td></tr></table></figure>
<pre><code>Tensor dtype from Python integers: paddle.int64
Tensor dtype from Python floating point: paddle.float32
</code></pre><p>如果想改变张量的数据类型，可以通过调用<code>paddle.cast</code>API来实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义dtype为float32的Tensor</span></span><br><span class="line">float32_Tensor = paddle.to_tensor(<span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># paddle.cast可以将输入数据的数据类型转换为指定的dtype并输出。支持输出和输入数据类型相同。</span></span><br><span class="line">int64_Tensor = paddle.cast(float32_Tensor, dtype=<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor after cast to int64:&quot;</span>, int64_Tensor.dtype)</span><br></pre></td></tr></table></figure>
<pre><code>Tensor after cast to int64: paddle.int64
</code></pre><h4 id="1-2-2-4-张量的设备位置"><a href="#1-2-2-4-张量的设备位置" class="headerlink" title="1.2.2.4 张量的设备位置"></a>1.2.2.4 张量的设备位置</h4><p>初始化张量时可以通过place来指定其分配的设备位置，可支持的设备位置有三种：CPU、GPU和固定内存。</p>
<p>固定内存也称为不可分页内存或锁页内存，它与GPU之间具有更高的读写效率，并且支持异步传输，这对网络整体性能会有进一步提升，但它的缺点是分配空间过多时可能会降低主机系统的性能，因为它减少了用于存储虚拟内存数据的可分页内存。当未指定设备位置时，张量默认设备位置和安装的飞桨版本一致，如安装了GPU版本的飞桨，则设备位置默认为GPU。</p>
<p>如下代码分别创建了CPU、GPU和固定内存上的张量，并通过<code>Tensor.place</code>查看张量所在的设备位置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建CPU上的Tensor</span></span><br><span class="line">cpu_Tensor = paddle.to_tensor(<span class="number">1</span>, place=paddle.CPUPlace())</span><br><span class="line"><span class="comment"># 通过Tensor.place查看张量所在设备位置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;cpu Tensor: &#x27;</span>, cpu_Tensor.place)</span><br><span class="line"><span class="comment"># 创建GPU上的Tensor</span></span><br><span class="line">gpu_Tensor = paddle.to_tensor(<span class="number">1</span>, place=paddle.CUDAPlace(<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;gpu Tensor: &#x27;</span>, gpu_Tensor.place)</span><br><span class="line"><span class="comment"># 创建固定内存上的Tensor</span></span><br><span class="line">pin_memory_Tensor = paddle.to_tensor(<span class="number">1</span>, place=paddle.CUDAPinnedPlace())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;pin memory Tensor: &#x27;</span>, pin_memory_Tensor.place)</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cpu Tensor:  CPUPlace</span><br><span class="line">gpu Tensor:  CUDAPlace(0)</span><br><span class="line">pin memory Tensor:  CUDAPinnedPlace</span><br></pre></td></tr></table></figure>
<h3 id="1-2-3-张量与Numpy数组转换"><a href="#1-2-3-张量与Numpy数组转换" class="headerlink" title="1.2.3 张量与Numpy数组转换"></a>1.2.3 张量与Numpy数组转换</h3><p>张量和Numpy数组可以相互转换。第1.2.2.3节中我们了解到paddle.to_tensor()函数可以将Numpy数组转化为张量，也可以通过<code>Tensor.numpy()</code>函数将张量转化为Numpy数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ndim_1_Tensor = paddle.to_tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line"><span class="comment"># 将当前 Tensor 转化为 numpy.ndarray</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Tensor to convert: &#x27;</span>, ndim_1_Tensor.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Tensor to convert:  [1. 2.]
</code></pre><h3 id="1-2-4-张量的访问"><a href="#1-2-4-张量的访问" class="headerlink" title="1.2.4 张量的访问"></a>1.2.4 张量的访问</h3><h4 id="1-2-4-1-索引和切片"><a href="#1-2-4-1-索引和切片" class="headerlink" title="1.2.4.1 索引和切片"></a>1.2.4.1 索引和切片</h4><p>我们可以通过索引或切片方便地访问或修改张量。飞桨使用标准的Python索引规则与Numpy索引规则，具有以下特点：</p>
<ul>
<li>基于$0-n$的下标进行索引，如果下标为负数，则从尾部开始计算。</li>
<li>通过冒号“:”分隔切片参数start:stop:step来进行切片操作，也就是访问start到stop范围内的部分元素并生成一个新的序列。其中start为切片的起始位置，stop为切片的截止位置，step是切片的步长，这三个参数均可缺省。</li>
</ul>
<h4 id="1-2-4-2-访问张量"><a href="#1-2-4-2-访问张量" class="headerlink" title="1.2.4.2 访问张量"></a>1.2.4.2 访问张量</h4><p>针对一维张量，对单个轴进行索引和切片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义1个一维Tensor</span></span><br><span class="line">ndim_1_Tensor = paddle.to_tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Origin Tensor:&quot;</span>, ndim_1_Tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First element:&quot;</span>, ndim_1_Tensor[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Last element:&quot;</span>, ndim_1_Tensor[-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;All element:&quot;</span>, ndim_1_Tensor[:])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Before 3:&quot;</span>, ndim_1_Tensor[:<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Interval of 3:&quot;</span>, ndim_1_Tensor[::<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reverse:&quot;</span>, ndim_1_Tensor[::-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Origin Tensor: Tensor(shape=[9], dtype=int64, place=CPUPlace, stop_gradient=True,
       [0, 1, 2, 3, 4, 5, 6, 7, 8])
First element: Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
       [0])
Last element: Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
       [8])
All element: Tensor(shape=[9], dtype=int64, place=CPUPlace, stop_gradient=True,
       [0, 1, 2, 3, 4, 5, 6, 7, 8])
Before 3: Tensor(shape=[3], dtype=int64, place=CPUPlace, stop_gradient=True,
       [0, 1, 2])
Interval of 3: Tensor(shape=[3], dtype=int64, place=CPUPlace, stop_gradient=True,
       [0, 3, 6])
Reverse: Tensor(shape=[9], dtype=int64, place=CPUPlace, stop_gradient=True,
       [8, 7, 6, 5, 4, 3, 2, 1, 0])
</code></pre><p>针对二维及以上维度的张量，在多个维度上进行索引或切片。索引或切片的第一个值对应第0维，第二个值对应第1维，以此类推，如果某个维度上未指定索引，则默认为“:”。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义1个二维Tensor</span></span><br><span class="line">ndim_2_Tensor = paddle.to_tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">                                  [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Origin Tensor:&quot;</span>, ndim_2_Tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First row:&quot;</span>, ndim_2_Tensor[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First row:&quot;</span>, ndim_2_Tensor[<span class="number">0</span>, :])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First column:&quot;</span>, ndim_2_Tensor[:, <span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Last column:&quot;</span>, ndim_2_Tensor[:, -<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;All element:&quot;</span>, ndim_2_Tensor[:])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First row and second column:&quot;</span>, ndim_2_Tensor[<span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Origin Tensor: Tensor(shape=[3, 4], dtype=int64, place=CPUPlace, stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11]])
First row: Tensor(shape=[4], dtype=int64, place=CPUPlace, stop_gradient=True,
       [0, 1, 2, 3])
First row: Tensor(shape=[4], dtype=int64, place=CPUPlace, stop_gradient=True,
       [0, 1, 2, 3])
First column: Tensor(shape=[3], dtype=int64, place=CPUPlace, stop_gradient=True,
       [0, 4, 8])
Last column: Tensor(shape=[3], dtype=int64, place=CPUPlace, stop_gradient=True,
       [3 , 7 , 11])
All element: Tensor(shape=[3, 4], dtype=int64, place=CPUPlace, stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11]])
First row and second column: Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
       [1])
</code></pre><h4 id="1-2-4-3-修改张量"><a href="#1-2-4-3-修改张量" class="headerlink" title="1.2.4.3 修改张量"></a>1.2.4.3 修改张量</h4><p>与访问张量类似，可以在单个或多个轴上通过索引或切片操作来修改张量。</p>
<hr>
<p><strong>提醒</strong><br>慎重通过索引或切片操作来修改张量，此操作仅会原地修改该张量的数值，且原值不会被保存。如果被修改的张量参与梯度计算，将仅会使用修改后的数值，这可能会给梯度计算引入风险。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义1个二维Tensor</span></span><br><span class="line">ndim_2_Tensor = paddle.ones([<span class="number">2</span>, <span class="number">3</span>], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Origin Tensor: &#x27;</span>, ndim_2_Tensor)</span><br><span class="line"><span class="comment"># 修改第1维为0</span></span><br><span class="line">ndim_2_Tensor[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;change Tensor: &#x27;</span>, ndim_2_Tensor)</span><br><span class="line"><span class="comment"># 修改第1维为2.1</span></span><br><span class="line">ndim_2_Tensor[<span class="number">0</span>:<span class="number">1</span>] = <span class="number">2.1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;change Tensor: &#x27;</span>, ndim_2_Tensor)</span><br><span class="line"><span class="comment"># 修改全部Tensor</span></span><br><span class="line">ndim_2_Tensor[...] = <span class="number">3</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;change Tensor: &#x27;</span>, ndim_2_Tensor)</span><br></pre></td></tr></table></figure>
<pre><code>Origin Tensor:  Tensor(shape=[2, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[1., 1., 1.],
        [1., 1., 1.]])
change Tensor:  Tensor(shape=[2, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[0., 0., 0.],
        [1., 1., 1.]])
change Tensor:  Tensor(shape=[2, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[2.09999990, 2.09999990, 2.09999990],
        [1.        , 1.        , 1.        ]])
change Tensor:  Tensor(shape=[2, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[3., 3., 3.],
        [3., 3., 3.]])
</code></pre><h3 id="1-2-5-张量的运算"><a href="#1-2-5-张量的运算" class="headerlink" title="1.2.5 张量的运算"></a>1.2.5 张量的运算</h3><p>张量支持包括基础数学运算、逻辑运算、矩阵运算等100余种运算操作，以加法为例，有如下两种实现方式：<br>1）使用飞桨API <code>paddle.add(x,y)</code>。<br>2）使用张量类成员函数<code>x.add(y)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义两个Tensor</span></span><br><span class="line">x = paddle.to_tensor([[<span class="number">1.1</span>, <span class="number">2.2</span>], [<span class="number">3.3</span>, <span class="number">4.4</span>]], dtype=<span class="string">&quot;float64&quot;</span>)</span><br><span class="line">y = paddle.to_tensor([[<span class="number">5.5</span>, <span class="number">6.6</span>], [<span class="number">7.7</span>, <span class="number">8.8</span>]], dtype=<span class="string">&quot;float64&quot;</span>)</span><br><span class="line"><span class="comment"># 第一种调用方法，paddle.add逐元素相加算子，并将各个位置的输出元素保存到返回结果中</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Method 1: &#x27;</span>, paddle.add(x, y))</span><br><span class="line"><span class="comment"># 第二种调用方法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Method 2: &#x27;</span>, x.add(y))</span><br></pre></td></tr></table></figure>
<pre><code>Method 1:  Tensor(shape=[2, 2], dtype=float64, place=CPUPlace, stop_gradient=True,
       [[6.60000000 , 8.80000000 ],
        [11.        , 13.20000000]])
Method 2:  Tensor(shape=[2, 2], dtype=float64, place=CPUPlace, stop_gradient=True,
       [[6.60000000 , 8.80000000 ],
        [11.        , 13.20000000]])
</code></pre><p>从输出结果看，使用张量类成员函数和飞桨API具有相同的效果。</p>
<hr>
<p><strong>笔记</strong><br>由于张量类成员函数操作更为方便，以下均从张量类成员函数的角度，对常用张量操作进行介绍。</p>
<hr>
<hr>
<p><strong>笔记</strong><br>更多张量操作相关的API，请参考<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html">飞桨官方文档</a>。</p>
<hr>
<h4 id="1-2-5-1-数学运算"><a href="#1-2-5-1-数学运算" class="headerlink" title="1.2.5.1 数学运算"></a>1.2.5.1 数学运算</h4><p>张量类的基础数学函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x.<span class="built_in">abs</span>()                       <span class="comment"># 逐元素取绝对值</span></span><br><span class="line">x.ceil()                      <span class="comment"># 逐元素向上取整</span></span><br><span class="line">x.floor()                     <span class="comment"># 逐元素向下取整</span></span><br><span class="line">x.<span class="built_in">round</span>()                     <span class="comment"># 逐元素四舍五入</span></span><br><span class="line">x.exp()                       <span class="comment"># 逐元素计算自然常数为底的指数</span></span><br><span class="line">x.log()                       <span class="comment"># 逐元素计算x的自然对数</span></span><br><span class="line">x.reciprocal()                <span class="comment"># 逐元素求倒数</span></span><br><span class="line">x.square()                    <span class="comment"># 逐元素计算平方</span></span><br><span class="line">x.sqrt()                      <span class="comment"># 逐元素计算平方根</span></span><br><span class="line">x.sin()                       <span class="comment"># 逐元素计算正弦</span></span><br><span class="line">x.cos()                       <span class="comment"># 逐元素计算余弦</span></span><br><span class="line">x.add(y)                      <span class="comment"># 逐元素加</span></span><br><span class="line">x.subtract(y)                 <span class="comment"># 逐元素减</span></span><br><span class="line">x.multiply(y)                 <span class="comment"># 逐元素乘（积）</span></span><br><span class="line">x.divide(y)                   <span class="comment"># 逐元素除</span></span><br><span class="line">x.mod(y)                      <span class="comment"># 逐元素除并取余</span></span><br><span class="line">x.<span class="built_in">pow</span>(y)                      <span class="comment"># 逐元素幂</span></span><br><span class="line">x.<span class="built_in">max</span>()                       <span class="comment"># 指定维度上元素最大值，默认为全部维度</span></span><br><span class="line">x.<span class="built_in">min</span>()                       <span class="comment"># 指定维度上元素最小值，默认为全部维度</span></span><br><span class="line">x.prod()                      <span class="comment"># 指定维度上元素累乘，默认为全部维度</span></span><br><span class="line">x.<span class="built_in">sum</span>()                       <span class="comment"># 指定维度上元素的和，默认为全部维度</span></span><br></pre></td></tr></table></figure>
<p>同时，为了更方便地使用张量，飞桨对Python数学运算相关的魔法函数进行了重写，以下操作与上述结果相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x + y  -&gt; x.add(y)            <span class="comment"># 逐元素加</span></span><br><span class="line">x - y  -&gt; x.subtract(y)       <span class="comment"># 逐元素减</span></span><br><span class="line">x * y  -&gt; x.multiply(y)       <span class="comment"># 逐元素乘（积）</span></span><br><span class="line">x / y  -&gt; x.divide(y)         <span class="comment"># 逐元素除</span></span><br><span class="line">x % y  -&gt; x.mod(y)            <span class="comment"># 逐元素除并取余</span></span><br><span class="line">x ** y -&gt; x.<span class="built_in">pow</span>(y)            <span class="comment"># 逐元素幂</span></span><br></pre></td></tr></table></figure>
<h4 id="1-2-5-2-逻辑运算"><a href="#1-2-5-2-逻辑运算" class="headerlink" title="1.2.5.2 逻辑运算"></a>1.2.5.2 逻辑运算</h4><p>张量类的逻辑运算函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x.isfinite()                  <span class="comment"># 判断Tensor中元素是否是有限的数字，即不包括inf与nan</span></span><br><span class="line">x.equal_all(y)                <span class="comment"># 判断两个Tensor的全部元素是否相等，并返回形状为[1]的布尔类Tensor</span></span><br><span class="line">x.equal(y)                    <span class="comment"># 判断两个Tensor的每个元素是否相等，并返回形状相同的布尔类Tensor</span></span><br><span class="line">x.not_equal(y)                <span class="comment"># 判断两个Tensor的每个元素是否不相等</span></span><br><span class="line">x.less_than(y)                <span class="comment"># 判断Tensor x的元素是否小于Tensor y的对应元素</span></span><br><span class="line">x.less_equal(y)               <span class="comment"># 判断Tensor x的元素是否小于或等于Tensor y的对应元素</span></span><br><span class="line">x.greater_than(y)             <span class="comment"># 判断Tensor x的元素是否大于Tensor y的对应元素</span></span><br><span class="line">x.greater_equal(y)            <span class="comment"># 判断Tensor x的元素是否大于或等于Tensor y的对应元素</span></span><br><span class="line">x.allclose(y)                 <span class="comment"># 判断两个Tensor的全部元素是否接近</span></span><br></pre></td></tr></table></figure>
<p>同样地，飞桨对Python逻辑比较相关的魔法函数也进行了重写，这里不再赘述。</p>
<h4 id="1-2-5-3-矩阵运算"><a href="#1-2-5-3-矩阵运算" class="headerlink" title="1.2.5.3 矩阵运算"></a>1.2.5.3 矩阵运算</h4><p>张量类还包含了矩阵运算相关的函数，如矩阵的转置、范数计算和乘法等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x.t()                         <span class="comment"># 矩阵转置</span></span><br><span class="line">x.transpose([<span class="number">1</span>, <span class="number">0</span>])           <span class="comment"># 交换第 0 维与第 1 维的顺序</span></span><br><span class="line">x.norm(<span class="string">&#x27;fro&#x27;</span>)                 <span class="comment"># 矩阵的弗罗贝尼乌斯范数</span></span><br><span class="line">x.dist(y, p=<span class="number">2</span>)                <span class="comment"># 矩阵（x-y）的2范数</span></span><br><span class="line">x.matmul(y)                   <span class="comment"># 矩阵乘法</span></span><br></pre></td></tr></table></figure>
<p>有些矩阵运算中也支持大于两维的张量，比如matmul函数，对最后两个维度进行矩阵乘。比如x是形状为[j,k,n,m]的张量，另一个y是[j,k,m,p]的张量，则x.matmul(y)输出的张量形状为[j,k,n,p]。</p>
<h4 id="1-2-5-4-广播机制"><a href="#1-2-5-4-广播机制" class="headerlink" title="1.2.5.4 广播机制"></a>1.2.5.4 广播机制</h4><p>飞桨的一些API在计算时支持广播(Broadcasting)机制，允许在一些运算时使用不同形状的张量。通常来讲，如果有一个形状较小和一个形状较大的张量，会希望多次使用较小的张量来对较大的张量执行某些操作，看起来像是形状较小的张量首先被扩展到和较大的张量形状一致，然后再做运算。</p>
<h5 id="广播机制的条件"><a href="#广播机制的条件" class="headerlink" title="广播机制的条件"></a>广播机制的条件</h5><p>飞桨的广播机制主要遵循如下规则（参考Numpy广播机制）：</p>
<p>1）每个张量至少为一维张量。</p>
<p>2）从后往前比较张量的形状，当前维度的大小要么相等，要么其中一个等于1，要么其中一个不存在。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当两个Tensor的形状一致时，可以广播</span></span><br><span class="line">x = paddle.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">y = paddle.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">z = x + y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;broadcasting with two same shape tensor: &#x27;</span>, z.shape)</span><br><span class="line"></span><br><span class="line">x = paddle.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">y = paddle.ones((<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 从后往前依次比较：</span></span><br><span class="line"><span class="comment"># 第一次：y的维度大小是1</span></span><br><span class="line"><span class="comment"># 第二次：x的维度大小是1</span></span><br><span class="line"><span class="comment"># 第三次：x和y的维度大小相等，都为3</span></span><br><span class="line"><span class="comment"># 第四次：y的维度不存在</span></span><br><span class="line"><span class="comment"># 所以x和y是可以广播的</span></span><br><span class="line">z = x + y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;broadcasting with two different shape tensor:&#x27;</span>, z.shape)</span><br></pre></td></tr></table></figure>
<pre><code>broadcasting with two same shape tensor:  [2, 3, 4]
broadcasting with two different shape tensor: [2, 3, 4, 5]
</code></pre><ul>
<li>从输出结果看，x与y在上述两种情况中均遵循广播规则，因此在张量相加时可以广播。我们再定义两个shape分别为[2, 3, 4]和[2, 3, 6]的张量，观察这两个张量是否能够通过广播操作相加。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">y = paddle.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>))</span><br><span class="line">z = x + y</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: (InvalidArgument) Broadcast dimension mismatch.</span><br></pre></td></tr></table></figure>
<p>从输出结果看，此时x和y是不能广播的，因为在第一次从后往前的比较中，4和6不相等，不符合广播规则。</p>
<h5 id="广播机制的计算规则"><a href="#广播机制的计算规则" class="headerlink" title="广播机制的计算规则"></a>广播机制的计算规则</h5><p>现在我们知道在什么情况下两个张量是可以广播的。两个张量进行广播后的结果张量的形状计算规则如下：</p>
<p>1）如果两个张量shape的长度不一致，那么需要在较小长度的shape前添加1，直到两个张量的形状长度相等。</p>
<p>2） 保证两个张量形状相等之后，每个维度上的结果维度就是当前维度上较大的那个。</p>
<p>以张量x和y进行广播为例，x的shape为[2, 3, 1，5]，张量y的shape为[3，4，1]。首先张量y的形状长度较小，因此要将该张量形状补齐为[1, 3, 4, 1]，再对两个张量的每一维进行比较。从第一维看，x在一维上的大小为2，y为1，因此，结果张量在第一维的大小为2。以此类推，对每一维进行比较，得到结果张量的形状为[2, 3, 4, 5]。</p>
<p>由于矩阵乘法函数paddle.matmul在深度学习中使用非常多，这里需要特别说明一下它的广播规则：</p>
<p>1）如果两个张量均为一维，则获得点积结果。</p>
<p>2） 如果两个张量都是二维的，则获得矩阵与矩阵的乘积。</p>
<p>3） 如果张量x是一维，y是二维，则将x的shape转换为[1, D]，与y进行矩阵相乘后再删除前置尺寸。</p>
<p>4） 如果张量x是二维，y是一维，则获得矩阵与向量的乘积。</p>
<p>5） 如果两个张量都是N维张量（N &gt; 2），则根据广播规则广播非矩阵维度（除最后两个维度外其余维度）。比如：如果输入x是形状为[j,1,n,m]的张量，另一个y是[k,m,p]的张量，则输出张量的形状为[j,k,n,p]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.ones([<span class="number">10</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line">y = paddle.ones([<span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>])</span><br><span class="line">z = paddle.matmul(x, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After matmul: &#x27;</span>, z.shape)</span><br></pre></td></tr></table></figure>
<pre><code>After matmul:  [10, 3, 5, 5]
</code></pre><p>从输出结果看，计算张量乘积时会使用到广播机制。</p>
<hr>
<p><strong>笔记</strong><br>飞桨的API有原位（inplace）操作和非原位操作之分。原位操作即在原张量上保存操作结果，非原位操作则不会修改原张量，而是返回一个新的张量来表示运算结果。在飞桨框架V2.1及之后版本，部分API有对应的原位操作版本，在API后加上’_‘表示，如：<code>x.add(y)</code>是非原位操作，<code>x.add_(y)</code>为原位操作。</p>
<hr>
<hr>
<p><strong>动手练习1.2</strong><br>尝试和熟悉本节中的各种张量运算，特别是掌握张量计算时的广播机制。</p>
<hr>
<h2 id="1-3-算子"><a href="#1-3-算子" class="headerlink" title="1.3 算子"></a>1.3 算子</h2><p>一个复杂的机器学习模型（比如神经网络）可以看作一个复合函数，输入是数据特征，输出是标签的值或概率。简单起见，假设一个由$L$个函数复合的神经网络定义为：</p>
<script type="math/tex; mode=display">
y=f_L(\cdots f_2(f_1(x))),</script><p>其中$f_l(\cdot)$可以为带参数的函数，也可以为不带参数的函数，$x$为输入特征，$y$为某种损失。<br>我们将从$x$到$y$的计算看作一个前向计算过程。而神经网络的参数学习需要计算损失关于所有参数的偏导数（即梯度）。假设函数$f_l(\cdot)$包含参数$\theta_l$，根据链式法则，</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial y}{\partial \theta_l} &= {\frac{\partial f_l}{\partial \theta_l}} \frac{\partial y}{\partial f_l} \\
&= \frac{\partial f_l}{\partial \theta_l} \frac{\partial f_{l+1}}{\partial f_l} \cdots \frac{\partial f_L}{\partial f_{L-1}} .
\end{aligned}</script><p>在实践中，一种比较高效的计算$y$关于每个函数$f_l$的偏导数的方式是利用递归进行反向计算。令$\delta_l\triangleq \frac{\partial y}{\partial f_l}$，则有</p>
<script type="math/tex; mode=display">
\delta_{l-1} = \frac{\partial f_l}{\partial f_{1-1}} \delta_{l}.</script><p>如果将函数$f_l(\cdot)$称为前向函数，则$\delta_{l-1}$的计算称为函数$f(x)$的反向函数。</p>
<p>如果我们实现每个基础函数的前向函数和反向函数，就可以非常方便地通过这些基础函数组合出复杂函数，并通过链式法则反向计算复杂函数的偏导数。<br>在深度学习框架中，这些基本函数的实现称为算子(Operator，Op)。有了算子，就可以像搭积木一样构建复杂的模型。</p>
<h3 id="1-3-1-算子定义"><a href="#1-3-1-算子定义" class="headerlink" title="1.3.1 算子定义"></a>1.3.1 算子定义</h3><p>算子是构建复杂机器学习模型的基础组件，包含一个函数$f(x)$的前向函数和反向函数。为了可以更便捷地进行算子组合，本书中定义算子Op}的接口如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Op</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向函数</span></span><br><span class="line">    <span class="comment"># 输入：张量inputs</span></span><br><span class="line">    <span class="comment"># 输出：张量outputs</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># return outputs</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向函数</span></span><br><span class="line">    <span class="comment"># 输入：最终输出对outputs的梯度outputs_grads</span></span><br><span class="line">    <span class="comment"># 输出：最终输出对inputs的梯度inputs_grads</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, outputs_grads</span>):</span><br><span class="line">        <span class="comment"># return inputs_grads</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<p>在上面的接口中，<code>forward</code>是自定义Op的前向函数，必须被子类重写，它的参数为输入对象，参数的类型和数量任意；<code>backward</code>是自定义Op的反向函数，必须被子类重写，它的参数为<code>forward</code>输出张量的梯度<code>outputs_grads</code>，它的输出为<code>forward</code>输入张量的梯度<code>inputs_grads</code>。</p>
<hr>
<p><strong>笔记</strong><br>在飞桨中，可以直接调用模型的<code>forward()</code>方法进行前向执行，也可以调用<code>__call__</code>，从而执行在<code>forward()</code>中定义的前向计算逻辑。</p>
<hr>
<p>下面以$g = \exp(a \times b+c \times d)$为例，分别实现加法、乘法和指数运算三个算子，通过算子组合计算$y$值。</p>
<h4 id="1-3-1-1-加法算子"><a href="#1-3-1-1-加法算子" class="headerlink" title="1.3.1.1 加法算子"></a>1.3.1.1 加法算子</h4><p>图1.7展示了加法算子的前反向计算过程。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/308b6d78396b4db0a3df548dcb4de971900d90bdc34a481b9211260cd25c9377' width='500'></center>
<center>图1.7 加法算子的前反向函数</center>

<p><br></br></p>
<h5 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h5><p>当进行前向计算时，加法计算输出$z=x+y$。</p>
<h5 id="反向计算"><a href="#反向计算" class="headerlink" title="反向计算"></a>反向计算</h5><p>假设经过一个其他操作后，最终输出为$L$，令$\delta_z=\frac{\partial L}{\partial z}$，$\delta_x=\frac{\partial L}{\partial x}$，$\delta_y=\frac{\partial L}{\partial y}$。<br>加法算子的反向计算的输入是梯度$\delta_z$，输出是梯度$\delta_x$和$\delta_y$。</p>
<p>根据链式法则，$\delta_x =  \delta_z \times 1$，$\delta_y =  \delta_z \times 1$。<br>加法算子的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">add</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(add, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        outputs = x + y</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grads</span>):</span><br><span class="line">        grads_x = grads * <span class="number">1</span></span><br><span class="line">        grads_y = grads * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> grads_x, grads_y</span><br></pre></td></tr></table></figure>
<p>定义$x=1$、$y=4$，根据反向计算，得到$x$、$y$的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">1</span></span><br><span class="line">y = <span class="number">4</span></span><br><span class="line">add_op = add()</span><br><span class="line">z = add_op(x, y)</span><br><span class="line">grads_x, grads_y = add_op.backward(grads=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x&#x27;s grad is: &quot;</span>, grads_x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y&#x27;s grad is: &quot;</span>, grads_y)</span><br></pre></td></tr></table></figure>
<pre><code>x&#39;s grad is:  1
y&#39;s grad is:  1
</code></pre><h4 id="1-3-1-2-乘法算子"><a href="#1-3-1-2-乘法算子" class="headerlink" title="1.3.1.2 乘法算子"></a>1.3.1.2 乘法算子</h4><p>同理，乘法算子的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">multiply</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(multiply, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        outputs = x * y</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grads</span>):</span><br><span class="line">        grads_x = grads * self.y</span><br><span class="line">        grads_y = grads * self.x</span><br><span class="line">        <span class="keyword">return</span> grads_x, grads_y</span><br></pre></td></tr></table></figure>
<h4 id="1-3-1-3-指数算子"><a href="#1-3-1-3-指数算子" class="headerlink" title="1.3.1.3 指数算子"></a>1.3.1.3 指数算子</h4><p>同理，指数算子的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">exponential</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(exponential, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        outputs = math.exp(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grads</span>):</span><br><span class="line">        grads = grads * math.exp(self.x)</span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>分别指定$a、b、c、d$的值，通过实例化算子，调用加法、乘法和指数运算算子，计算得到$y$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a, b, c, d = <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span></span><br><span class="line"><span class="comment"># 实例化算子</span></span><br><span class="line">multiply_op = multiply()</span><br><span class="line">add_op = add()</span><br><span class="line">exp_op = exponential()</span><br><span class="line">y = exp_op(add_op(multiply_op(a, b), multiply_op(c, d)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y: &#x27;</span>, y)</span><br></pre></td></tr></table></figure>
<pre><code>y:  22026.465794806718
</code></pre><hr>
<p><strong>动手练习1.3</strong> </p>
<p>执行上述算子的反向过程，并验证梯度是否正确。</p>
<hr>
<h3 id="1-3-2-自动微分机制"><a href="#1-3-2-自动微分机制" class="headerlink" title="1.3.2 自动微分机制"></a>1.3.2 自动微分机制</h3><p>目前大部分深度学习平台都支持自动微分（Automatic Differentiation），即根据<code>forward()</code>函数来自动构建<code>backward()</code>函数。</p>
<hr>
<p><strong>笔记</strong>  </p>
<p>自动微分的原理是将所有的数值计算都分解为基本的原子操作，并构建\mykey{计算图}{Computational Graph}。计算图上每个节点都是一个原子操作，保留前向和反向的计算结果，很方便通过链式法则来计算梯度。自动微分的详细介绍可以参考《神经网络与深度学习》第4.5节。</p>
<hr>
<p>飞桨的自动微分是通过<code>trace</code>的方式，记录各种算子和张量的前向计算，并自动创建相应的反向函数和反向变量，来实现反向梯度的计算。</p>
<hr>
<p><strong>笔记</strong>  </p>
<p>在飞桨中，可以通<code>过paddle.grad()</code>API或张量类成员函数x.grad来查看张量的梯度。</p>
<hr>
<p>下面用一个比较简单的例子来了解整个过程。定义两个张量a和b，并用<code>stop_gradient</code>属性用来设置是否传递梯度。将a的<code>stop_gradient</code>属性设为False，会自动为a创建一个反向张量，将b的<code>stop_gradient</code>属性设为True，即不会为b创建反向张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义张量a，stop_gradient=False代表进行梯度传导</span></span><br><span class="line">a = paddle.to_tensor(<span class="number">2.0</span>, stop_gradient=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 定义张量b，stop_gradient=True代表不进行梯度传导</span></span><br><span class="line">b = paddle.to_tensor(<span class="number">5.0</span>, stop_gradient=<span class="literal">True</span>)</span><br><span class="line">c = a * b</span><br><span class="line"><span class="comment"># 自动计算反向梯度</span></span><br><span class="line">c.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor a&#x27;s grad is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(a.grad))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor b&#x27;s grad is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(b.grad))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor c&#x27;s grad is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(c.grad))</span><br></pre></td></tr></table></figure>
<pre><code>Tensor a&#39;s grad is: Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,
       [5.])
Tensor b&#39;s grad is: None
Tensor c&#39;s grad is: Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,
       [1.])


/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py:392: UserWarning: [93m
Warning:
tensor.grad will return the tensor value of the gradient. This is an incompatible upgrade for tensor.grad API.  It&#39;s return type changes from numpy.ndarray in version 2.0 to paddle.Tensor in version 2.1.0.  If you want to get the numpy value of the gradient, you can use :code:`x.grad.numpy()` [0m
  warnings.warn(warning_msg)
</code></pre><p>下面我们解释下上面代码的执行逻辑。</p>
<h4 id="1-3-2-1-前向执行"><a href="#1-3-2-1-前向执行" class="headerlink" title="1.3.2.1 前向执行"></a>1.3.2.1 前向执行</h4><p>在上面代码中，第7行<code>c.backward()</code>被执行前，会为每个张量和算子创建相应的反向张量和反向函数。</p>
<p>当创建张量或执行算子的前向计算时，会自动创建反向张量或反向算子。这里以上面代码中乘法为例来进行说明。</p>
<ol>
<li>当创建张量a时，由于其属性<code>stop_gradient=False</code>，因此会自动为a创建一个反向张量，也就是图1.8中的a_grad。由于a不依赖其它张量或算子，a_grad的<code>grad_op</code>为None。</li>
<li>当创建张量b时，由于其属性<code>stop_gradient=True</code>，因此不会为b创建一个反向张量。</li>
<li>执行乘法$c=a\times b$ 时，$\times$是一个前向算子Mul，为其构建反向算子MulBackward。由于Mul的输入是a和b，输出是c，对应反向算子<code>MulBackward</code>的输入是张量c的反向张量<code>c_grad</code>，输出是a和b的反向张量。如果输入定义<code>stop_gradient=True</code>，反向张量即为None。在此例子中就是a_grad和None。</li>
<li>反向算子MulBackward中的<code>grad_pending_ops</code>用于在自动构建反向网络时，明确该反向算子的下一个可执行的反向算子。可以理解为在反向计算中，该算子衔接的下一个反向算子。</li>
<li>当c通过乘法算子Mul被创建后，c会创建一个反向张量c_grad，它的<code>grad_op</code>为该乘法算子的反向算子，即MulBackward。</li>
</ol>
<p>由于此时还没有进行反向计算，因此这些反向张量和反向算子中的具体数值为空(data = None)。此时，上面代码对应的计算图状态如图1.8所示。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/5b7d98c33a734f2c9ad2da362bd6bb0d37e4427cb4714a55bc21860129cf4620' width='500'></center>
<center>图1.8 调用backward()前的计算图状态</center>

<p><br></br></p>
<h4 id="1-3-2-2-反向执行"><a href="#1-3-2-2-反向执行" class="headerlink" title="1.3.2.2 反向执行"></a>1.3.2.2 反向执行</h4><p>调用<code>backward()</code>后，执行计算图上的反向过程，即通过链式法则自动计算每个张量或算子的微分，计算过程如图1.9所示。经过自动反向梯度计算，获得c_grad和a_grad的值。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/b237b41e7815474885b2e71e19a199ecd4fe58e288bc4dfd8e1f45cfb4e1bfc4' width='500'></center>
<center>图1.9 调用backward()后的计算图状态</center>

<p><br></br></p>
<h3 id="1-3-3-预定义的算子"><a href="#1-3-3-预定义的算子" class="headerlink" title="1.3.3 预定义的算子"></a>1.3.3 预定义的算子</h3><p>从零开始构建各种复杂的算子和模型是一个很复杂的过程，在开发的过程中也难以避免地会出现很多冗余代码，因此飞桨提供了基础算子和中间算子，可以便捷地实现复杂模型。</p>
<p>在深度学习中，大多数模型都是以各种神经网络为主，由一系列层(Layer)组成，层是模型的基础逻辑执行单元。飞桨提供了<code>paddle.nn.Layer</code>类来方便快速地实现自己的层和模型。模型和层都可以基于<code>paddle.nn.Layer</code>扩充实现，模型只是一种特殊的层。</p>
<p>当我们实现的算子继承<code>paddle.nn.Layer</code>类时，就不用再定义<code>backward</code>函数。飞桨的自动微分机制可以自动完成反向传播过程，让我们只关注模型构建的前向过程，不必再进行烦琐的梯度求导。</p>
<h3 id="1-3-4-本书中实现的算子"><a href="#1-3-4-本书中实现的算子" class="headerlink" title="1.3.4 本书中实现的算子"></a>1.3.4 本书中实现的算子</h3><p>更深入的理解深度学习的模型和算法，在本书中，我们也手动实现自己的算子库：nndl，并基于自己的算子库来构建机器学习模型。本书中的自定义算子分为两类：一类是继承在第1.3.1节中定义Op类，这些算子是为了进行更好的展示模型的实现细节，需要自己动手计算并实现其反向函数；另一类是继承飞桨的paddle.nn.Layer类，更方便地搭建复杂算子，并和飞桨预定义算子混合使用。</p>
<p>本书中实现的算子见表1.1所示，其中Model_开头为完整的模型。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/5bc27443b5ab4df99c7578f75ea973e84cab41d1caf949bea84af0b814325621' width='400'></center>
<center>表1.1 本书中使用的算子</center>

<p><br></br></p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/cd5e251e1b9c4f37888af85b4ee9386ffc65fca2d4f4473a8f6adc14e1e3238d' width='400'></center>
<center>表1.1 本书中使用的算子</center>

<p><br></br></p>
<h3 id="1-3-5-本书中实现的优化器"><a href="#1-3-5-本书中实现的优化器" class="headerlink" title="1.3.5 本书中实现的优化器"></a>1.3.5 本书中实现的优化器</h3><p>针对继承Op类的算子的优化，本书还实现了自定义的优化器，见表1.2所示。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/085004d14449417392350eec7404219a803444b4e49b4bbc963bdae4f59d2fcb' width='400'></center>
<center>表1.2 本书中使用的优化器</center>

<p><br></br></p>
<h2 id="1-4-本书中实现的DataSet类"><a href="#1-4-本书中实现的DataSet类" class="headerlink" title="1.4 本书中实现的DataSet类"></a>1.4 本书中实现的DataSet类</h2><p>为了更好地实践，本书在模型解读部分主要使用简单任务和数据集，在案例实践部分主要使用公开的实际案例数据集。下面介绍我们用到的数据集以及对应构建的DataSet类。</p>
<h3 id="1-4-1-数据集"><a href="#1-4-1-数据集" class="headerlink" title="1.4.1 数据集"></a>1.4.1 数据集</h3><p>本书中使用的数据集如下：</p>
<ol>
<li>线性回归数据集ToyLinear150：在第2.2.1.2节中构建，用于简单的线性回归任务。ToyLinear150数据集包含150条带噪音的样本数据，其中训练集100条、测试集50条，由在第2.2.1.2节中create_toy_data函数构建。</li>
<li>非线性回归数据集ToySin25：在第2.3.1节中构建，用于简单的多项式回归任务.ToySin25数据集包含25条样本数据，其中训练集15条、测试集10条。ToySin25数据集同样使用在第2.2.1.1节中create_toy_data函数进行构建。</li>
<li>波士顿房价预测数据集：波士顿房价预测数据集共506条样本数据，每条样本包含了12种可能影响房价的因素和该类房屋价格的中位数。该数据集在第2.5节中使用。</li>
<li>二分类数据集Moon1000：在第3.1中构建，二分类数据集Moon1000数据是从两个带噪音 的弯月形状数据分布中采样得到，每个样本包含2个特征，其中训练集640条、验证集160条、测试集200条。该数据集在本书第3.1节和第4.2节中使用。数据集构建函数make_moons在第7.4.2.3节和第7.6节中使用。</li>
<li>三分类数据集Multi1000：在第3.2.1节中构建三分类数据集集Multi1000，其中训练集640条、验证集160条、测试集200条。该数据集来自三个不同的簇，每个簇对应一个类别。</li>
<li>鸢尾花数据集：鸢尾花数据集包含了3种鸢尾花类别(Setosa、Versicolour、Virginica)，每种类别有50个样本，共计150个样本。每个样本中包含了4个属性：花萼长度、花萼宽度、花瓣长度以及花瓣宽度。该数据集在第3.3节和第4.5节使用。</li>
<li>MNIST数据集：MNIST手写数字识别数据集是计算机视觉领域的经典入门数据集，包含了训练集60 000条、测试集10 000条。MNIST数据集在第5.3.1节和第7.2节中使用。</li>
<li>CIFAR-10 数据集：CIFAR-10数据集是计算机视觉领域的经典数据集，包含了10种不同的类别、共 60 000 张图像，其中每个类别的图像都是6 000 张，图像大小均为32×32像素。CIFAR-10数据集在第5.5节中使用.</li>
<li>IMDB电影评论数据集：IMDB电影评论数据集是一份关于电影评论的经典二分类数据集。IMDB按照评分的高低筛选出了积极评论和消极评论，如果评分≥7，则认为是积极评论；如果评分≤4，则认为是消极评论。数据集包含训练集和测试集数据，数量各为25 000 条，每条数据都是一段用户关于某个电影的真实评价，以及观众对这个电影的情感倾向。IMDB数据集在第6.4节、第8.1节和第8.2节中使用.</li>
<li>数字求和数据集DigitSum：在第6.1.1节中构建，包含用于数字求和任务的不同长度的数据集。数字求和任务的输入是一串数字，前两个位置的数字为0-9，其余数字随机生成(主要为0)，预测目标是输入序列中前两个数字的加和，用来测试模型的对序列数据的记忆能力。</li>
<li>LCQMC通用领域问题匹配数据集：LCQMC数据集是百度知道领域的中文问题匹配数据集，目的是为了解决在中文领域大规模问题匹配数据集的缺失。该数据集从百度知道不同领域的用户问题中抽取构建数据。LCQMC数据集共包含训练集238 766条、验证集8 802条和测试集12 500 条。LCQMC数据集在第8.3节中使用。</li>
</ol>
<h3 id="1-4-2-Dataset类"><a href="#1-4-2-Dataset类" class="headerlink" title="1.4.2 Dataset类"></a>1.4.2 Dataset类</h3><p>为了更好地支持使用随机梯度下降进行参数学习，我们构建了DataSet类，以便可以更好地进行数据迭代。</p>
<p>本书中构建的DataSet类见表1.3。关于Dataset类的具体介绍见第4.5.1.1节。</p>
<center><image src='https://ai-studio-static-online.cdn.bcebos.com/a7fe6d71b8754f5a80bfbd44639508a64024972f4af14c609e4bebf0f2e0d21c' width='300'></center>

<center> 表1.3 本书中构建的DataSet类</center>

<p><br></br></p>
<h2 id="1-5-本书中使用的Runner类"><a href="#1-5-本书中使用的Runner类" class="headerlink" title="1.5 本书中使用的Runner类"></a>1.5 本书中使用的Runner类</h2><p>在一个任务上应用机器学习方法的流程基本上包括：数据集构建、模型构建、损失函数定义、优化器定义、评价指标定义、模型训练、模型评价和模型预测等环节。为了将上述环节规范化，我们将机器学习模型的基本要素封装成一个Runner类，使得我们可以更方便进行机器学习实践。除上述提到的要素外，Runner类还包括模型保存、模型加载等功能。Runner类的具体介绍可参见第2节.<br>这里我们对本书中用到的三个版本的Runner类进行汇总，说明每一个版本Runner类的构成方式.</p>
<ol>
<li>RunnerV1：在第2节中实现，用于线性回归模型的训练，其中训练过程通过直接求解解析解的方式得到模型参数，没有模型优化及计算损失函数过程，模型训练结束后保存模型参数。</li>
<li>RunnerV2：在第3.1.6节中实现。RunnerV2主要增加的功能为：</li>
</ol>
<ul>
<li>在训练过程引入梯度下降法进行模型优化；</li>
<li>模型训练过程中计算在训练集和验证集上的损失及评价指标并打印，训练过程中保存最优模型。我们在第4.3.2节和第4.3.2节分别对RunnerV2进行了完善，加入自定义日志输出、模型阶段控制等功能。</li>
</ul>
<ol>
<li>RunnerV3：在第4.5.4节中实现。RunnerV3主要增加三个功能：使用随机梯度下降法进行参数优化；训练过程使用DataLoader加载批量数据；模型加载与保存中，模型参数使用state_dict方法获取，使用state_dict加载。</li>
</ol>
<h2 id="1-6-小结"><a href="#1-6-小结" class="headerlink" title="1.6 小结"></a>1.6 小结</h2><p>本节介绍了我们在后面实践中需要的一些基础知识。在后续章节中，我们会逐步学习和了解更多的实践知识。此外，如需查阅张量、算子或其他飞桨的知识，可参阅<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/tutorials">飞桨的帮助文档</a>。</p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/08/12/nndl/chapter2A/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          第2章（上）：线性回归理论解读
        
      </div>
    </a>
  
  
    <a href="/2022/08/11/notes/How-does-GAN-work/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">How does GAN work?</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2022 Pokemon Master
      </div>
        <div class="footer-right">
          <a href="https://space.bilibili.com/782159" target="_blank">访问我的哔哩哔哩</a> 
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: ,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":170,"height":340},"mobile":{"show":true},"log":false});</script></body>
</html>