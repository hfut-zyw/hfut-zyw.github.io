<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Pytorch Tutorial | Homepage | 张有为</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="个人笔记，根据练习小结而来，有时间就更新">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch Tutorial">
<meta property="og:url" content="http://example.com/2022/08/10/pytorch-tutorial/index.html">
<meta property="og:site_name" content="Homepage | 张有为">
<meta property="og:description" content="个人笔记，根据练习小结而来，有时间就更新">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/artimg/torch/Torch.png">
<meta property="article:published_time" content="2022-08-10T06:56:02.615Z">
<meta property="article:modified_time" content="2022-08-10T10:24:01.519Z">
<meta property="article:author" content="宝可梦训练师">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/artimg/torch/Torch.png">
  
    <link rel="alternative" href="/atom.xml" title="Homepage | 张有为" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>


  
<meta name="generator" content="Hexo 6.2.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
				<img lazy-src="/img/face.png" class="js-avatar">
			
		</a>

		<hgroup>
			<h1 class="header-author"><a href="/">宝可梦训练师</a></h1>
		</hgroup>

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">Home</a></li>
				        
							<li><a href="/archives">文章归档</a></li>
				        
						</ul>
					</nav>
					<nav class="half-header-menu">
						<a class="hide">Home</a>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
					        
						</div>
						<!-- music -->
						
							<!-- <div style="position: absolute; bottom: 120px; left: auto; width: 85%;"> -->
							<div style="position: absolute; left: auto; width: 85%;">
								<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=220 height=86 src="//music.163.com/outchain/player?type=2&id=557584656&auto=1&height=66"></iframe>
							</div>
						
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/face.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">Home</a></li>
		        
					<li><a href="/archives">文章归档</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-pytorch-tutorial" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/08/10/pytorch-tutorial/" class="article-date">
  	<time datetime="2022-08-10T06:56:02.615Z" itemprop="datePublished">2022-08-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Pytorch Tutorial
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        

        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>个人笔记，根据练习小结而来，有时间就更新<br>  <span id="more"></span></p>
</blockquote>
<h2 id="Pytorch的核心模块说明"><a href="#Pytorch的核心模块说明" class="headerlink" title="Pytorch的核心模块说明"></a>Pytorch的核心模块说明</h2><ul>
<li>Pytorch两大主要功能：计算图构建与反向传播，优化 </li>
<li>计算图构建与反向传播实现方案：定义一个基类，实现forward和backward方法，并且可以自动创建计算图，子类各种算子分别实现forward和backward；<br>自定义的算子或者网络如果使用已经有的算子进行forward，则不用定义backward</li>
<li>优化器的实现方案：基于object提供各种优化器类 </li>
<li><a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/tree/master/torch">Pytorch源码</a></li>
</ul>
<img src="/artimg/torch/Torch.png" style="zoom:100%;" />

<h3 id="autograd模块"><a href="#autograd模块" class="headerlink" title="autograd模块"></a>autograd模块</h3><ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/autograd.html">doc</a></li>
<li>定义Function类，作为复杂的算子的基类；需要用户自己写更为复杂的算子，autograd中没有提供写好的算子包</li>
<li>上述实现操作和反向传播方法的时候需要调用functional中的API，而functional中的API又是调用C++的底层实现</li>
</ul>
<h3 id="nn模块"><a href="#nn模块" class="headerlink" title="nn模块"></a>nn模块</h3><ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">doc</a></li>
<li>定义Module类，作为各种Layer和Loss的基类 </li>
<li>基于Module扩充为各种算子，Layer和Loss，实现神经单元的前向传播和反向传播算法</li>
<li>上述实现操作和反向传播方法的时候需要调用functional中的API</li>
</ul>
<h3 id="optim模块"><a href="#optim模块" class="headerlink" title="optim模块"></a>optim模块</h3><ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">doc</a></li>
<li>基于object定义Optimizer类，作为所有优化器的基类 </li>
<li>基于Optimizer实现各种优化器，SGD,Adam等</li>
</ul>
<h3 id="计算图与优化器的交互"><a href="#计算图与优化器的交互" class="headerlink" title="计算图与优化器的交互"></a>计算图与优化器的交互</h3><ul>
<li>对于nn模块，Module提供了parameter方法收集参数（Tensor类），并传递给优化器</li>
<li>parameter方法使用了Parameter容器类来存储参数，Parameter类是Tensor的子类，但是不参与计算图构建，不需要实现forward和backward方法</li>
</ul>
<h3 id="Homework-1"><a href="#Homework-1" class="headerlink" title="Homework 1"></a>Homework 1</h3><ul>
<li>基于Function写出exp（）算子 </li>
<li>使用autograd提供的Variable，Function构建一个简单的神经网络 </li>
<li>对某个叶子节点参数进行优化</li>
</ul>
<h3 id="Homeworak-2"><a href="#Homeworak-2" class="headerlink" title="Homeworak 2"></a>Homeworak 2</h3><ul>
<li>基于Module写出一个简单的神经元 </li>
<li>使用nn模块提供的Module，各种Layer构建一个简单的神经网络</li>
<li>对神经网络的所有参数进行优化</li>
</ul>
<h3 id="Homeworak-3"><a href="#Homeworak-3" class="headerlink" title="Homeworak 3"></a>Homeworak 3</h3><ul>
<li>在Homework2的基础上基于Module模块看懂Transformer论文源码<br><a target="_blank" rel="noopener" href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/master/transformer">Transformer</a></li>
</ul>
<h3 id="算子实现参考邱锡鹏《神经网络与深度学习：案例与实践》"><a href="#算子实现参考邱锡鹏《神经网络与深度学习：案例与实践》" class="headerlink" title="算子实现参考邱锡鹏《神经网络与深度学习：案例与实践》"></a>算子实现参考邱锡鹏《神经网络与深度学习：案例与实践》</h3><hr>
<hr>
<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><a id="Top">目录</a></h1><ul>
<li><a href="#Tensor"><em>矩阵(Tensor)</em></a><ul>
<li><a href="#Tensor0">1.矩阵类型</a></li>
<li><a href="#Tensor1">2.矩阵创建</a></li>
<li><a href="#Tensor2">3.矩阵切片</a></li>
<li><a href="#Tensor3">4.矩阵运算</a></li>
<li><a href="#Tensor4">5.矩阵变形</a></li>
<li><a href="#Tensor5">6.求和均值方差</a></li>
<li><a href="#Autograd">7.自动求导</a></li>
<li><a href="#GPU">8.GPU的使用</a></li>
</ul>
</li>
<li><a href="#Dataset/DataLoader"><em>数据集(Dataset&#x2F;DataLoader)</em></a></li>
<li><a href="#Loss-function"><em>损失函数(Loss-function)</em></a></li>
<li><a href="#Optimizer"><em>优化器(Optimizer)</em></a></li>
<li><a href="#Model"><em>模型搭建(Model)</em></a></li>
<li><a href="#Train/Test"><em>训练&#x2F;测试(Train&#x2F;Test)</em></a></li>
<li><a href="#Visualization"><em>可视化(Visualization)</em></a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<hr>
<hr>
<h2 id="矩阵-Tensor"><a href="#矩阵-Tensor" class="headerlink" title="矩阵(Tensor)"></a><a id="Tensor"><em>矩阵(Tensor)</em></a></h2><hr>
<h3 id="1-矩阵类型"><a href="#1-矩阵类型" class="headerlink" title="1.矩阵类型"></a><a id="Tensor0">1.矩阵类型</a></h3><ul>
<li>Pytorch中定义了8种CPU张量类型和对应的GPU张量类型  </li>
<li>相同数据类型的tensor才能做运算</li>
<li>全局矩阵类型设置    <ul>
<li>torch.set_default_tensor_type(torch.FloatTensor)</li>
</ul>
</li>
<li>数据类型转换  <ul>
<li>1.Tensor.long() ,  .int() ,  .float() ,  .double()   </li>
<li>2.Tensor.to()   </li>
<li>3.dtype&#x3D;’torch.float32’</li>
</ul>
</li>
<li>与numpy数据类型转换  <ul>
<li>Tensor–&gt; Numpy ：Tensor.numpy()</li>
<li>Numpy –&gt; Tensor ：torch.from_numpy()</li>
</ul>
</li>
<li>与Python数据类型转换  <ul>
<li>Tensor –&gt; list：data.tolist()</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="center">数据类型</th>
<th align="center">dytpe</th>
<th align="center">CPU</th>
<th align="center">GPU</th>
</tr>
</thead>
<tbody><tr>
<td align="center">16位浮点型</td>
<td align="center">&#x2F;&#x2F;</td>
<td align="center">&#x2F;&#x2F;</td>
<td align="center">&#x2F;&#x2F;</td>
</tr>
<tr>
<td align="center">32位浮点型</td>
<td align="center">torch.float32或torch.float</td>
<td align="center">torch.FloatTensor</td>
<td align="center">torch.cuda.FloatTensor</td>
</tr>
<tr>
<td align="center">64位浮点型</td>
<td align="center">torch.float64或torch.double</td>
<td align="center">torch.DoubleTensor</td>
<td align="center">torch.cuda.DoubleTensor</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">8位无符号整型</td>
<td align="center">torch.uint8</td>
<td align="center">&#x2F;&#x2F;</td>
<td align="center">&#x2F;&#x2F;</td>
</tr>
<tr>
<td align="center">8位有符号整型</td>
<td align="center">torch.int8</td>
<td align="center">&#x2F;&#x2F;</td>
<td align="center">&#x2F;&#x2F;</td>
</tr>
<tr>
<td align="center">16位有符号整型</td>
<td align="center">torch.int16</td>
<td align="center">&#x2F;&#x2F;</td>
<td align="center">&#x2F;&#x2F;</td>
</tr>
<tr>
<td align="center">32位有符号整型</td>
<td align="center">torch.int32</td>
<td align="center">&#x2F;&#x2F;</td>
<td align="center">&#x2F;&#x2F;</td>
</tr>
<tr>
<td align="center">64位有符号整型</td>
<td align="center">torch.int64</td>
<td align="center">&#x2F;&#x2F;</td>
<td align="center">&#x2F;&#x2F;</td>
</tr>
</tbody></table>
<hr>
<h3 id="2-矩阵创建"><a href="#2-矩阵创建" class="headerlink" title="2.矩阵创建"></a><a id="Tensor1">2.矩阵创建</a></h3><p><font color=green>一、从list,numpy创建</font>  </p>
<ul>
<li>torch.tensor(),&amp;emsp;torch.from_numpy()</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]],dtype=torch.int32)</span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]],dtype=torch.float32)</span><br><span class="line">z=torch.from_numpy(np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]]))</span><br><span class="line">x,y,z</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 2, 3, 4],
         [2, 3, 4, 5]], dtype=torch.int32),
 tensor([[1., 2., 3., 4.],
         [2., 3., 4., 5.]]),
 tensor([[1, 2, 3, 4],
         [2, 3, 4, 5]], dtype=torch.int32))
</code></pre>
<p><font color=green>二、从常见函数创建</font>  </p>
<ul>
<li><p><font color=MediumPurple >torch.empty(尺寸)&amp;emsp;torch.full(尺寸,值)</font>   </p>
</li>
<li><p><font color=MediumPurple >torch.zeros(尺寸)&amp;emsp;torch.ones(尺寸)&amp;emsp;torch.eye(维数)   </font> </p>
</li>
<li><p><font color=MediumPurple >torch.zeros_like(另一个矩阵)&amp;emsp;torch.ones_like(另一个矩阵)  </font></p>
</li>
</ul>
<p>note：尺寸可以是一维的，也可以是多维的，一般用列表框起来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a1=torch.empty(<span class="number">3</span>)</span><br><span class="line">a2=torch.empty(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">a3=torch.eye(<span class="number">3</span>)   </span><br><span class="line">a4=torch.zeros(<span class="number">3</span>)</span><br><span class="line">a5=torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">a6=torch.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a7=torch.full([<span class="number">2</span>,<span class="number">3</span>],<span class="number">6</span>)</span><br><span class="line">a8=torch.zeros_like(a1)</span><br><span class="line">a1,a2,a3,a4,a5,a6,a7,a8</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([9.1477e-41, 0.0000e+00, 4.4842e-44]),
 tensor([[1., 2.],
         [3., 4.],
         [2., 3.]]),
 tensor([[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]),
 tensor([0., 0., 0.]),
 tensor([[0., 0., 0.],
         [0., 0., 0.]]),
 tensor([[1., 1., 1.],
         [1., 1., 1.]]),
 tensor([[6, 6, 6],
         [6, 6, 6]]),
 tensor([0., 0., 0.]))
</code></pre>
<p><font color=green>三、区间线性采样</font>  </p>
<ul>
<li><p><font color=MediumPurple >torch.arange(首，尾，可选步长) </font><br>note：不包括尾巴  </p>
</li>
<li><p><font color=MediumPurple >torch.linspace(首，尾，数量)  </font><br>note：包括尾巴，步长&#x3D;(尾-首)&#x2F;(n-1),因为starts+(n-1)step&#x3D;end</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a=torch.arange(<span class="number">5</span>,<span class="number">8</span>)</span><br><span class="line">b=torch.arange(<span class="number">5</span>,<span class="number">8</span>,<span class="number">2</span>)</span><br><span class="line">c=torch.linspace(<span class="number">5</span>,<span class="number">8</span>,<span class="number">1</span>)</span><br><span class="line">d=torch.linspace(<span class="number">5</span>,<span class="number">8</span>,<span class="number">10</span>)</span><br><span class="line">a,b,c,d</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([5, 6, 7]),
 tensor([5, 7]),
 tensor([5.]),
 tensor([5.0000, 5.3333, 5.6667, 6.0000, 6.3333, 6.6667, 7.0000, 7.3333, 7.6667,
         8.0000]))
</code></pre>
<p><font color=green >四、一些常见随机矩阵生成</font>  </p>
<ul>
<li><font color=MediumPurple >torch.rand(尺寸)</font><br>均匀分布$U(0,1)$  </li>
<li><font color=MediumPurple >torch.randint(low,high,尺寸)</font><br>均匀分布$U(low,high)$  </li>
<li><font color=MediumPurple > torch.randn(尺寸) </font><br>正态分布$N(0,1)$  </li>
<li><font color=MediumPurple > torch.normal(均值，方差，尺寸) </font><br>正态分布$N(u,\sigma^2)$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.rand(<span class="number">2</span>,<span class="number">3</span>)  </span><br><span class="line">y=torch.randint(<span class="number">1</span>,<span class="number">9</span>,[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a=torch.randn([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">b=torch.normal(<span class="number">10</span>,<span class="number">3</span>,[<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">x,y,a,b</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[0.7697, 0.3093, 0.2239],
         [0.9295, 0.6902, 0.0886]]),
 tensor([[1, 2, 8],
         [6, 7, 8]]),
 tensor([[-0.3317, -1.4514,  0.6665,  0.7765],
         [ 1.0954, -0.3389, -0.8067, -1.3568],
         [-1.6465, -1.8853, -1.8286,  0.0149]]),
 tensor([[16.8522, 11.1349,  7.1411, 16.0858],
         [15.2712,  7.4130,  8.6740, 12.8134],
         [ 9.0616,  6.4860, 10.2000,  9.6383]]))
</code></pre>
<hr>
<h3 id="3-矩阵切片"><a href="#3-矩阵切片" class="headerlink" title="3.矩阵切片"></a><a id="Tensor2">3.矩阵切片</a></h3><ul>
<li>逗号   <ul>
<li>逗号前表示行，逗号后表示列</li>
</ul>
</li>
<li>冒号 <ul>
<li>一个冒号&amp;nbsp;start ：end  &amp;nbsp;&amp;nbsp;&amp;nbsp;</li>
<li>两个冒号&amp;nbsp;start ：end ：step  &amp;nbsp;&amp;nbsp;&amp;nbsp;</li>
</ul>
</li>
</ul>
<p><font color=green>一、1维矩阵的切片</font>  </p>
<ul>
<li>同列表切片</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.random.seed()</span><br><span class="line">a=torch.rand(<span class="number">10</span>) </span><br><span class="line">a,a[<span class="number">5</span>],a[<span class="number">0</span>:<span class="number">3</span>],a[:<span class="number">6</span>],a[:-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([0.1226, 0.7973, 0.4854, 0.7224, 0.8433, 0.7793, 0.4325, 0.8004, 0.7140,
         0.0311]),
 tensor(0.7793),
 tensor([0.1226, 0.7973, 0.4854]),
 tensor([0.1226, 0.7973, 0.4854, 0.7224, 0.8433, 0.7793]),
 tensor([0.1226, 0.7973, 0.4854, 0.7224, 0.8433, 0.7793, 0.4325, 0.8004, 0.7140]))
</code></pre>
<p><font color=green>二、2维矩阵的切片</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand([<span class="number">6</span>,<span class="number">4</span>])</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0.6353, 0.8920, 0.9199, 0.4035],
        [0.6945, 0.6330, 0.9331, 0.8373],
        [0.3548, 0.9944, 0.9018, 0.9718],
        [0.2909, 0.1973, 0.9949, 0.4955],
        [0.9215, 0.6109, 0.3772, 0.4395],
        [0.1501, 0.8836, 0.3299, 0.1494]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>],a[<span class="number">0</span>:<span class="number">2</span>],a[<span class="number">0</span>:-<span class="number">1</span>:<span class="number">2</span>],a[[<span class="number">0</span>,<span class="number">2</span>]]  <span class="comment">#取指定行</span></span><br></pre></td></tr></table></figure>




<pre><code>(tensor([0.6353, 0.8920, 0.9199, 0.4035]),
 tensor([[0.6353, 0.8920, 0.9199, 0.4035],
         [0.6945, 0.6330, 0.9331, 0.8373]]),
 tensor([[0.6353, 0.8920, 0.9199, 0.4035],
         [0.3548, 0.9944, 0.9018, 0.9718],
         [0.9215, 0.6109, 0.3772, 0.4395]]),
 tensor([[0.6353, 0.8920, 0.9199, 0.4035],
         [0.3548, 0.9944, 0.9018, 0.9718]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[:,<span class="number">0</span>]  <span class="comment">#取指定列</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([0.6353, 0.6945, 0.3548, 0.2909, 0.9215, 0.1501])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>:<span class="number">2</span>,-<span class="number">1</span>]  <span class="comment">#取任意元素</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([0.4035, 0.8373])
</code></pre>
<hr>
<h3 id="4-矩阵运算"><a href="#4-矩阵运算" class="headerlink" title="4.矩阵运算"></a><a id="Tensor3">4.矩阵运算</a></h3><ul>
<li>加减乘除：按元素 <ul>
<li>$a+b&#x3D;torch.add(a,b)$  </li>
<li>$a-b&#x3D;torch.sub(a,b)$  </li>
<li>$a*b&#x3D;torch.multiply(a,b)$  </li>
<li>$a&#x2F;b&#x3D;torch.div(a,b)$<br>  &amp;nbsp;</li>
</ul>
</li>
<li>加法的广播机制  <ul>
<li>维数一致：如shape&#x3D;[3,5,8]和shape&#x3D;[1,5,1],运算的结果为shape&#x3D;[3,5,8]。</li>
<li>维数不一致：如shape&#x3D;[3,5,8和shape&#x3D;[1,8],两个矩阵右对齐，然后维数少的左边补齐，[1,8]–&gt;[1,1,8]，然后运算，结果shape&#x3D;[3,5,8]  </li>
<li>对比相应维数的长度时，如果不等，必须有一个为1才能进行广播，否则出错。如shape&#x3D;[3,6]和shape&#x3D;[2,6]无法运算<br>  &amp;nbsp;</li>
</ul>
</li>
<li>矩阵乘法  <ul>
<li>x是一维行向量，y是一维行向量  <ul>
<li>$torch.matmul(x,y)&#x3D;xy^T$  </li>
<li>$torch.matmul(y,x)&#x3D;yx^T$</li>
</ul>
</li>
<li>x是一维行向量，W是二维矩阵  <ul>
<li>$torch.matmul(W,x)&#x3D;(Wx^T)^T$  </li>
<li>$torch.matmul(x,W)&#x3D;xW$</li>
</ul>
</li>
<li>A是矩阵，B是矩阵  <ul>
<li>$torch.matmul(A,B)&#x3D;AB$  </li>
<li>$torch.multiply(A,B)&#x3D;A\cdot B$</li>
</ul>
</li>
<li>M.shape&#x3D;[j,1,n,m]，N.shape&#x3D;[k,m,p]：最后两个维度作矩阵乘法，其他维度进行广播机制  <ul>
<li>[j,1]广播[k],就是[j,k],然后[n,m]与[m,p]作矩阵乘法，就是[n,p]，最终结果shape&#x3D;[j,k,n,p]  </li>
<li>实际情况，N有k个shape&#x3D;[m,p]的矩阵，将M也变成k个shape&#x3D;[n,m]的矩阵，作运算，最后复制为j份存到dim&#x3D;0这一维<br>  &amp;nbsp;</li>
</ul>
</li>
</ul>
</li>
<li>按批次运算1：若神经网络输入为一个D维的行向量x，输出为K维的行向量y：如 FNN，RNN   <ul>
<li>x–&gt;A，torch.matmul(x,W)–&gt;torch.matmul(A,W)  </li>
<li>输入输均由一个行向量，变成N个行向量，且乘法依然符合矩阵乘法<br>  &amp;nbsp;</li>
</ul>
</li>
<li>按批次运算2：若神经网络输入为一个LxD维的矩阵X，输出为LxD的矩阵O：如 Transformer   <ul>
<li>X–&gt; M，torch.matmul(X,W)–&gt;torch.matmul(M,W)  </li>
<li>输入输均由一排矩阵，变成N排矩阵，矩阵乘法为最后两维的矩阵乘法  </li>
<li>注意，CNN输入虽然也是矩阵，但是其运算是卷积，以点积形式实现，而非矩阵乘法</li>
</ul>
</li>
</ul>
<p><font color=green>一、两个一维行向量相乘</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">y=torch.tensor([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">a=torch.matmul(y,x)</span><br><span class="line">b=torch.matmul(x.t(),y)</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>




<pre><code>(tensor(11), tensor(11))
</code></pre>
<p><font color=green>二、一维行向量左乘矩阵</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">a=torch.matmul(x,y)</span><br><span class="line">b=torch.matmul(x.t(),y)</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([11, 16]), tensor([11, 16]))
</code></pre>
<p><font color=green>三、一维行向量右乘矩阵</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">a=torch.matmul(y,x)</span><br><span class="line">b=torch.matmul(y,x.t())</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([ 8, 18]), tensor([ 8, 18]))
</code></pre>
<p><font color=green>四、矩阵相乘</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">8</span>],[<span class="number">33</span>,<span class="number">21</span>]])    <span class="comment">#shape=[4,2]</span></span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])              <span class="comment">#shape=[2,3]</span></span><br><span class="line">a=torch.matmul(x,y)                            <span class="comment">#shape=[4,3]</span></span><br><span class="line">a</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 14,  19,  24],
        [  5,   7,   9],
        [ 35,  46,  57],
        [117, 171, 225]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">8</span>],[<span class="number">33</span>,<span class="number">21</span>]],[[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]]])    <span class="comment">#shape=[2,4,2]</span></span><br><span class="line">y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])                                          <span class="comment">#shape=[2,3]</span></span><br><span class="line">a=torch.matmul(x,y)                                                        <span class="comment">#shape=[2,4,3]，实际就是将x的每排shape=[4,2]的矩阵分别与矩阵y相乘</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 14,  19,  24],
         [  5,   7,   9],
         [ 35,  46,  57],
         [117, 171, 225]],

        [[  5,   7,   9],
         [  5,   7,   9],
         [  5,   7,   9],
         [  5,   7,   9]]])
</code></pre>
<hr>
<h3 id="5-矩阵变形"><a href="#5-矩阵变形" class="headerlink" title="5.矩阵变形"></a><a id="Tensor4">5.矩阵变形</a></h3><p><font color=green >转置与变形：t()，reshape()</font>  </p>
<ul>
<li>torch.reshape(尺寸)<br>  将矩阵拉平后，变成想要的尺寸  </li>
<li>torch.flatten()<br>  将矩阵拉平  </li>
<li>torch.transpose(dim,dim)<br>  指定维数进行转置，没指定的看成整体</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]]])</span><br><span class="line">x.shape,x.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>(torch.Size([1, 2, 3]), 2)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.flatten()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3, 6, 7, 8])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.reshape(<span class="number">1</span>,<span class="number">3</span>,-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1, 2],
         [3, 6],
         [7, 8]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1, 6],
         [2, 7],
         [3, 8]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1, 2, 3]],

        [[6, 7, 8]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).t()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 3],
        [2, 4]])
</code></pre>
<p><font color=green >分割：chunk()</font></p>
<ul>
<li>torch.chunk(块，dim)  <ul>
<li>在指定维数将矩阵分成相应的块数</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]).chunk(<span class="number">3</span>,<span class="number">0</span>)                 <span class="comment">#包含9个元素，分成3块</span></span><br></pre></td></tr></table></figure>




<pre><code>(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],[[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">3</span>,<span class="number">7</span>,<span class="number">0</span>]]])        <span class="comment">#第0维包含2个元素(矩阵)，第1维包含2行向量，第2维包含3个元素 </span></span><br><span class="line">x.chunk(<span class="number">2</span>,<span class="number">0</span>)                                                 <span class="comment">#把第0维的两个矩阵分开，获得两个矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[[1, 2, 3],
          [4, 5, 6]]]),
 tensor([[[7, 8, 9],
          [3, 7, 0]]]))
</code></pre>
<p><font color=green >拼接：cat()</font></p>
<ul>
<li>torch.cat((a,b),dim)<ul>
<li>dim&#x3D;0，分别将第0维的元素看成整体，将它们拉平横向拼在一块</li>
<li>dim&#x3D;1，分别将第1维的元素看成整体，将它们拉平横向拼在一块</li>
<li>dim&#x3D;2，分别将第2维的元素看成整体，将它们拉平横向拼在一块</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a,b=x.chunk(<span class="number">2</span>,<span class="number">0</span>)                                           <span class="comment">#将x分成两块，用a，b接收   </span></span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[[1, 2, 3],
          [4, 5, 6]]]),
 tensor([[[7, 8, 9],
          [3, 7, 0]]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat((a,b),<span class="number">0</span>)                                         <span class="comment">#按第0维拼接，第0维的元素是矩阵，所以获得所有矩阵的拼接</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1, 2, 3],
         [4, 5, 6]],

        [[7, 8, 9],
         [3, 7, 0]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat((a,b),<span class="number">1</span>)                                          <span class="comment">#按第1维拼接，第1维是行向量，所以获得所有行向量的拼接</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1, 2, 3],
         [4, 5, 6],
         [7, 8, 9],
         [3, 7, 0]]])
</code></pre>
<p><font color=green >堆叠：stack()</font></p>
<ul>
<li>torch.stack((a,b),dim)<ul>
<li>首先把每个矩阵最外层扩展一个维度，也就是加个方括号</li>
<li>dim&#x3D;0，分别将第0维的元素看成整体，将它们拉平并罗列放置</li>
<li>dim&#x3D;1，分别将第1维的元素看成整体，将它们拉平并罗列放置</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        	      [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">B = torch.tensor([[<span class="number">12</span>, <span class="number">22</span>, <span class="number">33</span>],</span><br><span class="line">        	      [<span class="number">44</span>, <span class="number">55</span>, <span class="number">66</span>],</span><br><span class="line">                  [<span class="number">77</span>, <span class="number">88</span>,<span class="number">99</span>]])</span><br><span class="line">A,B</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 2, 3],
         [4, 5, 6],
         [7, 8, 9]]),
 tensor([[12, 22, 33],
         [44, 55, 66],
         [77, 88, 99]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack((A,B))                            <span class="comment">#第一步，最外围加个方括号，增加一维；第二步，按dim=0堆叠，也就是将多个矩阵堆叠，获得多排矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 1,  2,  3],
         [ 4,  5,  6],
         [ 7,  8,  9]],

        [[12, 22, 33],
         [44, 55, 66],
         [77, 88, 99]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack((A,B),dim=<span class="number">1</span>)                      <span class="comment">#第一步，最外围加个方括号，增加一维；第二步，按dim=1堆叠，分别将行向量堆叠，分别获得一个矩阵，再把这些矩阵罗列放在一个列表中国</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 1,  2,  3],
         [12, 22, 33]],

        [[ 4,  5,  6],
         [44, 55, 66]],

        [[ 7,  8,  9],
         [77, 88, 99]]])
</code></pre>
<p><font color=green >拼接与堆叠的应用：合并样本与样本的特征</font></p>
<ul>
<li>To be continued…</li>
</ul>
<p><font color=green >squeeze()与unsqueeze()</font></p>
<ul>
<li>torch.squeeze() <ul>
<li>删除所有维数为1的维度</li>
</ul>
</li>
<li>torch.squeeze(dim) <ul>
<li>删除指定维度为1的维度，其实就是去掉指定维数的括号</li>
</ul>
</li>
<li>torch.unsqueeze(dim) <ul>
<li>在指定的地方增加一个维数，假如原本是个二维矩阵 </li>
<li>dim&#x3D;0,   &amp;nbsp;最外层加一个括号 </li>
<li>dim&#x3D;1,   &amp;nbsp;对第1层每个元素，也就是每个行向量，分别加个括号</li>
<li>dim&#x3D;2,   &amp;nbsp;对第2层每个元素，也就是每个数值，分别加个括号</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]],[[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]]])</span><br><span class="line">x.squeeze(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y=x.squeeze(<span class="number">2</span>)</span><br><span class="line">y,y.unsqueeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 2, 3],
         [4, 5, 6]]),
 tensor([[[1, 2, 3]],
 
         [[4, 5, 6]]]))
</code></pre>
<p><font color=green >关于维度的小结  </font><br>    - [[[ &amp;nbsp;分别看作第0，1，2维的墙<br>    - 如果对第0维进行操作，比如sum，mean，min的操作，那么进入第0维房间，把里面的元素看成整体，进行操作，操作后只改变第0维的维数<br>    - 如果是增加维数（墙面），按指定的位置增加墙面即可，如squeeze和unsqueeze</p>
<hr>
<h3 id="6-求和均值方差"><a href="#6-求和均值方差" class="headerlink" title="6.求和均值方差"></a><a id="Tensor5">6.求和均值方差</a></h3><ul>
<li>torch.sum() &amp;nbsp; torch.mean() &amp;nbsp; torch.var() &amp;nbsp; torch.std()  <ul>
<li>对全部元素求和,均值，方差，标准差</li>
</ul>
</li>
<li>torch.sum(dim) &amp;nbsp; torch.mean(dim) &amp;nbsp; torch.var(dim) &amp;nbsp; torch.std(dim)<ul>
<li>进入指定的维数房间，对里面的元素进行求和，均值，方差，标准差</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">x,x.<span class="built_in">sum</span>(dim=<span class="number">0</span>),x.<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 2, 3],
         [4, 5, 6]]),
 tensor([5, 7, 9]),
 tensor([ 6, 15]))
</code></pre>
<hr>
<h3 id="7-自动求导"><a href="#7-自动求导" class="headerlink" title="7.自动求导"></a><a id="Autograd">7.自动求导</a></h3><p><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/2528424">PaddlePaddle</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=R_m4kanPy6Q">Youtube1</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=MswxJw-8PvE">Youtube2</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ALMKFS_QFm4">Youtube3</a></p>
<p><font color=green >基本概念</font></p>
<ul>
<li>这里仅讨论标量对参数的求导，梯度与参数矩阵形状一致，表示每个参数动一动，标量的变化  </li>
<li>被求导的变量，需要设置requires_grad&#x3D;True，dtype&#x3D;torch.float32  </li>
<li>求导的梯度，在参数的grad属性中保存</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=torch.ones([<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=a.<span class="built_in">sum</span>()</span><br><span class="line">b.backward()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
<p><font color=green >梯度的累加</font>  </p>
<ul>
<li>梯度每次计算都会被累加在grad属性中  </li>
<li>使用torch.grad.zero_()方法可以进行梯度归零</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a=torch.ones([<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    b=a.<span class="built_in">sum</span>()</span><br><span class="line">    b.backward()</span><br><span class="line">    <span class="built_in">print</span>(a.grad)</span><br><span class="line">a.grad.zero_()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[2., 2., 2.],
        [2., 2., 2.]])
tensor([[3., 3., 3.],
        [3., 3., 3.]])
tensor([[0., 0., 0.],
        [0., 0., 0.]])
</code></pre>
<p><font color=green >计算图的构建与销毁</font>  </p>
<ul>
<li>每次进行前向计算时都会自动构建计算图，调用backward后自动销毁  </li>
<li>在调用backward方法的时候设置retain_graph&#x3D;True，计算图可保留，不用再次前向计算</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=torch.ones([<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=a.<span class="built_in">sum</span>()</span><br><span class="line">b.backward(retain_graph=<span class="literal">True</span>) <span class="comment">#计算图保留，相当于对计算图什么也没做，就求了一次梯度</span></span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line">b.backward() <span class="comment">#调用后自动销毁计算图</span></span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[2., 2., 2.],
        [2., 2., 2.]])
</code></pre>
<p><font color=green >不构建计算图</font>  </p>
<ul>
<li>前向计算时，构建正向计算图的同时，会通过回溯的方式，构建反向算子与反向计算图  </li>
<li>可以通过查看grad_fn属性查看有没有构建反向算子</li>
<li>with torch.no_grad(): 后面的计算，不会构建计算图</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">2.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">f=<span class="number">2</span>*a                                  <span class="comment">#f参与构建计算图</span></span><br><span class="line">g=<span class="number">3</span>*b                                  <span class="comment">#g参与构建计算图</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    f=<span class="number">3</span>*a                              <span class="comment">#f重新定义，不构建计算图</span></span><br><span class="line"><span class="built_in">print</span>(f.grad_fn,g.grad_fn)</span><br></pre></td></tr></table></figure>

<pre><code>None &lt;MulBackward0 object at 0x000001E4035A2E50&gt;
</code></pre>
<p><font color=green >将变量变成常量</font>  </p>
<ul>
<li>a.detach()返回一个张量，data区就是a的data区，requires_grad&#x3D;False  </li>
<li>可以用out来接收，输入到下一个网络中，并且作为常量传入的，不参与梯度计算  </li>
<li>由于是新创建的张量，不影响原来的计算图 ；但是不要用同一个变量名接收这个张量，否则计算图中的那个变量名就没了 </li>
<li>固定网络A的参数，更新网络B的参数的方法  <ul>
<li>方法一：将网络A的输入detach一下，创建新的标量张量作为网络B的输入，这时候只会构建B网络的计算图  </li>
<li>方法二：用for遍历A网络的参数，将requires_grad属性全部设为False</li>
</ul>
</li>
<li>一个不常用的torch.detach_()操作，原地修改计算图，有点复杂，实际上所有的tensor操作后加个下划线都是原地操作</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">2.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=a.detach()</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>




<pre><code>(tensor(2., requires_grad=True), tensor(2.))
</code></pre>
<hr>
<h3 id="8-GPU的使用"><a href="#8-GPU的使用" class="headerlink" title="8.GPU的使用"></a><a id="GPU">8.GPU的使用</a></h3><ul>
<li>这里仅使用一块gpu</li>
<li>查看设备是否可用：torch.cuda.is_available()查看设备</li>
<li>指定设备：device &#x3D; torch.device(“cuda:0”)或device &#x3D; torch.device( “cpu”)或device &#x3D; torch.device(“cuda:0”  if torch.cuda.is_available() else “cpu”)</li>
<li>使用tensor.to(device)和model.to(tensor)把输入张量和模型参数送进gpu，这样计算图便在gpu中构建了；使用.device可以查看设备属性</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available(),torch.tensor(<span class="number">2.0</span>).device</span><br></pre></td></tr></table></figure>




<pre><code>(False, device(type=&#39;cpu&#39;))
</code></pre>
<hr>
<h3 id="综合案例1："><a href="#综合案例1：" class="headerlink" title="综合案例1："></a>综合案例1：</h3><p>采集两个圆上的点数据，x作为一个特征，y作为一个特征，每个圆各采集5个样本，最后合并成10个样本，每个样本有2个特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">n_samples = <span class="number">10</span></span><br><span class="line">n_samples_out = n_samples // <span class="number">2</span></span><br><span class="line">n_samples_in = n_samples - n_samples_out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采集第1类数据，特征为(x,y)</span></span><br><span class="line"><span class="comment"># 使用&#x27;paddle.linspace&#x27;在0到pi上均匀取n_samples_out个值</span></span><br><span class="line"><span class="comment"># 使用&#x27;paddle.cos&#x27;计算上述取值的余弦值作为特征1，使用&#x27;paddle.sin&#x27;计算上述取值的正弦值作为特征2</span></span><br><span class="line">outer_circ_x = paddle.cos(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_out))</span><br><span class="line">outer_circ_y = paddle.sin(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_out))</span><br><span class="line"></span><br><span class="line">inner_circ_x = <span class="number">1</span> - paddle.cos(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_in))</span><br><span class="line">inner_circ_y = <span class="number">0.5</span> - paddle.sin(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_in))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;outer_circ_x.shape:&#x27;</span>, outer_circ_x.shape,<span class="string">&#x27;\n&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;outer_circ_y.shape:&#x27;</span>, outer_circ_y.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;inner_circ_x.shape:&#x27;</span>, inner_circ_x.shape,<span class="string">&#x27;\n&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;inner_circ_y.shape:&#x27;</span>, inner_circ_y.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;outer_circ_x:&#x27;</span>, outer_circ_x,<span class="string">&#x27;\n&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;outer_circ_y:&#x27;</span>, outer_circ_y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;inner_circ_x:&#x27;</span>, inner_circ_x,<span class="string">&#x27;\n&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;inner_circ_y:&#x27;</span>, inner_circ_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用&#x27;paddle.concat&#x27;将两类数据的特征1和特征2分别延维度0拼接在一起，得到全部特征1和特征2</span></span><br><span class="line"><span class="comment"># 使用&#x27;paddle.stack&#x27;将两类特征延维度1堆叠在一起</span></span><br><span class="line">X = paddle.stack(</span><br><span class="line">    [paddle.concat([outer_circ_x, inner_circ_x]),</span><br><span class="line">     paddle.concat([outer_circ_y, inner_circ_y])],</span><br><span class="line">    axis=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;after concat shape:&#x27;</span>, paddle.concat(</span><br><span class="line">    [outer_circ_x, inner_circ_x]).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X shape:&#x27;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;after concat :&#x27;</span>, paddle.concat(</span><br><span class="line">    [outer_circ_x, inner_circ_x]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X :&#x27;</span>, X)</span><br><span class="line"><span class="comment"># 使用&#x27;paddle. zeros&#x27;将第一类数据的标签全部设置为0</span></span><br><span class="line"><span class="comment"># 使用&#x27;paddle. ones&#x27;将第一类数据的标签全部设置为1</span></span><br><span class="line">y = paddle.concat(</span><br><span class="line">    [paddle.zeros(shape=[n_samples_out]),</span><br><span class="line">     paddle.ones(shape=[n_samples_in])]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y shape:&#x27;</span>, y.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y :&#x27;</span>, y)</span><br></pre></td></tr></table></figure>

<pre><code>outer_circ_x.shape: [5] 
 outer_circ_y.shape: [5]
inner_circ_x.shape: [5] 
 inner_circ_y.shape: [5]
outer_circ_x: Tensor(shape=[5], dtype=float32, place=Place(cpu), stop_gradient=True,
       [ 1.        ,  0.70710677, -0.00000004, -0.70710683, -1.        ]) 
 outer_circ_y: Tensor(shape=[5], dtype=float32, place=Place(cpu), stop_gradient=True,
       [ 0.        ,  0.70710683,  1.        ,  0.70710683, -0.00000009])
inner_circ_x: Tensor(shape=[5], dtype=float32, place=Place(cpu), stop_gradient=True,
       [0.        , 0.29289323, 1.        , 1.70710683, 2.        ]) 
 inner_circ_y: Tensor(shape=[5], dtype=float32, place=Place(cpu), stop_gradient=True,
       [ 0.50000000, -0.20710683, -0.50000000, -0.20710683,  0.50000006])
after concat shape: [10]
X shape: [10, 2]
after concat : Tensor(shape=[10], dtype=float32, place=Place(cpu), stop_gradient=True,
       [ 1.        ,  0.70710677, -0.00000004, -0.70710683, -1.        ,
         0.        ,  0.29289323,  1.        ,  1.70710683,  2.        ])
X : Tensor(shape=[10, 2], dtype=float32, place=Place(cpu), stop_gradient=True,
       [[ 1.        ,  0.        ],
        [ 0.70710677,  0.70710683],
        [-0.00000004,  1.        ],
        [-0.70710683,  0.70710683],
        [-1.        , -0.00000009],
        [ 0.        ,  0.50000000],
        [ 0.29289323, -0.20710683],
        [ 1.        , -0.50000000],
        [ 1.70710683, -0.20710683],
        [ 2.        ,  0.50000006]])
y shape: [10]
y : Tensor(shape=[10], dtype=float32, place=Place(cpu), stop_gradient=True,
       [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])
</code></pre>
<hr>
<h3 id="综合案例2：分类任务的准确率计算"><a href="#综合案例2：分类任务的准确率计算" class="headerlink" title="综合案例2：分类任务的准确率计算"></a>综合案例2：分类任务的准确率计算</h3><pre><code>- 二分类任务imputs为NxD，labels为Nx1，outputs为Nx1  
- 多分类任务imputs为NxD，labels为Nx1，outputs为NxC
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">preds, labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - preds：预测值，二分类时，shape=[N, 1]，N为样本数量，多分类时，shape=[N, C]，C为类别数量</span></span><br><span class="line"><span class="string">        - labels：真实标签，shape=[N, 1]</span></span><br><span class="line"><span class="string">    输出：</span></span><br><span class="line"><span class="string">        - 准确率：shape=[1]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 判断是二分类任务还是多分类任务，preds.shape[1]=1时为二分类任务，preds.shape[1]&gt;1时为多分类任务</span></span><br><span class="line">    <span class="keyword">if</span> preds.shape[<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 二分类时，判断每个概率值是否大于0.5，当大于0.5时，类别为1，否则类别为0</span></span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.cast&#x27;将preds的数据类型转换为float32类型</span></span><br><span class="line">        preds = paddle.cast((preds&gt;=<span class="number">0.5</span>),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 多分类时，使用&#x27;paddle.argmax&#x27;计算最大元素索引作为类别</span></span><br><span class="line">        preds = paddle.argmax(preds,axis=<span class="number">1</span>, dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> paddle.mean(paddle.cast(paddle.equal(preds, labels),dtype=<span class="string">&#x27;float32&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设模型的预测值为[[0.],[1.],[1.],[0.]]，真实类别为[[1.],[1.],[0.],[0.]]，计算准确率</span></span><br><span class="line">preds = paddle.to_tensor([[<span class="number">0.</span>],[<span class="number">1.</span>],[<span class="number">1.</span>],[<span class="number">0.</span>]])</span><br><span class="line">labels = paddle.to_tensor([[<span class="number">1.</span>],[<span class="number">1.</span>],[<span class="number">0.</span>],[<span class="number">0.</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuracy is:&quot;</span>, accuracy(preds, labels))</span><br></pre></td></tr></table></figure>

<pre><code>accuracy is: Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
       [0.50000000])
</code></pre>
<hr>
<h3 id="综合案例3：Simple-RNN（了解矩阵行向量左乘矩阵以及一个批次的行向量左乘矩阵）"><a href="#综合案例3：Simple-RNN（了解矩阵行向量左乘矩阵以及一个批次的行向量左乘矩阵）" class="headerlink" title="综合案例3：Simple RNN（了解矩阵行向量左乘矩阵以及一个批次的行向量左乘矩阵）"></a>综合案例3：Simple RNN（了解矩阵行向量左乘矩阵以及一个批次的行向量左乘矩阵）</h3><h4 id="输入与输出分析"><a href="#输入与输出分析" class="headerlink" title="输入与输出分析"></a>输入与输出分析</h4><ul>
<li>inputs：3维矩阵，如shape&#x3D;[3，4，8]，表示3个batch（3个句子），每个句子包含4个单词，每个单词是一个8维向量  </li>
<li>中间层计算（1）：先分析一个句子的情况，并且忽略记忆单元。第一个词进去，出来一个词，第二个词进去，出来一个词，依次进行到最后一个词进去，出来一个词，把最后出来的词作为输出  </li>
<li>中间层计算（2）：由于神经网络一次性并行计算一个batch的样本，如一个batch包含5句话，那么将5句话的第一个词是一个矩阵，每一行就是每句话的第一个词，那么每层输出都是一个矩阵，表示这5句话的第j个词对应的输出  </li>
<li>隐藏层：就是中间层的输出  </li>
<li>outputs：就是最后一个词和隐藏层丢进网络的输出  </li>
<li>单次计算的本质：处理单个词，输出单个词；对于一个batch，输入多个并行的词，输出多个并行的词，并行的词来自不同的句子；对于一个句子的多个词，采用串行依次处理</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SRN模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SRN</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size,  hidden_size, W_attr=<span class="literal">None</span>, U_attr=<span class="literal">None</span>, b_attr=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SRN, self).__init__()</span><br><span class="line">        <span class="comment"># 嵌入向量的维度</span></span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        <span class="comment"># 隐状态的维度</span></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        <span class="comment"># 定义模型参数W，其shape为 input_size x hidden_size</span></span><br><span class="line">        self.W = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="string">&quot;float32&quot;</span>, attr=W_attr)</span><br><span class="line">        <span class="comment"># 定义模型参数U，其shape为hidden_size x hidden_size</span></span><br><span class="line">        self.U = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="string">&quot;float32&quot;</span>,attr=U_attr)</span><br><span class="line">        <span class="comment"># 定义模型参数b，其shape为 1 x hidden_size</span></span><br><span class="line">        self.b = paddle.create_parameter(shape=[<span class="number">1</span>, hidden_size], dtype=<span class="string">&quot;float32&quot;</span>, attr=b_attr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化向量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, batch_size</span>):</span><br><span class="line">        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> hidden_state</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, hidden_state=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># inputs: 输入数据, 其shape为batch_size x seq_len x input_size</span></span><br><span class="line">        batch_size, seq_len, input_size = inputs.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化起始状态的隐向量, 其shape为 batch_size x hidden_size</span></span><br><span class="line">        <span class="keyword">if</span> hidden_state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            hidden_state = self.init_state(batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 循环执行RNN计算</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">            <span class="comment"># 获取当前时刻的输入数据step_input, 其shape为 batch_size x input_size</span></span><br><span class="line">            step_input = inputs[:, step, :]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;中间层的x:<span class="subst">&#123;step_input&#125;</span>&#x27;</span>,<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            <span class="comment"># 获取当前时刻的隐状态向量hidden_state, 其shape为 batch_size x hidden_size</span></span><br><span class="line">            hidden_state = F.tanh(paddle.matmul(step_input, self.W) + paddle.matmul(hidden_state, self.U) + self.b)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;中间层的h:<span class="subst">&#123;hidden_state&#125;</span>&#x27;</span>,<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> hidden_state</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">W_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="number">0.1</span>, <span class="number">0.2</span>], [<span class="number">0.1</span>,<span class="number">0.2</span>]]))</span><br><span class="line">U_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="number">0.0</span>, <span class="number">0.1</span>], [<span class="number">0.1</span>,<span class="number">0.0</span>]]))</span><br><span class="line">b_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="number">0.1</span>, <span class="number">0.1</span>]]))</span><br><span class="line"></span><br><span class="line">srn = SRN(<span class="number">2</span>, <span class="number">2</span>, W_attr=W_attr, U_attr=U_attr, b_attr=b_attr)</span><br><span class="line"></span><br><span class="line">inputs = paddle.to_tensor([[[<span class="number">1</span>, <span class="number">0</span>],[<span class="number">0</span>, <span class="number">2</span>]],[[<span class="number">2</span>, <span class="number">4</span>],[<span class="number">1</span>, <span class="number">3</span>]]], dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">hidden_state = srn(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hidden_state&quot;</span>, hidden_state)</span><br></pre></td></tr></table></figure>

<pre><code>中间层的x:Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,
       [[1., 0.],
        [2., 4.]]) 

中间层的h:Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,
       [[0.19737528, 0.29131261],
        [0.60436779, 0.86172318]]) 

中间层的x:Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,
       [[0., 2.],
        [1., 3.]]) 

中间层的h:Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,
       [[0.31773996, 0.47749743],
        [0.52713710, 0.74447167]]) 

hidden_state Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,
       [[0.31773996, 0.47749743],
        [0.52713710, 0.74447167]])
</code></pre>
<hr>
<hr>
<h2 id="数据集-Dataset-x2F-DataLoader"><a href="#数据集-Dataset-x2F-DataLoader" class="headerlink" title="数据集(Dataset&#x2F;DataLoader)"></a><a id="Dataset/DataLoader"><em>数据集(Dataset&#x2F;DataLoader)</em></a></h2><hr>
<hr>
<h2 id="损失函数-Loss-function"><a href="#损失函数-Loss-function" class="headerlink" title="损失函数(Loss-function)"></a><a id="Loss-function"><em>损失函数(Loss-function)</em></a></h2><hr>
<hr>
<h2 id="优化器-Optimizer"><a href="#优化器-Optimizer" class="headerlink" title="优化器(Optimizer)"></a><a id="Optimizer"><em>优化器(Optimizer)</em></a></h2><hr>
<hr>
<h2 id="模型搭建-Model"><a href="#模型搭建-Model" class="headerlink" title="模型搭建(Model)"></a><a id="Model"><em>模型搭建(Model)</em></a></h2><hr>
<hr>
<h2 id="训练-x2F-测试-Train-x2F-Test"><a href="#训练-x2F-测试-Train-x2F-Test" class="headerlink" title="训练&#x2F;测试(Train&#x2F;Test)"></a><a id="Train/Test"><em>训练&#x2F;测试(Train&#x2F;Test)</em></a></h2><hr>
<hr>
<h2 id="可视化-Visualization"><a href="#可视化-Visualization" class="headerlink" title="可视化(Visualization)"></a><a id="Visualization"><em>可视化(Visualization)</em></a></h2><p><a href="#Top">To the top</a></p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/08/10/Matplotlib-Tutorial/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          Matplotlib-Tutorial
        
      </div>
    </a>
  
  
    <a href="/2022/08/10/markdown-tutorial/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Markdown Tutorial</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2022 宝可梦训练师
      </div>
        <div class="footer-right">
          <a href="https://space.bilibili.com/782159" target="_blank">访问我的哔哩哔哩</a> 
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: false,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




  </div>
</body>
</html>