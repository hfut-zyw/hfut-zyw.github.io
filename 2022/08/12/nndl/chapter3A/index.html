<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第3章（上）：线性分类理论解读 | Homepage | 张有为</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="第3章 线性分类分类是机器学习中最常见的一类任务，其预测标签是一些离散的类别（符号）。根据分类任务的类别数量又可以分为二分类任务和多分类任务。">
<meta property="og:type" content="article">
<meta property="og:title" content="第3章（上）：线性分类理论解读">
<meta property="og:url" content="http://example.com/2022/08/12/nndl/chapter3A/index.html">
<meta property="og:site_name" content="Homepage | 张有为">
<meta property="og:description" content="第3章 线性分类分类是机器学习中最常见的一类任务，其预测标签是一些离散的类别（符号）。根据分类任务的类别数量又可以分为二分类任务和多分类任务。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/797cd285fbb14112944378971ddf402a7a7f3d8fdc41461ebd4373b3770a3b46">
<meta property="og:image" content="http://example.com/img-nndl/output_5_2.png">
<meta property="og:image" content="http://example.com/img-nndl/output_13_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_38_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_44_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_49_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_65_1.png">
<meta property="og:image" content="http://example.com/img-nndl/output_69_1.png">
<meta property="article:published_time" content="2022-08-12T01:26:52.000Z">
<meta property="article:modified_time" content="2022-09-22T13:03:40.082Z">
<meta property="article:author" content="LabmemNo.001">
<meta property="article:tag" content="nndl案例与实践">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ai-studio-static-online.cdn.bcebos.com/797cd285fbb14112944378971ddf402a7a7f3d8fdc41461ebd4373b3770a3b46">
  
    <link rel="alternative" href="/atom.xml" title="Homepage | 张有为" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/icon.gif">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://priesttomb.github.io/js/md5.min.js"></script>

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
<meta name="generator" content="Hexo 6.2.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/face.png" class="js-avatar show" style="width: 100%;height: 100%;opacity: 1;">
		</a>

		<hgroup>
			<h1 class="header-author"><a href="/">LabmemNo.001</a></h1>
		</hgroup>

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">

					<nav class="header-menu">
						<ul>
						
							<li><a href="/2022/08/09/me/aboutme">个人简介</a>
							</li>
				        
							<li><a href="/categories/notes">一些随写</a>
							</li>
				        
							<li><a href="/categories/analysis">抽象分析基础</a>
							</li>
				        
							<li><a href="/categories/probability">高等概率统计</a>
							</li>
				        
							<li><a href="/categories/opt">矩阵论与最优化</a>
							</li>
				        
						</ul>
					</nav>

					<nav class="header-menu">
						<ul>
						<li><a style="color:rgb(228, 117, 238)">Deep Learning </a>
							<nav class="header-submenu">
								<ul>
									
										<li><a href="/categories/pytorch"><div>Pytorch</div></a></li>  
									
										<li><a href="/categories/pydata"><div>数据处理</div></a></li>  
									
										<li><a href="/categories/nndl"><div>nndl案例与实践</div></a></li>  
									
								</ul>
							</nav>	
						</li>     
						</ul>
					</nav>
					
								
					


					<nav class="half-header-menu">
						<a class="hide">Home</a>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
					        
								<a class="mail" target="_blank" href="mailto:lucario@qq.com" title="mail">mail</a>
					        
						</div>
						<!-- music -->
						
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Matplotlib%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">Matplotlib笔记</a> <a href="/tags/Pandas%E7%AC%94%E8%AE%B0/" style="font-size: 18px;">Pandas笔记</a> <a href="/tags/Pytorch%E7%AC%94%E8%AE%B0/" style="font-size: 12px;">Pytorch笔记</a> <a href="/tags/nndl%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/" style="font-size: 20px;">nndl案例与实践</a> <a href="/tags/%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/" style="font-size: 14px;">优化笔记</a> <a href="/tags/%E5%85%B3%E4%BA%8E%E6%88%91/" style="font-size: 10px;">关于我</a> <a href="/tags/%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0/" style="font-size: 16px;">分析笔记</a> <a href="/tags/%E9%9A%8F%E5%86%99/" style="font-size: 14px;">随写</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
	<div class="overlay">
		<div class="slider-trigger"></div>
		<h1 class="header-author js-mobile-header hide"></h1>
	</div>
  <div class="intrude-less">
	  <header id="header" class="inner">
		  <div class="profilepic">
			  <img src="/img/face4.png" class="js-avatar show" style="width: 100%;height: 100%;opacity: 1;">
		  </div>
		  <hgroup>
			<h1 class="header-author"></h1>
		  </hgroup>
		  

		  <nav class="header-menu">
			  <ul>
			  
				  <li><a href="/2022/08/09/me/aboutme" style="color:rgba(69, 7, 241, 0.904)" >个人简介</a></li>
			  
				  <li><a href="/categories/notes" style="color:rgba(69, 7, 241, 0.904)" >一些随写</a></li>
			  
				  <li><a href="/categories/analysis" style="color:rgba(69, 7, 241, 0.904)" >抽象分析基础</a></li>
			  
				  <li><a href="/categories/probability" style="color:rgba(69, 7, 241, 0.904)" >高等概率统计</a></li>
			  
				  <li><a href="/categories/opt" style="color:rgba(69, 7, 241, 0.904)" >矩阵论与最优化</a></li>
			  
			  <div class="clearfix"></div>
			  </ul>
		  </nav>

		  <nav class="header-menu">
			  <ul>
			  <li><a style="color:rgba(69, 7, 241, 0.904)">Deep Learning </a>
				  <nav class="header-submenu">
					  <ul>
						  
							  <li><a href="/categories/pytorch" style="color:rgba(69, 7, 241, 0.904)" ><div>Pytorch</div></a></li>  
						  
							  <li><a href="/categories/pydata" style="color:rgba(69, 7, 241, 0.904)" ><div>数据处理</div></a></li>  
						  
							  <li><a href="/categories/nndl" style="color:rgba(69, 7, 241, 0.904)" ><div>nndl案例与实践</div></a></li>  
						  
					  </ul>
				  </nav>	
			  </li>     
			  </ul>
		  </nav>

		  <nav class="header-nav">
			  <div class="social">
				  
					  <a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
				  
					  <a class="mail" target="_blank" href="mailto:lucario@qq.com" title="mail">mail</a>
				  
			  </div>
		  </nav>
	  </header>				
  </div>
</nav>
      <div class="body-wrap">

<article id="post-nndl/chapter3A" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/08/12/nndl/chapter3A/" class="article-date">
  	<time datetime="2022-08-12T01:26:52.000Z" itemprop="datePublished">2022-08-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第3章（上）：线性分类理论解读
      
          <span class="title-pop-out"></a>
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nndl%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/" rel="tag">nndl案例与实践</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/nndl/">nndl</a>
	</div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第3章-线性分类"><a href="#第3章-线性分类" class="headerlink" title="第3章 线性分类"></a>第3章 线性分类</h1><p>分类是机器学习中最常见的一类任务，其预测标签是一些离散的类别（符号）。根据分类任务的类别数量又可以分为二分类任务和多分类任务。</p>
<span id="more"></span>
<p>线性分类是指利用一个或多个线性函数将样本进行分类。常用的线性分类模型有Logistic回归和Softmax回归。<br>Logistic回归是一种常用的处理二分类问题的线性模型。Softmax回归是Logistic回归在多分类问题上的推广。</p>
<p>在学习本章内容前，建议您先阅读《神经网络与深度学习》第3章：线性模型的相关内容，关键知识点如 <strong>图3.1</strong> 所示，以便更好的理解和掌握相应的理论知识，及其在实践中的应用方法。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/797cd285fbb14112944378971ddf402a7a7f3d8fdc41461ebd4373b3770a3b46" width=400></center>



<p><br><center>图3.1 线性模型关键知识点回顾</center></br></p>
<p>本章内容基于 <strong>《神经网络与深度学习》第3章：线性模型</strong> 相关内容进行设计，主要包含两部分：</p>
<ul>
<li><p><strong>模型解读</strong>：介绍两个最常用的线性分类模型Logistic回归和Softmax回归的原理剖析和相应的代码实现。通过理论和代码的结合，加深对线性模型的理解；</p>
</li>
<li><p><strong>案例实践</strong>：基于Softmax回归算法完成鸢尾花分类任务。</p>
</li>
</ul>
<h2 id="3-1-基于Logistic回归的二分类任务"><a href="#3-1-基于Logistic回归的二分类任务" class="headerlink" title="3.1 基于Logistic回归的二分类任务"></a>3.1 基于Logistic回归的二分类任务</h2><p>在本节中，我们实现一个Logistic回归模型，并对一个简单的数据集进行二分类实验。</p>
<h3 id="3-1-1-数据集构建"><a href="#3-1-1-数据集构建" class="headerlink" title="3.1.1 数据集构建"></a>3.1.1 数据集构建</h3><p>我们首先构建一个简单的分类任务，并构建训练集、验证集和测试集。<br>本任务的数据来自带噪音的两个弯月形状函数，每个弯月对一个类别。我们采集1000条样本，每个样本包含2个特征。</p>
<p>数据集的构建函数<code>make_moons</code>的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_moons</span>(<span class="params">n_samples=<span class="number">1000</span>, shuffle=<span class="literal">True</span>, noise=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    生成带噪音的弯月形状数据</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - n_samples：数据量大小，数据类型为int</span></span><br><span class="line"><span class="string">        - shuffle：是否打乱数据，数据类型为bool</span></span><br><span class="line"><span class="string">        - noise：以多大的程度增加噪声，数据类型为None或float，noise为None时表示不增加噪声</span></span><br><span class="line"><span class="string">    输出：</span></span><br><span class="line"><span class="string">        - X：特征数据，shape=[n_samples,2]</span></span><br><span class="line"><span class="string">        - y：标签数据, shape=[n_samples]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n_samples_out = n_samples // <span class="number">2</span></span><br><span class="line">    n_samples_in = n_samples - n_samples_out</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 采集第1类数据，特征为(x,y)</span></span><br><span class="line">    <span class="comment"># 使用&#x27;paddle.linspace&#x27;在0到pi上均匀取n_samples_out个值</span></span><br><span class="line">    <span class="comment"># 使用&#x27;paddle.cos&#x27;计算上述取值的余弦值作为特征1，使用&#x27;paddle.sin&#x27;计算上述取值的正弦值作为特征2</span></span><br><span class="line">    outer_circ_x = paddle.cos(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_out))</span><br><span class="line">    outer_circ_y = paddle.sin(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_out))</span><br><span class="line"></span><br><span class="line">    inner_circ_x = <span class="number">1</span> - paddle.cos(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_in))</span><br><span class="line">    inner_circ_y = <span class="number">0.5</span> - paddle.sin(paddle.linspace(<span class="number">0</span>, math.pi, n_samples_in))</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;outer_circ_x.shape:&#x27;</span>, outer_circ_x.shape, <span class="string">&#x27;outer_circ_y.shape:&#x27;</span>, outer_circ_y.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;inner_circ_x.shape:&#x27;</span>, inner_circ_x.shape, <span class="string">&#x27;inner_circ_y.shape:&#x27;</span>, inner_circ_y.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用&#x27;paddle.concat&#x27;将两类数据的特征1和特征2分别延维度0拼接在一起，得到全部特征1和特征2</span></span><br><span class="line">    <span class="comment"># 使用&#x27;paddle.stack&#x27;将两类特征延维度1堆叠在一起</span></span><br><span class="line">    X = paddle.stack(</span><br><span class="line">        [paddle.concat([outer_circ_x, inner_circ_x]),</span><br><span class="line">        paddle.concat([outer_circ_y, inner_circ_y])],</span><br><span class="line">        axis=<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;after concat shape:&#x27;</span>, paddle.concat([outer_circ_x, inner_circ_x]).shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X shape:&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用&#x27;paddle. zeros&#x27;将第一类数据的标签全部设置为0</span></span><br><span class="line">    <span class="comment"># 使用&#x27;paddle. ones&#x27;将第一类数据的标签全部设置为1</span></span><br><span class="line">    y = paddle.concat(</span><br><span class="line">        [paddle.zeros(shape=[n_samples_out]), paddle.ones(shape=[n_samples_in])]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;y shape:&#x27;</span>, y.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果shuffle为True，将所有数据打乱</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.randperm&#x27;生成一个数值在0到X.shape[0]，随机排列的一维Tensor做索引值，用于打乱数据</span></span><br><span class="line">        idx = paddle.randperm(X.shape[<span class="number">0</span>])</span><br><span class="line">        X = X[idx]</span><br><span class="line">        y = y[idx]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果noise不为None，则给特征值加入噪声</span></span><br><span class="line">    <span class="keyword">if</span> noise <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.normal&#x27;生成符合正态分布的随机Tensor作为噪声，并加到原始特征上</span></span><br><span class="line">        X += paddle.normal(mean=<span class="number">0.0</span>, std=noise, shape=X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br></pre></td></tr></table></figure>
<p>随机采集1000个样本，并进行可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采样1000个样本</span></span><br><span class="line">n_samples = <span class="number">1000</span></span><br><span class="line">X, y = make_moons(n_samples=n_samples, shuffle=<span class="literal">True</span>, noise=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 可视化生产的数据集，不同颜色代表不同类别</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.scatter(x=X[:, <span class="number">0</span>].tolist(), y=X[:, <span class="number">1</span>].tolist(), marker=<span class="string">&#x27;*&#x27;</span>, c=y.tolist())</span><br><span class="line">plt.xlim(-<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">plt.ylim(-<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;linear-dataset-vis.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>outer_circ_x.shape: [500] outer_circ_y.shape: [500]
inner_circ_x.shape: [500] inner_circ_y.shape: [500]
after concat shape: [1000]
X shape: [1000, 2]
y shape: [1000]


/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working
  from collections import MutableMapping
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working
  from collections import Iterable, Mapping
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working
  from collections import Sized
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2349: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working
  if isinstance(obj, collections.Iterator):
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2366: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working
  return list(data) if isinstance(data, collections.MappingView) else data
</code></pre><p><img src="/img-nndl/output_5_2.png" alt="png"></p>
<p>将1000条样本数据拆分成训练集、验证集和测试集，其中训练集640条、验证集160条、测试集200条。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_train = <span class="number">640</span></span><br><span class="line">num_dev = <span class="number">160</span></span><br><span class="line">num_test = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">X_train, y_train = X[:num_train], y[:num_train]</span><br><span class="line">X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]</span><br><span class="line">X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]</span><br><span class="line"></span><br><span class="line">y_train = y_train.reshape([-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">y_dev = y_dev.reshape([-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">y_test = y_test.reshape([-<span class="number">1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>这样，我们就完成了Moon1000数据集的构建。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印X_train和y_train的维度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train shape: &quot;</span>, X_train.shape, <span class="string">&quot;y_train shape: &quot;</span>, y_train.shape)</span><br></pre></td></tr></table></figure>
<pre><code>X_train shape:  [640, 2] y_train shape:  [640, 1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印一下前5个数据的标签</span></span><br><span class="line"><span class="built_in">print</span> (y_train[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Tensor(shape=[5, 1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[1.],
        [1.],
        [1.],
        [1.],
        [1.]])
</code></pre><h3 id="3-1-2-模型构建"><a href="#3-1-2-模型构建" class="headerlink" title="3.1.2 模型构建"></a>3.1.2 模型构建</h3><p>Logistic回归是一种常用的处理二分类问题的线性模型。与线性回归一样，Logistic回归也会将输入特征与权重做线性叠加。不同之处在于，Logistic回归引入了非线性函数$g:\mathbb{R}^D \rightarrow (0,1)$，预测类别标签的后验概率 $p(y=1|\mathbf x)$ ，从而解决连续的线性函数不适合进行分类的问题。</p>
<script type="math/tex; mode=display">
p(y=1|\mathbf x) = \sigma(\mathbf w^ \mathrm{ T } \mathbf x+b),（3.1）</script><p>其中判别函数$\sigma(\cdot)$为Logistic函数，也称为激活函数，作用是将线性函数$f(\mathbf x;\mathbf w,b)$的输出从实数区间“挤压”到（0,1）之间，用来表示概率。Logistic函数定义为：</p>
<script type="math/tex; mode=display">
\sigma(x) = \frac{1}{1+\exp(-x)}。（3.2）</script><p><strong>Logistic函数</strong></p>
<p>Logistic函数的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义Logistic函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + paddle.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在[-10,10]的范围内生成一系列的输入值，用于绘制函数曲线</span></span><br><span class="line">x = paddle.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">10000</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x.tolist(), logistic(x).tolist(), color=<span class="string">&quot;#E20079&quot;</span>, label=<span class="string">&quot;Logistic Function&quot;</span>)</span><br><span class="line"><span class="comment"># 设置坐标轴</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line"><span class="comment"># 取消右侧和上侧坐标轴</span></span><br><span class="line">ax.spines[<span class="string">&#x27;top&#x27;</span>].set_color(<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">ax.spines[<span class="string">&#x27;right&#x27;</span>].set_color(<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># 设置默认的x轴和y轴方向</span></span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">&#x27;bottom&#x27;</span>) </span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"><span class="comment"># 设置坐标原点为(0,0)</span></span><br><span class="line">ax.spines[<span class="string">&#x27;left&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>,<span class="number">0</span>))</span><br><span class="line">ax.spines[<span class="string">&#x27;bottom&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>,<span class="number">0</span>))</span><br><span class="line"><span class="comment"># 添加图例</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">&#x27;linear-logistic.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_13_0.png" alt="png"><br>​    </p>
<p>从输出结果看，当输入在0附近时，Logistic函数近似为线性函数；而当输入值非常大或非常小时，函数会对输入进行抑制。输入越小，则越接近0；输入越大，则越接近1。正因为Logistic函数具有这样的性质，使得其输出可以直接看作为概率分布。</p>
<p><strong>Logistic回归算子</strong></p>
<p>Logistic回归模型其实就是线性层与Logistic函数的组合，通常会将 Logistic回归模型中的权重和偏置初始化为0，同时，为了提高预测样本的效率，我们将$N$个样本归为一组进行成批地预测。</p>
<script type="math/tex; mode=display">
\hat{\mathbf y} = p(\mathbf y|\mathbf x) = \sigma(\boldsymbol{X} \boldsymbol{w} + b), (3.3)</script><p>其中$\boldsymbol{X}\in \mathbb{R}^{N\times D}$为$N$个样本的特征矩阵，$\hat{\boldsymbol{y}}$为$N$个样本的预测值构成的$N$维向量。</p>
<p>这里，我们构建一个Logistic回归算子，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> op</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">model_LR</span>(op.Op):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(model_LR, self).__init__()</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        <span class="comment"># 将线性层的权重参数全部初始化为0</span></span><br><span class="line">        self.params[<span class="string">&#x27;w&#x27;</span>] = paddle.zeros(shape=[input_dim, <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># self.params[&#x27;w&#x27;] = paddle.normal(mean=0, std=0.01, shape=[input_dim, 1])</span></span><br><span class="line">        <span class="comment"># 将线性层的偏置参数初始化为0</span></span><br><span class="line">        self.params[<span class="string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - inputs: shape=[N,D], N是样本数量，D为特征维度</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - outputs：预测标签为1的概率，shape=[N,1]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 线性计算</span></span><br><span class="line">        score = paddle.matmul(inputs, self.params[<span class="string">&#x27;w&#x27;</span>]) + self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        <span class="comment"># Logistic 函数</span></span><br><span class="line">        outputs = logistic(score)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p><strong>测试一下</strong></p>
<p>随机生成3条长度为4的数据输入Logistic回归模型，观察输出结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子，保持每次运行结果一致</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 随机生成3条长度为4的数据</span></span><br><span class="line">inputs = paddle.randn(shape=[<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Input is:&#x27;</span>, inputs)</span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = model_LR(<span class="number">4</span>)</span><br><span class="line">outputs = model(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Output is:&#x27;</span>, outputs)</span><br></pre></td></tr></table></figure>
<pre><code>Input is: Tensor(shape=[3, 4], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[-0.75711036, -0.38059190,  0.10946669,  1.34467661],
        [-0.84002435, -1.27341712,  2.47224617,  0.14070207],
        [ 0.60608417,  0.23396523,  1.35604191,  0.10350471]])
Output is: Tensor(shape=[3, 1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[0.50000000],
        [0.50000000],
        [0.50000000]])
</code></pre><p>从输出结果看，模型最终的输出$g(·)$恒为0.5。这是由于采用全0初始化后，不论输入值的大小为多少，Logistic函数的输入值恒为0，因此输出恒为0.5。</p>
<h3 id="3-1-3-损失函数"><a href="#3-1-3-损失函数" class="headerlink" title="3.1.3 损失函数"></a>3.1.3 损失函数</h3><p>在模型训练过程中，需要使用损失函数来量化预测值和真实值之间的差异。<br>给定一个分类任务，$\mathbf y$表示样本$\mathbf x$的标签的真实概率分布，向量$\hat{\mathbf y}=p(\mathbf y|\mathbf x)$表示预测的标签概率分布。<br>训练目标是使得$\hat{\mathbf y}$尽可能地接近$\mathbf y$，通常可以使用<strong>交叉熵损失函数</strong>。<br>在给定$\mathbf y$的情况下，如果预测的概率分布$\hat{\mathbf y}$与标签真实的分布$\mathbf y$越接近，则交叉熵越小；如果$p(\mathbf x)$和$\mathbf y$越远，交叉熵就越大。</p>
<p>对于二分类任务，我们只需要计算$\hat{y}=p(y=1|\mathbf x)$，用$1-\hat{y}$来表示$p(y=0|\mathbf x)$。<br>给定有$N$个训练样本的训练集$\{(\mathbf x^{(n)},y^{(n)})\} ^N_{n=1}$，使用交叉熵损失函数，Logistic回归的风险函数计算方式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\cal R(\mathbf w,b) &= -\frac{1}{N}\sum_{n=1}^N \Big(y^{(n)}\log\hat{y}^{(n)} + (1-y^{(n)})\log(1-\hat{y}^{(n)})\Big)。（3.4）
\end{aligned}</script><p>向量形式可以表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\cal R(\mathbf w,b) &= -\frac{1}{N}\Big(\mathbf y^ \mathrm{ T }  \log\hat{\mathbf y} + (1-\mathbf y)^ \mathrm{ T } \log(1-\hat{\mathbf y})\Big),（3.5）
\end{aligned}</script><p>其中$\mathbf y\in [0,1]^N$为$N$个样本的真实标签构成的$N$维向量，$\hat{\mathbf y}$为$N$个样本标签为1的后验概率构成的$N$维向量。</p>
<p>二分类任务的交叉熵损失函数的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现交叉熵损失函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BinaryCrossEntropyLoss</span>(op.Op):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.predicts = <span class="literal">None</span></span><br><span class="line">        self.labels = <span class="literal">None</span></span><br><span class="line">        self.num = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, predicts, labels</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(predicts, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, predicts, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - predicts：预测值，shape=[N, 1]，N为样本数量</span></span><br><span class="line"><span class="string">            - labels：真实标签，shape=[N, 1]</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - 损失值：shape=[1]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.predicts = predicts</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.num = self.predicts.shape[<span class="number">0</span>]</span><br><span class="line">        loss = -<span class="number">1.</span> / self.num * (paddle.matmul(self.labels.t(), paddle.log(self.predicts)) + paddle.matmul((<span class="number">1</span>-self.labels.t()), paddle.log(<span class="number">1</span>-self.predicts)))</span><br><span class="line">        loss = paddle.squeeze(loss, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试一下</span></span><br><span class="line"><span class="comment"># 生成一组长度为3，值为1的标签数据</span></span><br><span class="line">labels = paddle.ones(shape=[<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 计算风险函数</span></span><br><span class="line">bce_loss = BinaryCrossEntropyLoss()</span><br><span class="line"><span class="built_in">print</span>(bce_loss(outputs, labels))</span><br></pre></td></tr></table></figure>
<pre><code>Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [0.69314718])
</code></pre><h3 id="3-1-4-模型优化"><a href="#3-1-4-模型优化" class="headerlink" title="3.1.4 模型优化"></a>3.1.4 模型优化</h3><p>不同于线性回归中直接使用最小二乘法即可进行模型参数的求解，Logistic回归需要使用优化算法对模型参数进行有限次地迭代来获取更优的模型，从而尽可能地降低风险函数的值。<br>在机器学习任务中，最简单、常用的优化算法是梯度下降法。</p>
<p>使用梯度下降法进行模型优化，首先需要初始化参数$\mathbf W$和 $b$，然后不断地计算它们的梯度，并沿梯度的反方向更新参数。</p>
<h4 id="3-1-4-1-梯度计算"><a href="#3-1-4-1-梯度计算" class="headerlink" title="3.1.4.1 梯度计算"></a>3.1.4.1 梯度计算</h4><p>在Logistic回归中，风险函数$\cal R(\mathbf w,b)$ 关于参数$\mathbf w$和$b$的偏导数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \cal R(\mathbf w,b)}{\partial \mathbf w} = -\frac{1}{N}\sum_{n=1}^N \mathbf x^{(n)}(y^{(n)}- \hat{y}^{(n)}) = -\frac{1}{N} \mathbf X^ \mathrm{ T }  (\mathbf y-\hat{\mathbf y})
\end{aligned}，（3.6）</script><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \cal R(\mathbf w,b)}{\partial b} = -\frac{1}{N}\sum_{n=1}^N (y^{(n)}- \hat{y}^{(n)}) = -\frac{1}{N} \mathbf {sum}(\mathbf y-\hat{\mathbf y})
\end{aligned}。（3.7）</script><p>通常将偏导数的计算过程定义在Logistic回归算子的<code>backward</code>函数中，代码实现如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">model_LR</span>(op.Op):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(model_LR, self).__init__()</span><br><span class="line">        <span class="comment"># 存放线性层参数</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        <span class="comment"># 将线性层的权重参数全部初始化为0</span></span><br><span class="line">        self.params[<span class="string">&#x27;w&#x27;</span>] = paddle.zeros(shape=[input_dim, <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># self.params[&#x27;w&#x27;] = paddle.normal(mean=0, std=0.01, shape=[input_dim, 1])</span></span><br><span class="line">        <span class="comment"># 将线性层的偏置参数初始化为0</span></span><br><span class="line">        self.params[<span class="string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 存放参数的梯度</span></span><br><span class="line">        self.grads = &#123;&#125;</span><br><span class="line">        self.X = <span class="literal">None</span></span><br><span class="line">        self.outputs = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        self.X = inputs</span><br><span class="line">        <span class="comment"># 线性计算</span></span><br><span class="line">        score = paddle.matmul(inputs, self.params[<span class="string">&#x27;w&#x27;</span>]) + self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        <span class="comment"># Logistic 函数</span></span><br><span class="line">        self.outputs = logistic(score)</span><br><span class="line">        <span class="keyword">return</span> self.outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - labels：真实标签，shape=[N, 1]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        N = labels.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 计算偏导数</span></span><br><span class="line">        self.grads[<span class="string">&#x27;w&#x27;</span>] = -<span class="number">1</span> / N * paddle.matmul(self.X.t(), (labels - self.outputs))</span><br><span class="line">        self.grads[<span class="string">&#x27;b&#x27;</span>] = -<span class="number">1</span> / N * paddle.<span class="built_in">sum</span>(labels - self.outputs)</span><br></pre></td></tr></table></figure>
<h4 id="3-1-4-2-参数更新"><a href="#3-1-4-2-参数更新" class="headerlink" title="3.1.4.2 参数更新"></a>3.1.4.2 参数更新</h4><p>在计算参数的梯度之后，我们按照下面公式更新参数：</p>
<script type="math/tex; mode=display">
\mathbf w\leftarrow \mathbf w - \alpha \frac{\partial \cal R(\mathbf w,b)}{\partial \mathbf w}，（3.8）</script><script type="math/tex; mode=display">
b\leftarrow b - \alpha \frac{\partial \cal R(\mathbf w,b)}{\partial b}，（3.9）</script><p>其中$\alpha$ 为学习率。</p>
<p>将上面的参数更新过程包装为优化器，首先定义一个优化器基类<code>Optimizer</code>，方便后续所有的优化器调用。在这个基类中，需要初始化优化器的初始学习率<code>init_lr</code>，以及指定优化器需要优化的参数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器基类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Optimizer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_lr, model</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        优化器类初始化</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 初始化学习率，用于参数更新的计算</span></span><br><span class="line">        self.init_lr = init_lr</span><br><span class="line">        <span class="comment"># 指定优化器需要优化的模型</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        定义每次迭代如何更新参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>然后实现一个梯度下降法的优化器函数<code>SimpleBatchGD</code>来执行参数更新过程。其中<code>step</code>函数从模型的<code>grads</code>属性取出参数的梯度并更新。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleBatchGD</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_lr, model</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleBatchGD, self).__init__(init_lr=init_lr, model=model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        <span class="comment"># 遍历所有参数，按照公式(3.8)和(3.9)更新参数</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(self.model.params, <span class="built_in">dict</span>):</span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> self.model.params.keys():</span><br><span class="line">                self.model.params[key] = self.model.params[key] - self.init_lr * self.model.grads[key]</span><br></pre></td></tr></table></figure>
<h3 id="3-1-5-评价指标"><a href="#3-1-5-评价指标" class="headerlink" title="3.1.5 评价指标"></a>3.1.5 评价指标</h3><p>在分类任务中，通常使用准确率（Accuracy）作为评价指标。如果模型预测的类别与真实类别一致，则说明模型预测正确。准确率即正确预测的数量与总的预测数量的比值：</p>
<script type="math/tex; mode=display">
\mathcal{A} 
= 
\frac{1}{N}
    \sum_{n=1}^N
    I
        (y^{(n)} = \hat{y}^{(n)}),（3.10）</script><p>其中$I(·)$是指示函数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">preds, labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - preds：预测值，二分类时，shape=[N, 1]，N为样本数量，多分类时，shape=[N, C]，C为类别数量</span></span><br><span class="line"><span class="string">        - labels：真实标签，shape=[N, 1]</span></span><br><span class="line"><span class="string">    输出：</span></span><br><span class="line"><span class="string">        - 准确率：shape=[1]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 判断是二分类任务还是多分类任务，preds.shape[1]=1时为二分类任务，preds.shape[1]&gt;1时为多分类任务</span></span><br><span class="line">    <span class="keyword">if</span> preds.shape[<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 二分类时，判断每个概率值是否大于0.5，当大于0.5时，类别为1，否则类别为0</span></span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.cast&#x27;将preds的数据类型转换为float32类型</span></span><br><span class="line">        preds = paddle.cast((preds&gt;=<span class="number">0.5</span>),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 多分类时，使用&#x27;paddle.argmax&#x27;计算最大元素索引作为类别</span></span><br><span class="line">        preds = paddle.argmax(preds,axis=<span class="number">1</span>, dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> paddle.mean(paddle.cast(paddle.equal(preds, labels),dtype=<span class="string">&#x27;float32&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设模型的预测值为[[0.],[1.],[1.],[0.]]，真实类别为[[1.],[1.],[0.],[0.]]，计算准确率</span></span><br><span class="line">preds = paddle.to_tensor([[<span class="number">0.</span>],[<span class="number">1.</span>],[<span class="number">1.</span>],[<span class="number">0.</span>]])</span><br><span class="line">labels = paddle.to_tensor([[<span class="number">1.</span>],[<span class="number">1.</span>],[<span class="number">0.</span>],[<span class="number">0.</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuracy is:&quot;</span>, accuracy(preds, labels))</span><br></pre></td></tr></table></figure>
<pre><code>accuracy is: Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [0.50000000])
</code></pre><h3 id="3-1-6-完善Runner类"><a href="#3-1-6-完善Runner类" class="headerlink" title="3.1.6 完善Runner类"></a>3.1.6 完善Runner类</h3><p>基于RunnerV1，本章的RunnerV2类在训练过程中使用梯度下降法进行网络优化，模型训练过程中计算在训练集和验证集上的损失及评估指标并打印，训练过程中保存最优模型。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用RunnerV2类封装整个训练过程</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RunnerV2</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, optimizer, metric, loss_fn</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.metric = metric</span><br><span class="line">        <span class="comment"># 记录训练过程中的评价指标变化情况</span></span><br><span class="line">        self.train_scores = []</span><br><span class="line">        self.dev_scores = []</span><br><span class="line">        <span class="comment"># 记录训练过程中的损失函数变化情况</span></span><br><span class="line">        self.train_loss = []</span><br><span class="line">        self.dev_loss = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, train_set, dev_set, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 传入训练轮数，如果没有传入值则默认为0</span></span><br><span class="line">        num_epochs = kwargs.get(<span class="string">&quot;num_epochs&quot;</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 传入log打印频率，如果没有传入值则默认为100</span></span><br><span class="line">        log_epochs = kwargs.get(<span class="string">&quot;log_epochs&quot;</span>, <span class="number">100</span>)</span><br><span class="line">        <span class="comment"># 传入模型保存路径，如果没有传入值则默认为&quot;best_model.pdparams&quot;</span></span><br><span class="line">        save_path = kwargs.get(<span class="string">&quot;save_path&quot;</span>, <span class="string">&quot;best_model.pdparams&quot;</span>)</span><br><span class="line">        <span class="comment"># 梯度打印函数，如果没有传入则默认为&quot;None&quot;</span></span><br><span class="line">        print_grads = kwargs.get(<span class="string">&quot;print_grads&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># 记录全局最优指标</span></span><br><span class="line">        best_score = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 进行num_epochs轮训练</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            X, y = train_set</span><br><span class="line">            <span class="comment"># 获取模型预测</span></span><br><span class="line">            logits = self.model(X)</span><br><span class="line">            <span class="comment"># 计算交叉熵损失</span></span><br><span class="line">            trn_loss = self.loss_fn(logits, y).item()</span><br><span class="line">            self.train_loss.append(trn_loss)</span><br><span class="line">            <span class="comment"># 计算评价指标</span></span><br><span class="line">            trn_score = self.metric(logits, y).item()</span><br><span class="line">            self.train_scores.append(trn_score)</span><br><span class="line">            <span class="comment"># 计算参数梯度</span></span><br><span class="line">            self.model.backward(y)</span><br><span class="line">            <span class="keyword">if</span> print_grads <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 打印每一层的梯度</span></span><br><span class="line">                print_grads(self.model)</span><br><span class="line">            <span class="comment"># 更新模型参数</span></span><br><span class="line">            self.optimizer.step()</span><br><span class="line">            dev_score, dev_loss = self.evaluate(dev_set)</span><br><span class="line">            <span class="comment"># 如果当前指标为最优指标，保存该模型</span></span><br><span class="line">            <span class="keyword">if</span> dev_score &gt; best_score:</span><br><span class="line">                self.save_model(save_path)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;best accuracy performence has been updated: <span class="subst">&#123;best_score:<span class="number">.5</span>f&#125;</span> --&gt; <span class="subst">&#123;dev_score:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">                best_score = dev_score</span><br><span class="line">            <span class="keyword">if</span> epoch % log_epochs == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;[Train] epoch: <span class="subst">&#123;epoch&#125;</span>, loss: <span class="subst">&#123;trn_loss&#125;</span>, score: <span class="subst">&#123;trn_score&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;[Dev] epoch: <span class="subst">&#123;epoch&#125;</span>, loss: <span class="subst">&#123;dev_loss&#125;</span>, score: <span class="subst">&#123;dev_score&#125;</span>&quot;</span>)</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">self, data_set</span>):</span><br><span class="line">        X, y = data_set</span><br><span class="line">        <span class="comment"># 计算模型输出</span></span><br><span class="line">        logits = self.model(X)</span><br><span class="line">        <span class="comment"># 计算损失函数</span></span><br><span class="line">        loss = self.loss_fn(logits, y).item()</span><br><span class="line">        self.dev_loss.append(loss)</span><br><span class="line">        <span class="comment"># 计算评价指标</span></span><br><span class="line">        score = self.metric(logits, y).item()</span><br><span class="line">        self.dev_scores.append(score)</span><br><span class="line">        <span class="keyword">return</span> score, loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.model(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_model</span>(<span class="params">self, save_path</span>):</span><br><span class="line">        paddle.save(self.model.params, save_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_model</span>(<span class="params">self, model_path</span>):</span><br><span class="line">        self.model.params = paddle.load(model_path)</span><br></pre></td></tr></table></figure>
<h3 id="3-1-7-模型训练"><a href="#3-1-7-模型训练" class="headerlink" title="3.1.7 模型训练"></a>3.1.7 模型训练</h3><p>下面进行Logistic回归模型的训练，使用交叉熵损失函数和梯度下降法进行优化。<br>使用训练集和验证集进行模型训练，共训练 500个epoch，每隔50个epoch打印出训练集上的指标。<br>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子，保持每次运行结果一致</span></span><br><span class="line">paddle.seed(<span class="number">102</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征维度</span></span><br><span class="line">input_dim = <span class="number">2</span></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = model_LR(input_dim=input_dim)</span><br><span class="line"><span class="comment"># 指定优化器</span></span><br><span class="line">optimizer = SimpleBatchGD(init_lr=lr, model=model)</span><br><span class="line"><span class="comment"># 指定损失函数</span></span><br><span class="line">loss_fn = BinaryCrossEntropyLoss()</span><br><span class="line"><span class="comment"># 指定评价方式</span></span><br><span class="line">metric = accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化RunnerV2类，并传入训练配置</span></span><br><span class="line">runner = RunnerV2(model, optimizer, metric, loss_fn)</span><br><span class="line"></span><br><span class="line">runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="number">500</span>, log_epochs=<span class="number">50</span>, save_path=<span class="string">&quot;best_model.pdparams&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>best accuracy performence has been updated: 0.00000 --&gt; 0.77500
[Train] epoch: 0, loss: 0.6931471824645996, score: 0.4984374940395355
[Dev] epoch: 0, loss: 0.6841436624526978, score: 0.7749999761581421
best accuracy performence has been updated: 0.77500 --&gt; 0.78125
best accuracy performence has been updated: 0.78125 --&gt; 0.78750
best accuracy performence has been updated: 0.78750 --&gt; 0.79375
best accuracy performence has been updated: 0.79375 --&gt; 0.80000
[Train] epoch: 50, loss: 0.4925098121166229, score: 0.796875
[Dev] epoch: 50, loss: 0.5093844532966614, score: 0.800000011920929
[Train] epoch: 100, loss: 0.4479391276836395, score: 0.807812511920929
[Dev] epoch: 100, loss: 0.4758768677711487, score: 0.78125
[Train] epoch: 150, loss: 0.43075019121170044, score: 0.8125
[Dev] epoch: 150, loss: 0.46530404686927795, score: 0.768750011920929
[Train] epoch: 200, loss: 0.4224272668361664, score: 0.8125
[Dev] epoch: 200, loss: 0.46143728494644165, score: 0.7749999761581421
[Train] epoch: 250, loss: 0.4178687036037445, score: 0.817187488079071
[Dev] epoch: 250, loss: 0.4600875973701477, score: 0.7749999761581421
[Train] epoch: 300, loss: 0.41517534852027893, score: 0.8187500238418579
[Dev] epoch: 300, loss: 0.4598068296909332, score: 0.7749999761581421
[Train] epoch: 350, loss: 0.41350212693214417, score: 0.817187488079071
[Dev] epoch: 350, loss: 0.46000194549560547, score: 0.762499988079071
[Train] epoch: 400, loss: 0.41242581605911255, score: 0.8187500238418579
[Dev] epoch: 400, loss: 0.4604030251502991, score: 0.762499988079071
[Train] epoch: 450, loss: 0.4117158055305481, score: 0.8187500238418579
[Dev] epoch: 450, loss: 0.46087923645973206, score: 0.768750011920929
</code></pre><p>可视化观察训练集与验证集的准确率和损失的变化情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化观察训练集与验证集的指标变化情况</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">runner,fig_name</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    epochs = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(runner.train_scores))]</span><br><span class="line">    <span class="comment"># 绘制训练损失变化曲线</span></span><br><span class="line">    plt.plot(epochs, runner.train_loss, color=<span class="string">&#x27;#8E004D&#x27;</span>, label=<span class="string">&quot;Train loss&quot;</span>)</span><br><span class="line">    <span class="comment"># 绘制评价损失变化曲线</span></span><br><span class="line">    plt.plot(epochs, runner.dev_loss, color=<span class="string">&#x27;#E20079&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&quot;Dev loss&quot;</span>)</span><br><span class="line">    <span class="comment"># 绘制坐标轴和图例</span></span><br><span class="line">    plt.ylabel(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 绘制训练准确率变化曲线</span></span><br><span class="line">    plt.plot(epochs, runner.train_scores, color=<span class="string">&#x27;#8E004D&#x27;</span>, label=<span class="string">&quot;Train accuracy&quot;</span>)</span><br><span class="line">    <span class="comment"># 绘制评价准确率变化曲线</span></span><br><span class="line">    plt.plot(epochs, runner.dev_scores, color=<span class="string">&#x27;#E20079&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&quot;Dev accuracy&quot;</span>)</span><br><span class="line">    <span class="comment"># 绘制坐标轴和图例</span></span><br><span class="line">    plt.ylabel(<span class="string">&quot;score&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.savefig(fig_name)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot(runner,fig_name=<span class="string">&#x27;linear-acc.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_38_0.png" alt="png"><br>​    </p>
<p>从输出结果可以看到，在训练集与验证集上，loss得到了收敛，同时准确率指标都达到了较高的水平，训练比较充分。</p>
<h3 id="3-1-8-模型评价"><a href="#3-1-8-模型评价" class="headerlink" title="3.1.8 模型评价"></a>3.1.8 模型评价</h3><p>使用测试集对训练完成后的最终模型进行评价，观察模型在测试集上的准确率和loss数据。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score, loss = runner.evaluate([X_test, y_test])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(score, loss))</span><br></pre></td></tr></table></figure>
<pre><code>[Test] score/loss: 0.7950/0.4475
</code></pre><p>可视化观察拟合的决策边界 $\boldsymbol{X} \boldsymbol{w} + b=0$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decision_boundary</span>(<span class="params">w, b, x1</span>):</span><br><span class="line">    w1, w2 = w</span><br><span class="line">    x2 = (- w1 * x1 - b) / w2</span><br><span class="line">    <span class="keyword">return</span> x2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment"># 绘制原始数据</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>].tolist(), X[:, <span class="number">1</span>].tolist(), marker=<span class="string">&#x27;*&#x27;</span>, c=y.tolist())</span><br><span class="line"></span><br><span class="line">w = model.params[<span class="string">&#x27;w&#x27;</span>]</span><br><span class="line">b = model.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">x1 = paddle.linspace(-<span class="number">2</span>, <span class="number">3</span>, <span class="number">1000</span>)</span><br><span class="line">x2 = decision_boundary(w, b, x1)</span><br><span class="line"><span class="comment"># 绘制决策边界</span></span><br><span class="line">plt.plot(x1.tolist(), x2.tolist(), color=<span class="string">&quot;red&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_44_0.png" alt="png"><br>​    </p>
<h2 id="3-2-基于Softmax回归的多分类任务"><a href="#3-2-基于Softmax回归的多分类任务" class="headerlink" title="3.2 基于Softmax回归的多分类任务"></a>3.2 基于Softmax回归的多分类任务</h2><p>Logistic回归可以有效地解决二分类问题，但在分类任务中，还有一类多分类问题，即类别数$C$大于2 的分类问题。Softmax回归就是Logistic回归在多分类问题上的推广。</p>
<p>使用Softmax回归模型对一个简单的数据集进行多分类实验。</p>
<h3 id="3-2-1-数据集构建"><a href="#3-2-1-数据集构建" class="headerlink" title="3.2.1 数据集构建"></a>3.2.1 数据集构建</h3><p>我们首先构建一个简单的多分类任务，并构建训练集、验证集和测试集。<br>本任务的数据来自3个不同的簇，每个簇对一个类别。我们采集1000条样本，每个样本包含2个特征。</p>
<p>数据集的构建函数<code>make_multi</code>的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_multiclass_classification</span>(<span class="params">n_samples=<span class="number">100</span>, n_features=<span class="number">2</span>, n_classes=<span class="number">3</span>, shuffle=<span class="literal">True</span>, noise=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    生成带噪音的多类别数据</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - n_samples：数据量大小，数据类型为int</span></span><br><span class="line"><span class="string">        - n_features：特征数量，数据类型为int</span></span><br><span class="line"><span class="string">        - shuffle：是否打乱数据，数据类型为bool</span></span><br><span class="line"><span class="string">        - noise：以多大的程度增加噪声，数据类型为None或float，noise为None时表示不增加噪声</span></span><br><span class="line"><span class="string">    输出：</span></span><br><span class="line"><span class="string">        - X：特征数据，shape=[n_samples,2]</span></span><br><span class="line"><span class="string">        - y：标签数据, shape=[n_samples,1]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算每个类别的样本数量</span></span><br><span class="line">    n_samples_per_class = [<span class="built_in">int</span>(n_samples / n_classes) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n_classes)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_samples - <span class="built_in">sum</span>(n_samples_per_class)):</span><br><span class="line">        n_samples_per_class[i % n_classes] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 将特征和标签初始化为0</span></span><br><span class="line">    X = paddle.zeros([n_samples, n_features])</span><br><span class="line">    y = paddle.zeros([n_samples], dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="comment"># 随机生成3个簇中心作为类别中心</span></span><br><span class="line">    centroids = paddle.randperm(<span class="number">2</span> ** n_features)[:n_classes]</span><br><span class="line">    centroids_bin = np.unpackbits(centroids.numpy().astype(<span class="string">&#x27;uint8&#x27;</span>)).reshape((-<span class="number">1</span>, <span class="number">8</span>))[:, -n_features:]</span><br><span class="line">    centroids = paddle.to_tensor(centroids_bin, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    <span class="comment"># 控制簇中心的分离程度</span></span><br><span class="line">    centroids = <span class="number">1.5</span> * centroids - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 随机生成特征值</span></span><br><span class="line">    X[:, :n_features] = paddle.randn(shape=[n_samples, n_features])</span><br><span class="line"></span><br><span class="line">    stop = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 将每个类的特征值控制在簇中心附近</span></span><br><span class="line">    <span class="keyword">for</span> k, centroid <span class="keyword">in</span> <span class="built_in">enumerate</span>(centroids):</span><br><span class="line">        start, stop = stop, stop + n_samples_per_class[k]</span><br><span class="line">        <span class="comment"># 指定标签值</span></span><br><span class="line">        y[start:stop] = k % n_classes</span><br><span class="line">        X_k = X[start:stop, :n_features]</span><br><span class="line">        <span class="comment"># 控制每个类别特征值的分散程度</span></span><br><span class="line">        A = <span class="number">2</span> * paddle.rand(shape=[n_features, n_features]) - <span class="number">1</span></span><br><span class="line">        X_k[...] = paddle.matmul(X_k, A)</span><br><span class="line">        X_k += centroid</span><br><span class="line">        X[start:stop, :n_features] = X_k</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果noise不为None，则给特征加入噪声</span></span><br><span class="line">    <span class="keyword">if</span> noise &gt; <span class="number">0.0</span>:</span><br><span class="line">        <span class="comment"># 生成noise掩膜，用来指定给那些样本加入噪声</span></span><br><span class="line">        noise_mask = paddle.rand([n_samples]) &lt; noise</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(noise_mask)):</span><br><span class="line">            <span class="keyword">if</span> noise_mask[i]:</span><br><span class="line">                <span class="comment"># 给加噪声的样本随机赋标签值</span></span><br><span class="line">                y[i] = paddle.randint(n_classes, shape=[<span class="number">1</span>]).astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="comment"># 如果shuffle为True，将所有数据打乱</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        idx = paddle.randperm(X.shape[<span class="number">0</span>])</span><br><span class="line">        X = X[idx]</span><br><span class="line">        y = y[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br></pre></td></tr></table></figure>
<p>随机采集1000个样本，并进行可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子，保持每次运行结果一致</span></span><br><span class="line">paddle.seed(<span class="number">102</span>)</span><br><span class="line"><span class="comment"># 采样1000个样本</span></span><br><span class="line">n_samples = <span class="number">1000</span></span><br><span class="line">X, y = make_multiclass_classification(n_samples=n_samples, n_features=<span class="number">2</span>, n_classes=<span class="number">3</span>, noise=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化生产的数据集，不同颜色代表不同类别</span></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.scatter(x=X[:, <span class="number">0</span>].tolist(), y=X[:, <span class="number">1</span>].tolist(), marker=<span class="string">&#x27;*&#x27;</span>, c=y.tolist())</span><br><span class="line">plt.savefig(<span class="string">&#x27;linear-dataset-vis2.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_49_0.png" alt="png"><br>​    </p>
<p>将实验数据拆分成训练集、验证集和测试集。其中训练集640条、验证集160条、测试集200条。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_train = <span class="number">640</span></span><br><span class="line">num_dev = <span class="number">160</span></span><br><span class="line">num_test = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">X_train, y_train = X[:num_train], y[:num_train]</span><br><span class="line">X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]</span><br><span class="line">X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印X_train和y_train的维度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train shape: &quot;</span>, X_train.shape, <span class="string">&quot;y_train shape: &quot;</span>, y_train.shape)</span><br></pre></td></tr></table></figure>
<pre><code>X_train shape:  [640, 2] y_train shape:  [640]
</code></pre><p>这样，我们就完成了Multi1000数据集的构建。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印前5个数据的标签</span></span><br><span class="line"><span class="built_in">print</span>(y_train[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Tensor(shape=[5], dtype=int32, place=CPUPlace, stop_gradient=True,
       [0, 1, 2, 2, 0])
</code></pre><h3 id="3-2-2-模型构建"><a href="#3-2-2-模型构建" class="headerlink" title="3.2.2 模型构建"></a>3.2.2 模型构建</h3><p>在Softmax回归中，对类别进行预测的方式是预测输入属于每个类别的条件概率。与Logistic 回归不同的是，Softmax回归的输出值个数等于类别数$C$，而每个类别的概率值则通过Softmax函数进行求解。</p>
<h4 id="3-2-2-1-Softmax函数"><a href="#3-2-2-1-Softmax函数" class="headerlink" title="3.2.2.1 Softmax函数"></a>3.2.2.1 Softmax函数</h4><p>Softmax函数可以将多个标量映射为一个概率分布。对于一个$K$维向量，$\mathbf x=[x_1,\cdots,x_K]$，Softmax的计算公式为</p>
<script type="math/tex; mode=display">
\mathrm{softmax}(x_k) = \frac{\exp(x_k)}{\sum_{i=1}^K \exp(x_i)}。（3.11）</script><p>在Softmax函数的计算过程中，要注意<strong>上溢出</strong>和<strong>下溢出</strong>的问题。假设Softmax 函数中所有的$x_k$都是相同大小的数值$a$，理论上，所有的输出都应该为$\frac{1}{k}$。但需要考虑如下两种特殊情况：</p>
<ul>
<li>$a$为一个非常大的负数，此时$\exp(a)$ 会发生下溢出现象。计算机在进行数值计算时，当数值过小，会被四舍五入为0。此时，Softmax函数的分母会变为0，导致计算出现问题；</li>
<li>$a$为一个非常大的正数，此时会导致$\exp(a)$发生上溢出现象，导致计算出现问题。</li>
</ul>
<p>为了解决上溢出和下溢出的问题，在计算Softmax函数时，可以使用$x_k - \max(\mathbf x)$代替$x_k$。 此时，通过减去最大值，$x_k$最大为0，避免了上溢出的问题；同时，分母中至少会包含一个值为1的项，从而也避免了下溢出的问题。</p>
<p>Softmax函数的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x为tensor</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - X：shape=[N, C]，N为向量数量，C为向量维度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x_max = paddle.<span class="built_in">max</span>(X, axis=<span class="number">1</span>, keepdim=<span class="literal">True</span>)<span class="comment">#N,1</span></span><br><span class="line">    x_exp = paddle.exp(X - x_max)</span><br><span class="line">    partition = paddle.<span class="built_in">sum</span>(x_exp, axis=<span class="number">1</span>, keepdim=<span class="literal">True</span>)<span class="comment">#N,1</span></span><br><span class="line">    <span class="keyword">return</span> x_exp / partition</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察softmax的计算方式</span></span><br><span class="line">X = paddle.to_tensor([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">predict = softmax(X)</span><br><span class="line"><span class="built_in">print</span>(predict)</span><br></pre></td></tr></table></figure>
<pre><code>Tensor(shape=[2, 4], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[0.21383820, 0.23632778, 0.26118258, 0.28865141],
        [0.03205860, 0.08714432, 0.23688281, 0.64391422]])
</code></pre><h4 id="3-2-2-2-Softmax回归算子"><a href="#3-2-2-2-Softmax回归算子" class="headerlink" title="3.2.2.2 Softmax回归算子"></a>3.2.2.2 Softmax回归算子</h4><p>在Softmax回归中，类别标签$y\in\{1,2,…,C\}$。给定一个样本$\mathbf x$，使用Softmax回归预测的属于类别$c$的条件概率为</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y=c|\mathbf x) &= \mathrm{softmax}(\mathbf w_c^T \mathbf x+b_c)，（3.12）
\end{aligned}</script><p>其中$\mathbf w_c$是第 $c$ 类的权重向量，$b_c$是第 $c$ 类的偏置。</p>
<p>Softmax回归模型其实就是线性函数与Softmax函数的组合。</p>
<p>将$N$个样本归为一组进行成批地预测。</p>
<script type="math/tex; mode=display">
\hat{\mathbf Y} = \mathrm{softmax}(\boldsymbol{X} \boldsymbol{W} + \mathbf b), (3.13)</script><p>其中$\boldsymbol{X}\in \mathbb{R}^{N\times D}$为$N$个样本的特征矩阵，$\boldsymbol{W}=[\mathbf w_1,……,\mathbf w_C]$为$C$个类的权重向量组成的矩阵，$\hat{\mathbf Y}\in \mathbb{R}^{C}$为所有类别的预测条件概率组成的矩阵。</p>
<p>我们根据公式（3.13）实现Softmax回归算子，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">model_SR</span>(op.Op):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(model_SR, self).__init__()</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        <span class="comment"># 将线性层的权重参数全部初始化为0</span></span><br><span class="line">        self.params[<span class="string">&#x27;W&#x27;</span>] = paddle.zeros(shape=[input_dim, output_dim])</span><br><span class="line">        <span class="comment"># self.params[&#x27;W&#x27;] = paddle.normal(mean=0, std=0.01, shape=[input_dim, output_dim])</span></span><br><span class="line">        <span class="comment"># 将线性层的偏置参数初始化为0</span></span><br><span class="line">        self.params[<span class="string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[output_dim])</span><br><span class="line">        self.outputs = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - inputs: shape=[N,D], N是样本数量，D是特征维度</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - outputs：预测值，shape=[N,C]，C是类别数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 线性计算</span></span><br><span class="line">        score = paddle.matmul(inputs, self.params[<span class="string">&#x27;W&#x27;</span>]) + self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        <span class="comment"># Softmax 函数</span></span><br><span class="line">        self.outputs = softmax(score)</span><br><span class="line">        <span class="keyword">return</span> self.outputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成1条长度为4的数据</span></span><br><span class="line">inputs = paddle.randn(shape=[<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Input is:&#x27;</span>, inputs)</span><br><span class="line"><span class="comment"># 实例化模型，这里令输入长度为4，输出类别数为3</span></span><br><span class="line">model = model_SR(input_dim=<span class="number">4</span>, output_dim=<span class="number">3</span>)</span><br><span class="line">outputs = model(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Output is:&#x27;</span>, outputs)</span><br></pre></td></tr></table></figure>
<pre><code>Input is: Tensor(shape=[1, 4], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[-0.06042910,  0.97415614,  0.28900006,  0.37233669]])
Output is: Tensor(shape=[1, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[0.33333334, 0.33333334, 0.33333334]])
</code></pre><p>从输出结果可以看出，采用全0初始化后，属于每个类别的条件概率均为$\frac{1}{C}$。这是因为，不论输入值的大小为多少，线性函数$f(\mathbf x;\mathbf W,\mathbf b)$的输出值恒为0。此时，再经过Softmax函数的处理，每个类别的条件概率恒等。</p>
<h3 id="3-2-3-损失函数"><a href="#3-2-3-损失函数" class="headerlink" title="3.2.3 损失函数"></a>3.2.3 损失函数</h3><p>Softmax回归同样使用交叉熵损失作为损失函数，并使用梯度下降法对参数进行优化。通常使用$C$维的one-hot类型向量$\mathbf y \in \{0,1\}^C$来表示多分类任务中的类别标签。对于类别$c$，其向量表示为：</p>
<script type="math/tex; mode=display">
\mathbf y = [I(1=c),I(2=c),…,I(C=c)]^T，（3.14）</script><p>其中$I(·)$是指示函数，即括号内的输入为“真”，$I(·)=1$；否则，$I(·)=0$。</p>
<p>给定有$N$个训练样本的训练集$\{(\mathbf x^{(n)},y^{(n)})\} ^N_{n=1}$，令$\hat{\mathbf y}^{(n)}=\mathrm{softmax}(\mathbf W^ \mathrm{ T } \mathbf x^{(n)}+\mathbf b)$为样本$\mathbf x^{(n)}$在每个类别的后验概率。多分类问题的交叉熵损失函数定义为：</p>
<script type="math/tex; mode=display">
\cal R(\mathbf W,\mathbf b) = -\frac{1}{N}\sum_{n=1}^N (\mathbf y^{(n)})^ \mathrm{ T } \log\hat{\mathbf y}^{(n)} = -\frac{1}{N}\sum_{n=1}^N \sum_{c=1}^C \mathbf y_c^{(n)} \log\hat{\mathbf y}_c^{(n)}.（3.15）</script><p>观察上式，$\mathbf y_c^{(n)}$在$c$为真实类别时为1，其余都为0。也就是说，交叉熵损失只关心正确类别的预测概率，因此，上式又可以优化为：</p>
<script type="math/tex; mode=display">
\cal R(\mathbf W,\mathbf b) = -\frac{1}{N}\sum_{n=1}^N \log [\hat{\mathbf y}^{(n)}]_{y^{(n)}},（3.16）</script><p>其中$y^{(n)}$是第$n$个样本的标签。</p>
<p>因此，多类交叉熵损失函数的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiCrossEntropyLoss</span>(op.Op):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.predicts = <span class="literal">None</span></span><br><span class="line">        self.labels = <span class="literal">None</span></span><br><span class="line">        self.num = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, predicts, labels</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(predicts, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, predicts, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - predicts：预测值，shape=[N, 1]，N为样本数量</span></span><br><span class="line"><span class="string">            - labels：真实标签，shape=[N, 1]</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - 损失值：shape=[1]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.predicts = predicts</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.num = self.predicts.shape[<span class="number">0</span>]</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.num):</span><br><span class="line">            index = self.labels[i]</span><br><span class="line">            loss -= paddle.log(self.predicts[i][index])</span><br><span class="line">        <span class="keyword">return</span> loss / self.num</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试一下</span></span><br><span class="line"><span class="comment"># 假设真实标签为第1类</span></span><br><span class="line">labels = paddle.to_tensor([<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 计算风险函数</span></span><br><span class="line">mce_loss = MultiCrossEntropyLoss()</span><br><span class="line"><span class="built_in">print</span>(mce_loss(outputs, labels))</span><br></pre></td></tr></table></figure>
<pre><code>Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [1.09861231])
</code></pre><h3 id="3-2-4-模型优化"><a href="#3-2-4-模型优化" class="headerlink" title="3.2.4 模型优化"></a>3.2.4 模型优化</h3><p>使用梯度下降法进行参数学习。</p>
<h4 id="3-2-4-1-梯度计算"><a href="#3-2-4-1-梯度计算" class="headerlink" title="3.2.4.1 梯度计算"></a>3.2.4.1 梯度计算</h4><p>计算风险函数$\cal R(\mathbf W,\mathbf b)$关于参数$\mathbf W$和$\mathbf b$的偏导数。在Softmax回归中，计算方法为：</p>
<script type="math/tex; mode=display">
\frac{\partial \cal R(\mathbf W,\mathbf b)}{\partial \mathbf W} = -\frac{1}{N}\sum_{n=1}^N \mathbf x^{(n)}(y^{(n)}- \hat{ y}^{(n)})^T = -\frac{1}{N} \mathbf X^ \mathrm{ T } (\mathbf y- \hat{\mathbf y}),（3.17）</script><script type="math/tex; mode=display">
\frac{\partial \cal R(\mathbf W,\mathbf b)}{\partial \mathbf b} = -\frac{1}{N}\sum_{n=1}^N (y^{(n)}- \hat{y}^{(n)})^T = -\frac{1}{N} \mathbf 1 (\mathbf y- \hat{\mathbf y}).（3.18）</script><p>其中$\mathbf X\in \mathbb{R}^{N\times D}$为$N$个样本组成的矩阵，$\mathbf y\in \mathbb{R}^{N}$为$N$个样本标签组成的向量，$\hat{\mathbf y}\in \mathbb{R}^{N}$为$N$个样本的预测标签组成的向量，$\mathbf{1}$为$N$维的全1向量。</p>
<p>将上述计算方法定义在模型的<code>backward</code>函数中，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">model_SR</span>(op.Op):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(model_SR, self).__init__()</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        <span class="comment"># 将线性层的权重参数全部初始化为0</span></span><br><span class="line">        self.params[<span class="string">&#x27;W&#x27;</span>] = paddle.zeros(shape=[input_dim, output_dim])</span><br><span class="line">        <span class="comment"># self.params[&#x27;W&#x27;] = paddle.normal(mean=0, std=0.01, shape=[input_dim, output_dim])</span></span><br><span class="line">        <span class="comment"># 将线性层的偏置参数初始化为0</span></span><br><span class="line">        self.params[<span class="string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[output_dim])</span><br><span class="line">        <span class="comment"># 存放参数的梯度</span></span><br><span class="line">        self.grads = &#123;&#125;</span><br><span class="line">        self.X = <span class="literal">None</span></span><br><span class="line">        self.outputs = <span class="literal">None</span></span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        self.X = inputs</span><br><span class="line">        <span class="comment"># 线性计算</span></span><br><span class="line">        score = paddle.matmul(self.X, self.params[<span class="string">&#x27;W&#x27;</span>]) + self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        <span class="comment"># Softmax 函数</span></span><br><span class="line">        self.outputs = softmax(score)</span><br><span class="line">        <span class="keyword">return</span> self.outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - labels：真实标签，shape=[N, 1]，其中N为样本数量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 计算偏导数</span></span><br><span class="line">        N =labels.shape[<span class="number">0</span>]</span><br><span class="line">        labels = paddle.nn.functional.one_hot(labels, self.output_dim)</span><br><span class="line">        self.grads[<span class="string">&#x27;W&#x27;</span>] = -<span class="number">1</span> / N * paddle.matmul(self.X.t(), (labels-self.outputs))</span><br><span class="line">        self.grads[<span class="string">&#x27;b&#x27;</span>] = -<span class="number">1</span> / N * paddle.matmul(paddle.ones(shape=[N]), (labels-self.outputs))</span><br></pre></td></tr></table></figure>
<h4 id="3-2-4-2-参数更新"><a href="#3-2-4-2-参数更新" class="headerlink" title="3.2.4.2 参数更新"></a>3.2.4.2 参数更新</h4><p>在计算参数的梯度之后，我们使用3.1.4.2中实现的梯度下降法进行参数更新。</p>
<h3 id="3-2-5-模型训练"><a href="#3-2-5-模型训练" class="headerlink" title="3.2.5 模型训练"></a>3.2.5 模型训练</h3><p>实例化RunnerV2类，并传入训练配置。使用训练集和验证集进行模型训练，共训练500个epoch。每隔50个epoch打印训练集上的指标。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子，保持每次运行结果一致</span></span><br><span class="line">paddle.seed(<span class="number">102</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征维度</span></span><br><span class="line">input_dim = <span class="number">2</span></span><br><span class="line"><span class="comment"># 类别数</span></span><br><span class="line">output_dim = <span class="number">3</span></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = model_SR(input_dim=input_dim, output_dim=output_dim)</span><br><span class="line"><span class="comment"># 指定优化器</span></span><br><span class="line">optimizer = SimpleBatchGD(init_lr=lr, model=model)</span><br><span class="line"><span class="comment"># 指定损失函数</span></span><br><span class="line">loss_fn = MultiCrossEntropyLoss()</span><br><span class="line"><span class="comment"># 指定评价方式</span></span><br><span class="line">metric = accuracy</span><br><span class="line"><span class="comment"># 实例化RunnerV2类</span></span><br><span class="line">runner = RunnerV2(model, optimizer, metric, loss_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="number">500</span>, log_eopchs=<span class="number">50</span>, eval_epochs=<span class="number">1</span>, save_path=<span class="string">&quot;best_model.pdparams&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化观察训练集与验证集的准确率变化情况</span></span><br><span class="line">plot(runner,fig_name=<span class="string">&#x27;linear-acc2.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>best accuracy performence has been updated: 0.00000 --&gt; 0.71875
[Train] epoch: 0, loss: 1.098615050315857, score: 0.34375
[Dev] epoch: 0, loss: 1.0777472257614136, score: 0.71875
best accuracy performence has been updated: 0.71875 --&gt; 0.73750
best accuracy performence has been updated: 0.73750 --&gt; 0.74375
best accuracy performence has been updated: 0.74375 --&gt; 0.75000
best accuracy performence has been updated: 0.75000 --&gt; 0.75625
best accuracy performence has been updated: 0.75625 --&gt; 0.76250
best accuracy performence has been updated: 0.76250 --&gt; 0.76875
best accuracy performence has been updated: 0.76875 --&gt; 0.77500
[Train] epoch: 100, loss: 0.7547885775566101, score: 0.7515624761581421
[Dev] epoch: 100, loss: 0.74772709608078, score: 0.7749999761581421
[Train] epoch: 200, loss: 0.7442280054092407, score: 0.745312511920929
[Dev] epoch: 200, loss: 0.7368013262748718, score: 0.768750011920929
[Train] epoch: 300, loss: 0.7427152395248413, score: 0.7484375238418579
[Dev] epoch: 300, loss: 0.7358912825584412, score: 0.768750011920929
[Train] epoch: 400, loss: 0.7423940300941467, score: 0.7484375238418579
[Dev] epoch: 400, loss: 0.7359185218811035, score: 0.768750011920929
</code></pre><p><img src="/img-nndl/output_65_1.png" alt="png"></p>
<h3 id="3-2-6-模型评价"><a href="#3-2-6-模型评价" class="headerlink" title="3.2.6 模型评价"></a>3.2.6 模型评价</h3><p>使用测试集对训练完成后的最终模型进行评价，观察模型在测试集上的准确率。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score, loss = runner.evaluate([X_test, y_test])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(score, loss))</span><br></pre></td></tr></table></figure>
<pre><code>[Test] score/loss: 0.7400/0.7366
</code></pre><p>可视化观察类别划分结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均匀生成40000个数据点</span></span><br><span class="line">x1, x2 = paddle.meshgrid(paddle.linspace(-<span class="number">3.5</span>, <span class="number">2</span>, <span class="number">200</span>), paddle.linspace(-<span class="number">4.5</span>, <span class="number">3.5</span>, <span class="number">200</span>))</span><br><span class="line">x = paddle.stack([paddle.flatten(x1), paddle.flatten(x2)], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 预测对应类别</span></span><br><span class="line">y = runner.predict(x)</span><br><span class="line">y = paddle.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 绘制类别区域</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>].tolist(), x[:,<span class="number">1</span>].tolist(), c=y.tolist(), cmap=plt.cm.Spectral)</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">102</span>)</span><br><span class="line">n_samples = <span class="number">1000</span></span><br><span class="line">X, y = make_multiclass_classification(n_samples=n_samples, n_features=<span class="number">2</span>, n_classes=<span class="number">3</span>, noise=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>].tolist(), X[:, <span class="number">1</span>].tolist(), marker=<span class="string">&#x27;*&#x27;</span>, c=y.tolist())</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x7f16c841f110&gt;
</code></pre><p>​<br><img src="/img-nndl/output_69_1.png" alt="png"><br>​    </p>
<hr>
<p><strong>拓展</strong></p>
<p>提前停止是在使用梯度下降法进行模型优化时常用的正则化方法。对于某些拟合能力非常强的机器学习算法，当训练轮数较多时，容易发生过拟合现象，即在训练集上错误率很低，但是在未知数据（或测试集）上错误率很高。为了解决这一问题，通常会在模型优化时，使用验证集上的错误代替期望错误。当验证集上的错误率不在下降时，就停止迭代。</p>
<p>在3.4.3节的实验中，模型训练过程中会按照提前停止的思想保存验证集上的最优模型。</p>
<hr>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/08/12/nndl/chapter3B/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          第3章（下）：基于Softmax回归完成鸢尾花分类任务
        
      </div>
    </a>
  
  
    <a href="/2022/08/12/nndl/chapter2B/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">第2章（下）：基于线性回归完成波士顿房价预测任务</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






    <section id="comments" style="margin: 60px;">
      <div id="gitalk-container"></div>
      <script>
      var gitalk = new Gitalk({
          clientID: 'ecf230c60af3ed1123fa',
          clientSecret: '1e1daf2e6dd957a3f7d6b22b524387d14aafdb13',
          repo: 'hfut-zyw.github.io',
          owner: 'hfut-zyw',
          admin: ['hfut-zyw'],
          id: md5(location.pathname),      // Ensure uniqueness and length less than 50
          distractionFreeMode: false  // Facebook-like distraction free mode
        })
      gitalk.render('gitalk-container')
      </script>
      
    </section>

</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        <a href="https://space.bilibili.com/782159" target="_blank">访问我的哔哩哔哩</a> 
      </div>
        <div class="footer-right">
        &copy; 2022 LabmemNo.001
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: ,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":85,"height":170},"mobile":{"show":false},"log":false});</script></body>
</html>