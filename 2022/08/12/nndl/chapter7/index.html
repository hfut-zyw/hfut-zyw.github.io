<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第7章：网络优化与正则化 | Homepage | 张有为</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="第7章 网络优化与正则化神经网络具有非常强的表达能力，但当应用神经网络模型到机器学习时依然存在一些难点问题。首先，神经网络的损失函数是一个非凸函数，找到全局最优解通常比较困难。">
<meta property="og:type" content="article">
<meta property="og:title" content="第7章：网络优化与正则化">
<meta property="og:url" content="http://example.com/2022/08/12/nndl/chapter7/index.html">
<meta property="og:site_name" content="Homepage | 张有为">
<meta property="og:description" content="第7章 网络优化与正则化神经网络具有非常强的表达能力，但当应用神经网络模型到机器学习时依然存在一些难点问题。首先，神经网络的损失函数是一个非凸函数，找到全局最优解通常比较困难。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img-nndl/output_6_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_18_2.png">
<meta property="og:image" content="http://example.com/img-nndl/output_32_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_40_1.png">
<meta property="og:image" content="http://example.com/img-nndl/output_42_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_46_1.png">
<meta property="og:image" content="http://example.com/img-nndl/output_48_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_53_1.png">
<meta property="og:image" content="http://example.com/img-nndl/output_55_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_59_1.png">
<meta property="og:image" content="http://example.com/img-nndl/output_61_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_70_1.png">
<meta property="article:published_time" content="2022-08-12T01:27:48.000Z">
<meta property="article:modified_time" content="2022-09-22T13:04:06.224Z">
<meta property="article:author" content="LabmemNo.001">
<meta property="article:tag" content="nndl案例与实践">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img-nndl/output_6_0.png">
  
    <link rel="alternative" href="/atom.xml" title="Homepage | 张有为" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/icon.gif">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://priesttomb.github.io/js/md5.min.js"></script>

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
<meta name="generator" content="Hexo 6.2.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/face.png" class="js-avatar show" style="width: 100%;height: 100%;opacity: 1;">
		</a>

		<hgroup>
			<h1 class="header-author"><a href="/">LabmemNo.001</a></h1>
		</hgroup>

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">

					<nav class="header-menu">
						<ul>
						
							<li><a href="/2022/08/09/me/aboutme">个人简介</a>
							</li>
				        
							<li><a href="/categories/notes">一些随写</a>
							</li>
				        
							<li><a href="/categories/analysis">抽象分析基础</a>
							</li>
				        
							<li><a href="/categories/probability">高等概率统计</a>
							</li>
				        
							<li><a href="/categories/opt">矩阵论与最优化</a>
							</li>
				        
						</ul>
					</nav>

					<nav class="header-menu">
						<ul>
						<li><a style="color:rgb(228, 117, 238)">Deep Learning </a>
							<nav class="header-submenu">
								<ul>
									
										<li><a href="/categories/pytorch"><div>Pytorch</div></a></li>  
									
										<li><a href="/categories/pandas"><div>Pandas</div></a></li>  
									
										<li><a href="/categories/matplotlib"><div>Matplotlib</div></a></li>  
									
										<li><a href="/categories/nndl"><div>nndl案例与实践</div></a></li>  
									
								</ul>
							</nav>	
						</li>     
						</ul>
					</nav>
					
								
					


					<nav class="half-header-menu">
						<a class="hide">Home</a>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
					        
								<a class="mail" target="_blank" href="mailto:lucario@qq.com" title="mail">mail</a>
					        
						</div>
						<!-- music -->
						
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Matplotlib%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">Matplotlib笔记</a> <a href="/tags/Pandas%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">Pandas笔记</a> <a href="/tags/Pytorch%E7%AC%94%E8%AE%B0/" style="font-size: 12.5px;">Pytorch笔记</a> <a href="/tags/nndl%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/" style="font-size: 20px;">nndl案例与实践</a> <a href="/tags/%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/" style="font-size: 15px;">优化笔记</a> <a href="/tags/%E5%85%B3%E4%BA%8E%E6%88%91/" style="font-size: 10px;">关于我</a> <a href="/tags/%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0/" style="font-size: 17.5px;">分析笔记</a> <a href="/tags/%E9%9A%8F%E5%86%99/" style="font-size: 15px;">随写</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/face.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/2022/08/09/me/aboutme">个人简介</a></li>
		        
					<li><a href="/categories/notes">一些随写</a></li>
		        
					<li><a href="/categories/analysis">抽象分析基础</a></li>
		        
					<li><a href="/categories/probability">高等概率统计</a></li>
		        
					<li><a href="/categories/opt">矩阵论与最优化</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
			        
						<a class="mail" target="_blank" href="mailto:lucario@qq.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">

<article id="post-nndl/chapter7" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/08/12/nndl/chapter7/" class="article-date">
  	<time datetime="2022-08-12T01:27:48.000Z" itemprop="datePublished">2022-08-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第7章：网络优化与正则化
      
          <span class="title-pop-out"></a>
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nndl%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/" rel="tag">nndl案例与实践</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/nndl/">nndl</a>
	</div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第7章-网络优化与正则化"><a href="#第7章-网络优化与正则化" class="headerlink" title="第7章 网络优化与正则化"></a>第7章 网络优化与正则化</h1><p>神经网络具有非常强的表达能力，但当应用神经网络模型到机器学习时依然存在一些难点问题。首先，神经网络的损失函数是一个非凸函数，找到全局最优解通常比较困难。<span id="more"></span>其次，深度神经网络的参数非常多，训练数据也比较大，因此也无法使用计算代价很高的二阶优化方法，而一阶优化方法的训练效率通常比较低。此外，深度神经网络存在梯度消失或爆炸问题，导致基于梯度的优化方法经常失效。</p>
<p>目前，神经网络变得流行除了本身模型能力强之外，还有一个重要的原因是研究者从大量的实践中总结了一些经验方法，在神经网络的表示能力、复杂度、学习效率和泛化能力之间找到了比较好的平衡。本章主要介绍神经网络的参数学习中常用的优化和正则化方法。</p>
<p>本章内容基于神经网络与深度学习》第7章：网络优化与正则化相关内容进行设计。在阅读本章之前，建议先了解如图7.1所示的关键知识点，以便更好地理解和掌握相应的理论和实践知识。</p>
<p><center><image src="https://ai-studio-static-online.cdn.bcebos.com/95762b1e60984b56ac0d7ca32c1bff40ca0edf2de62a4cb598751257fe89e72f" width=500/></center><br></br></p>
<p><center>图7.1 网络优化和正则化关键知识点回顾</center><br></br></p>
<p>本章内容主要包含两部分：</p>
<ul>
<li>网络优化：通过案例和可视化对优化算法、参数初始化、逐层规范化等网络优化算法进行分析和对比，展示它们的效果，通过代码详细展示这些算法的实现过程。</li>
<li>网络正则化：通过案例和可视化对$\ell_{1}$和$\ell_{2}$正则化、权重衰减、暂退法等网络正则化方法进行分析和对比，展示它们的效果。</li>
</ul>
<p><strong>提醒</strong></p>
<p>在本书中，对《神经网络与深度学习》中一些术语的翻译进行修正。Normalization翻译为规范化、Dropout翻译为暂退法。</p>
<h2 id="7-1-小批量梯度下降法"><a href="#7-1-小批量梯度下降法" class="headerlink" title="7.1 小批量梯度下降法"></a>7.1 小批量梯度下降法</h2><p>目前，深度神经网络的优化方法主要是通过梯度下降法来寻找一组可以最小化结构风险的参数。在具体实现中，梯度下降法可以分为批量梯度下降、随机梯度下降和小批量梯度下降(Mini-Batch Gradient Descent)三种方式。它们的区别在于批大小（Batch Size）不同，这三种梯度下降法分别针对全部样本、单个随机样本和小批量随机样本进行梯度计算。根据不同的数据量和参数量，可以选择不同的实现形式。<br>下面我们以小批量梯度下降法为主进行介绍。</p>
<p>令$f(\bm x; \theta)$表示一个神经网络模型，$\theta$为模型参数，$\mathcal{L}(\cdot)$为可微分的损失函数，$\nabla_\theta \mathcal{L}(\bm y, f(\bm x; \theta))=\frac{\partial \mathcal{L}(\bm y, f(\bm x; \theta))}{\partial \theta}$为损失函数关于参数$\theta$的偏导数。在使用小批量梯度下降法进行优化时，每次选取$K$个训练样本$\mathcal{S}_t = {(\bm x^{(k)}, \bm y^{(k)})}^K_{k=1}$。第$t$次迭代时参数$\theta$的梯度为</p>
<script type="math/tex; mode=display">
\mathbf g_t = \frac{1}{K}\sum_{(\bm x, \bm y) \in \mathcal{S}_t} \nabla_{\theta} \mathcal{L}(\bm y, f(\bm x; \theta_{t-1})),</script><p>其中$\mathcal{L}(\cdot)$为可微分的损失函数，$K$为批大小。</p>
<p>使用梯度下降来更新参数，</p>
<script type="math/tex; mode=display">
\theta_t \leftarrow \theta_{t-1} - \alpha \mathbf g_t,</script><p>其中$\alpha &gt; 0$为学习率。</p>
<p>从上面公式可以看出，影响神经网络优化的主要超参有三个：</p>
<ol>
<li>批大小$K$</li>
<li>学习率$\alpha$</li>
<li>梯度计算$\mathbf g_t$</li>
</ol>
<p>不同优化算法主要从这三个方面进行改进。下面我们通过动手实践来更好地理解不同的网络优化方法。</p>
<h2 id="7-2-批大小的调整实验"><a href="#7-2-批大小的调整实验" class="headerlink" title="7.2 批大小的调整实验"></a>7.2 批大小的调整实验</h2><p>在训练深度神经网络时，训练数据的规模通常都比较大。如果在梯度下降时每次迭代都要计算整个训练数据上的梯度，这就需要比较多的计算资源。另外，大规模训练集中的数据通常会非常冗余，也没有必要在整个训练集上计算梯度。因此，在训练深度神经网络时，经常使用小批量梯度下降法。</p>
<p>为了观察不同批大小对模型收敛速度的影响，我们使用经典的LeNet网络进行图像分类，调用paddle.vision.datasets.MNIST函数读取MNIST数据集，并将数据进行规范化预处理。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图像值规范化到0~1之间</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transform</span>(<span class="params">image</span>):</span><br><span class="line">    image = paddle.to_tensor(image / <span class="number">255</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    image = paddle.unsqueeze(image, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure>
<p>方便起见，本节使用第4.5.4节构建的RunnerV3类进行模型训练，并使用paddle.vision.models.LeNet快速构建LeNet网络，使用paddle.io.DataLoader根据批大小对数据进行划分，使用交叉熵损失函数及标准的随机梯度下降优化器paddle.optimizer.SGD。RunnerV3类会保存每轮迭代和每个回合的损失值，可以方便地观察批大小对模型收敛速度的影响。</p>
<p>通常情况下，批大小与学习率大小成正比。选择批大小为16、32、64、128、256的情况进行训练。相应地，学习率大小被设置为0.01、0.02、0.04、0.08、0.16。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.io <span class="keyword">as</span> io</span><br><span class="line"><span class="keyword">import</span> paddle.optimizer <span class="keyword">as</span> optimizer</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> RunnerV3</span><br><span class="line"><span class="keyword">from</span> paddle.vision.models <span class="keyword">import</span> LeNet</span><br><span class="line"><span class="keyword">from</span> paddle.vision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line"><span class="comment"># 确保从paddle.vision.datasets.MNIST中加载的图像数据是np.ndarray类型</span></span><br><span class="line">paddle.vision.image.set_image_backend(<span class="string">&#x27;cv2&#x27;</span>)</span><br><span class="line">train_dataset = MNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=transform)</span><br><span class="line"><span class="comment"># 迭代器加载数据集</span></span><br><span class="line"><span class="comment"># 为保证每次输出结果相同，没有设置shuffle=True，真实模型训练场景需要开启</span></span><br><span class="line">train_loader1 = io.DataLoader(train_dataset, batch_size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络</span></span><br><span class="line">model1 = LeNet()</span><br><span class="line"><span class="comment"># 定义优化器，使用随机梯度下降（SGD）优化器</span></span><br><span class="line">opt1 = optimizer.SGD(learning_rate=<span class="number">0.01</span>, parameters=model1.parameters())</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss_fn = F.cross_entropy</span><br><span class="line"><span class="comment"># 定义runner类</span></span><br><span class="line">runner1 = RunnerV3(model1, opt1, loss_fn, <span class="literal">None</span>)</span><br><span class="line">runner1.train(train_loader1, num_epochs=<span class="number">30</span>, log_steps=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">model2 = LeNet()</span><br><span class="line">train_loader2 = io.DataLoader(train_dataset, batch_size=<span class="number">32</span>)</span><br><span class="line">opt2 = optimizer.SGD(learning_rate=<span class="number">0.02</span>, parameters=model2.parameters())</span><br><span class="line">runner2 = RunnerV3(model2, opt2, loss_fn, <span class="literal">None</span>)</span><br><span class="line">runner2.train(train_loader2, num_epochs=<span class="number">30</span>, log_steps=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">model3 = LeNet()</span><br><span class="line">train_loader3 = io.DataLoader(train_dataset, batch_size=<span class="number">64</span>)</span><br><span class="line">opt3 = optimizer.SGD(learning_rate=<span class="number">0.04</span>, parameters=model3.parameters())</span><br><span class="line">runner3 = RunnerV3(model3, opt3, loss_fn, <span class="literal">None</span>)</span><br><span class="line">runner3.train(train_loader3, num_epochs=<span class="number">30</span>, log_steps=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">model4 = LeNet()</span><br><span class="line">train_loader4 = io.DataLoader(train_dataset, batch_size=<span class="number">128</span>)</span><br><span class="line">opt4 = optimizer.SGD(learning_rate=<span class="number">0.08</span>, parameters=model4.parameters())</span><br><span class="line">runner4 = RunnerV3(model4, opt4, loss_fn, <span class="literal">None</span>)</span><br><span class="line">runner4.train(train_loader4, num_epochs=<span class="number">30</span>, log_steps=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">model5 = LeNet()</span><br><span class="line">train_loader5 = io.DataLoader(train_dataset, batch_size=<span class="number">256</span>)</span><br><span class="line">opt5 = optimizer.SGD(learning_rate=<span class="number">0.16</span>, parameters=model5.parameters())</span><br><span class="line">runner5 = RunnerV3(model5, opt5, loss_fn, <span class="literal">None</span>)</span><br><span class="line">runner5.train(train_loader5, num_epochs=<span class="number">30</span>, log_steps=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Cache file /home/aistudio/.cache/paddle/dataset/mnist/train-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-images-idx3-ubyte.gz 
Begin to download


item  384/2421 [===&gt;..........................] - ETA: 17s - 8ms/ite
item 2421/2421 [============================&gt;.] - ETA: 0s - 5ms/item
</code></pre><p>​    </p>
<pre><code>Download finished
Cache file /home/aistudio/.cache/paddle/dataset/mnist/train-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-labels-idx1-ubyte.gz 
Begin to download


item 8/8 [============================&gt;.] - ETA: 0s - 12ms/item
</code></pre><p>​    </p>
<pre><code>Download finished
W0716 15:38:02.097203   150 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1
W0716 15:38:02.100914   150 device_context.cc:465] device: 0, cuDNN Version: 7.6.


[Train] Training done!
[Train] Training done!
[Train] Training done!
[Train] Training done!
[Train] Training done!
</code></pre><p>可视化损失函数的变化趋势。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制每个回合的损失</span></span><br><span class="line">plt.plot(runner1.train_epoch_losses, label=<span class="string">&#x27;batch size: 16, lr: 0.01&#x27;</span>, c=<span class="string">&#x27;#9c9d9f&#x27;</span>)</span><br><span class="line">plt.plot(runner2.train_epoch_losses, label=<span class="string">&#x27;batch size: 32, lr: 0.02&#x27;</span>, c=<span class="string">&#x27;#f7d2e2&#x27;</span>)</span><br><span class="line">plt.plot(runner3.train_epoch_losses, label=<span class="string">&#x27;batch size: 64, lr: 0.04&#x27;</span>, c=<span class="string">&#x27;#f19ec2&#x27;</span>)</span><br><span class="line">plt.plot(runner4.train_epoch_losses, label=<span class="string">&#x27;batch size: 128, lr: 0.08&#x27;</span>, c=<span class="string">&#x27;#e86096&#x27;</span>, linestyle=<span class="string">&#x27;-.&#x27;</span>)</span><br><span class="line">plt.plot(runner5.train_epoch_losses, label=<span class="string">&#x27;batch size: 256, lr: 0.16&#x27;</span>, c=<span class="string">&#x27;#000000&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.legend(fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;epoch loss with different bs and lr&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;opt-mnist-loss.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_6_0.png" alt="png"></p>
<p>从输出结果看，如果按每个回合的损失来看，每批次样本数越小，下降效果越明显。适当小的批大小可以导致更快的收敛。</p>
<p><strong>动手练习7.1</strong></p>
<ul>
<li>尝试画出按迭代的损失变化图，观察不同的批大小在两种损失图中的下降效果是否相同。</li>
</ul>
<p><strong>动手练习7.2</strong></p>
<ul>
<li>对比下面两种实验设置，分析和比较它们的区别。1. 批大小增大一倍，学习率不变。2. 批大小不变，学习率减少一半。</li>
</ul>
<h2 id="7-3-不同优化算法的比较分析"><a href="#7-3-不同优化算法的比较分析" class="headerlink" title="7.3 不同优化算法的比较分析"></a>7.3 不同优化算法的比较分析</h2><p>除了批大小对模型收敛速度的影响外，学习率和梯度估计也是影响神经网络优化的重要因素。神经网络优化中常用的优化方法也主要是如下两方面的改进，包括：</p>
<ul>
<li>学习率调整：主要通过自适应地调整学习率使得优化更稳定。这类算法主要有AdaGrad、RMSprop、AdaDelta算法等。</li>
<li>梯度估计修正：主要通过修正每次迭代时估计的梯度方向来加快收敛速度。这类算法主要有动量法、Nesterov加速梯度方法等。</li>
</ul>
<p>除上述方法外，本节还会介绍综合学习率调整和梯度估计修正的优化算法，如Adam算法。</p>
<h3 id="7-3-1-优化算法的实验设定"><a href="#7-3-1-优化算法的实验设定" class="headerlink" title="7.3.1 优化算法的实验设定"></a>7.3.1 优化算法的实验设定</h3><p>为了更好地对比不同的优化算法，我们准备两个实验：第一个是2D可视化实验。第二个是简单拟合实验。</p>
<p>首先介绍下这两个实验的任务设定。</p>
<h4 id="7-3-1-1-2D可视化实验"><a href="#7-3-1-1-2D可视化实验" class="headerlink" title="7.3.1.1 2D可视化实验"></a>7.3.1.1 2D可视化实验</h4><p>为了更好地展示不同优化算法的能力对比，我们选择一个二维空间中的凸函数，然后用不同的优化算法来寻找最优解，并可视化梯度下降过程的轨迹。</p>
<p><strong>被优化函数</strong> 选择Sphere函数作为被优化函数，并对比它们的优化效果。Sphere函数的定义为</p>
<script type="math/tex; mode=display">
\mathrm{sphere}(\bm x) = \sum_{d=1}^{D} x_d^2 = \bm x^2,</script><p>其中$\bm x \in \mathbb{R}^D$，$\bm x^2$表示逐元素平方。Sphere函数有全局的最优点$\bm x^*=0$。</p>
<p>这里为了展示方便，我们使用二维的输入并略微修改Sphere函数，定义$\mathrm{sphere}(\bm x) = \bm w^\top \bm x^2$，并根据梯度下降公式计算对$\bm x$的偏导</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{sphere}(\bm x)}{\partial \bm x} = 2 \bm w \odot \bm x,</script><p>其中$\odot$表示逐元素积。</p>
<p>将被优化函数实现为OptimizedFunction算子，其forward方法是Sphere函数的前向计算，backward方法则计算被优化函数对$\bm x$的偏导。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl.op <span class="keyword">import</span> Op</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OptimizedFunction</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, w</span>):</span><br><span class="line">        <span class="built_in">super</span>(OptimizedFunction, self).__init__()</span><br><span class="line">        self.w = w</span><br><span class="line">        self.params = &#123;<span class="string">&#x27;x&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">        self.grads = &#123;<span class="string">&#x27;x&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        self.params[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line">        <span class="keyword">return</span> paddle.matmul(self.w.T, paddle.square(self.params[<span class="string">&#x27;x&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self</span>):</span><br><span class="line">        self.grads[<span class="string">&#x27;x&#x27;</span>] = <span class="number">2</span> * paddle.multiply(self.w.T, self.params[<span class="string">&#x27;x&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><strong>小批量梯度下降优化器</strong> 复用3.1.4.3节定义的梯度下降优化器SimpleBatchGD。按照梯度下降的梯度更新公式$\theta_t \leftarrow \theta_{t-1} - \alpha \mathbf g_t$进行梯度更新。</p>
<p><strong>训练函数</strong>  定义一个简易的训练函数，记录梯度下降过程中每轮的参数$\bm x$和损失。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_f</span>(<span class="params">model, optimizer, x_init, epoch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练函数</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - model：被优化函数</span></span><br><span class="line"><span class="string">        - optimizer：优化器</span></span><br><span class="line"><span class="string">        - x_init：x初始值</span></span><br><span class="line"><span class="string">        - epoch：训练回合数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = x_init</span><br><span class="line">    all_x = []</span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        all_x.append(x.numpy())</span><br><span class="line">        loss = model(x)</span><br><span class="line">        losses.append(loss)</span><br><span class="line">        model.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        x = model.params[<span class="string">&#x27;x&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> paddle.to_tensor(all_x), losses</span><br></pre></td></tr></table></figure>
<p><strong>可视化函数</strong> 定义一个Visualization类，用于绘制$\bm x$的更新轨迹。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Visualization</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化可视化类</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 只画出参数x1和x2在区间[-5, 5]的曲线部分</span></span><br><span class="line">        x1 = np.arange(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">        x2 = np.arange(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">        x1, x2 = np.meshgrid(x1, x2)</span><br><span class="line">        self.init_x = paddle.to_tensor([x1, x2])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">plot_2d</span>(<span class="params">self, model, x, fig_name</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        可视化参数更新轨迹</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        fig, ax = plt.subplots(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">        cp = ax.contourf(self.init_x[<span class="number">0</span>], self.init_x[<span class="number">1</span>], model(self.init_x.transpose([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])), colors=[<span class="string">&#x27;#e4007f&#x27;</span>, <span class="string">&#x27;#f19ec2&#x27;</span>, <span class="string">&#x27;#e86096&#x27;</span>, <span class="string">&#x27;#eb7aaa&#x27;</span>, <span class="string">&#x27;#f6c8dc&#x27;</span>, <span class="string">&#x27;#f5f5f5&#x27;</span>, <span class="string">&#x27;#000000&#x27;</span>])</span><br><span class="line">        c = ax.contour(self.init_x[<span class="number">0</span>], self.init_x[<span class="number">1</span>], model(self.init_x.transpose([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])), colors=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">        cbar = fig.colorbar(cp)</span><br><span class="line">        ax.plot(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], <span class="string">&#x27;-o&#x27;</span>, color=<span class="string">&#x27;#000000&#x27;</span>)</span><br><span class="line">        ax.plot(<span class="number">0</span>, <span class="string">&#x27;r*&#x27;</span>, markersize=<span class="number">18</span>, color=<span class="string">&#x27;#fefefe&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        ax.set_xlabel(<span class="string">&#x27;$x1$&#x27;</span>)</span><br><span class="line">        ax.set_ylabel(<span class="string">&#x27;$x2$&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        ax.set_xlim((-<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">        ax.set_ylim((-<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">        plt.savefig(fig_name)</span><br></pre></td></tr></table></figure>
<p>定义train_and_plot_f函数，调用train_f和Visualization，训练模型并可视化参数更新轨迹。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_plot_f</span>(<span class="params">model, optimizer, epoch, fig_name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练模型并可视化参数更新轨迹</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 设置x的初始值</span></span><br><span class="line">    x_init = paddle.to_tensor([<span class="number">3</span>, <span class="number">4</span>], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;x1 initiate: &#123;&#125;, x2 initiate: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(x_init[<span class="number">0</span>].numpy(), x_init[<span class="number">1</span>].numpy()))</span><br><span class="line">    x, losses = train_f(model, optimizer, x_init, epoch)</span><br><span class="line">    losses = np.array(losses)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 展示x1、x2的更新轨迹</span></span><br><span class="line">    vis = Visualization()</span><br><span class="line">    vis.plot_2d(model, x, fig_name)</span><br></pre></td></tr></table></figure>
<p><strong>模型训练与可视化</strong>  指定Sphere函数中$\bm w$的值，实例化被优化函数，通过小批量梯度下降法更新参数，并可视化$\bm x$的更新轨迹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl.op <span class="keyword">import</span> SimpleBatchGD</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">w = paddle.to_tensor([<span class="number">0.2</span>, <span class="number">2</span>])</span><br><span class="line">model = OptimizedFunction(w)</span><br><span class="line">opt = SimpleBatchGD(init_lr=<span class="number">0.2</span>, model=model)</span><br><span class="line">train_and_plot_f(model, opt, epoch=<span class="number">20</span>, fig_name=<span class="string">&#x27;opti-vis-para.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:


x1 initiate: [3.], x2 initiate: [4.]
</code></pre><p><img src="/img-nndl/output_18_2.png" alt="png"></p>
<p>输出图中不同颜色代表$f(x_1, x_2)$的值，具体数值可以参考图右侧的对应表，比如深粉色区域代表$f(x_1, x_2)$在0～8之间，不同颜色间黑色的曲线是等值线，代表落在该线上的点对应的$f(x_1, x_2)$的值都相同。</p>
<h4 id="7-3-1-2-简单拟合实验"><a href="#7-3-1-2-简单拟合实验" class="headerlink" title="7.3.1.2 简单拟合实验"></a>7.3.1.2 简单拟合实验</h4><p>除了2D可视化实验外，我们还设计一个简单的拟合任务，然后对比不同的优化算法。</p>
<p>这里我们随机生成一组数据作为数据样本，再构建一个简单的单层前馈神经网络，用于前向计算。</p>
<p><strong>数据集构建</strong>  通过paddle.randn随机生成一些训练数据$\bm X$，并根据一个预定义函数$y = 0.5\times x_{1}+ 0.8\times x_{2} + 0.01\times noise$ 计算得到$\bm y$，再将$\bm X$和$\bm y$拼接起来得到训练样本。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 随机生成shape为（1000，2）的训练数据</span></span><br><span class="line">X = paddle.randn([<span class="number">1000</span>, <span class="number">2</span>])</span><br><span class="line">w = paddle.to_tensor([<span class="number">0.5</span>, <span class="number">0.8</span>])</span><br><span class="line">w = paddle.unsqueeze(w, axis=<span class="number">1</span>)</span><br><span class="line">noise = <span class="number">0.01</span> * paddle.rand([<span class="number">1000</span>])</span><br><span class="line">noise = paddle.unsqueeze(noise, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算y</span></span><br><span class="line">y = paddle.matmul(X, w) + noise</span><br><span class="line"><span class="comment"># 打印X, y样本</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X[<span class="number">0</span>].numpy())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y: &#x27;</span>, y[<span class="number">0</span>].numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># X，y组成训练样本数据</span></span><br><span class="line">data = paddle.concat((X, y), axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input data shape: &#x27;</span>, data.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;data: &#x27;</span>, data[<span class="number">0</span>].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>X:  [-4.080414  -1.3719953]
y:  [-3.136211]
input data shape:  [1000, 3]
data:  [-4.080414  -1.3719953 -3.136211 ]
</code></pre><p><strong>模型构建</strong>  定义单层前馈神经网络，$\bm X \in \mathbb{R}^{N \times D}$为网络输入, $\bm w \in \mathbb{R}^{D}$是网络的权重矩阵，$\bm b \in \mathbb{R}$为偏置。</p>
<script type="math/tex; mode=display">
\bm y =\bm X \bm w + b \in \mathbb{R}^{K\times 1},</script><p>其中$K$代表一个批次中的样本数量，$D$为单层网络的输入特征维度。</p>
<p><strong>损失函数</strong>  使用均方误差作为训练时的损失函数，计算损失函数关于参数$\bm w$和$b$的偏导数。定义均方误差损失函数的计算方法为</p>
<script type="math/tex; mode=display">
\mathcal{L} = \frac{1}{2K}\sum_{k=1}^K(\bm y^{(k)} - \bm z^{(k)})^2,</script><p>其中$\bm z^{(k)}$是网络对第$k$个样本的预测值。根据损失函数关于参数的偏导公式，得到$\mathcal{L}(\cdot)$对于参数$\bm w$和$b$的偏导数，</p>
<script type="math/tex; mode=display">
    \frac{\partial \mathcal{L}}{\partial \bm w} = \frac{1}{K}\sum_{k=1}^K\bm x^{(k)}(\bm z^{(k)} - \bm y^{(k)}) = \frac{1}{K}\bm X^\top(\bm z - \bm y), \\
    \frac{\partial \mathcal{L}}{\partial b} = \frac{1}{K}\sum_{k=1}^K(\bm z^{(k)} - \bm y^{(k)}) = \frac{1}{K}\mathbf{1}^\top(\bm z - \bm y).</script><p>定义Linear算子，实现一个线性层的前向和反向计算。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, weight_init=paddle.standard_normal, bias_init=paddle.zeros</span>):</span><br><span class="line">        <span class="built_in">super</span>(Linear, self).__init__()</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">&#x27;W&#x27;</span>] = weight_init(shape=[input_size, <span class="number">1</span>])</span><br><span class="line">        self.params[<span class="string">&#x27;b&#x27;</span>] = bias_init(shape=[<span class="number">1</span>])</span><br><span class="line">        self.inputs = <span class="literal">None</span></span><br><span class="line">        self.grads = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        self.inputs = inputs</span><br><span class="line">        self.outputs = paddle.matmul(self.inputs, self.params[<span class="string">&#x27;W&#x27;</span>]) + self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> self.outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, labels</span>):</span><br><span class="line">        K = self.inputs.shape[<span class="number">0</span>]</span><br><span class="line">        self.grads[<span class="string">&#x27;W&#x27;</span>] = <span class="number">1.</span> /K * paddle.matmul(self.inputs.T, (self.outputs - labels))</span><br><span class="line">        self.grads[<span class="string">&#x27;b&#x27;</span>] = <span class="number">1.</span> /K * paddle.<span class="built_in">sum</span>(self.outputs - labels, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><strong>笔记</strong></p>
<p>这里backward函数中实现的梯度并不是forward函数对应的梯度，而是最终损失关于参数的梯度．由于这里的梯度是手动计算的，所以直接给出了最终的梯度。</p>
<p><strong>训练函数</strong>  在准备好样本数据和网络以后，复用优化器SimpleBatchGD类，使用小批量梯度下降来进行简单的拟合实验。</p>
<p>这里我们重新定义模型训练train函数。主要以下两点原因：</p>
<ul>
<li>在一般的随机梯度下降中要在每回合迭代开始之前随机打乱训练数据的顺序，再按批大小进行分组。这里为了保证每次运行结果一致以便更好地对比不同的优化算法，这里不再随机打乱数据。</li>
<li>与RunnerV2中的训练函数相比，这里使用小批量梯度下降。而与RunnerV3中的训练函数相比，又通过继承优化器基类Optimizer实现不同的优化器。</li>
</ul>
<p>模型训练train函数的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">data, num_epochs, batch_size, model, calculate_loss, optimizer, verbose=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练神经网络</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - data：训练样本</span></span><br><span class="line"><span class="string">        - num_epochs：训练回合数</span></span><br><span class="line"><span class="string">        - batch_size：批大小</span></span><br><span class="line"><span class="string">        - model：实例化的模型</span></span><br><span class="line"><span class="string">        - calculate_loss：损失函数</span></span><br><span class="line"><span class="string">        - optimizer：优化器</span></span><br><span class="line"><span class="string">        - verbose：日志显示，默认为False</span></span><br><span class="line"><span class="string">    输出：</span></span><br><span class="line"><span class="string">        - iter_loss：每一次迭代的损失值</span></span><br><span class="line"><span class="string">        - epoch_loss：每个回合的平均损失值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 记录每个回合损失的变化</span></span><br><span class="line">    epoch_loss = []</span><br><span class="line">    <span class="comment"># 记录每次迭代损失的变化</span></span><br><span class="line">    iter_loss = []</span><br><span class="line">    N = <span class="built_in">len</span>(data)</span><br><span class="line">    <span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># np.random.shuffle(data) #不再随机打乱数据</span></span><br><span class="line">        <span class="comment"># 将训练数据进行拆分，每个mini_batch包含batch_size条的数据</span></span><br><span class="line">        mini_batches = [data[i:i+batch_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, batch_size)]</span><br><span class="line">        <span class="keyword">for</span> iter_id, mini_batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(mini_batches):</span><br><span class="line">            <span class="comment"># data中前两个分量为X</span></span><br><span class="line">            inputs = mini_batch[:, :-<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># data中最后一个分量为y</span></span><br><span class="line">            labels = mini_batch[:, -<span class="number">1</span>:]</span><br><span class="line">            <span class="comment"># 前向计算</span></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss = calculate_loss(outputs, labels).numpy()[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 计算梯度</span></span><br><span class="line">            model.backward(labels)</span><br><span class="line">            <span class="comment"># 梯度更新</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            iter_loss.append(loss)</span><br><span class="line">        <span class="comment"># verbose = True 则打印当前回合的损失</span></span><br><span class="line">        <span class="keyword">if</span> verbose:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch &#123;:3d&#125;, loss = &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch_id, np.mean(iter_loss)))</span><br><span class="line">        epoch_loss.append(np.mean(iter_loss))</span><br><span class="line">    <span class="keyword">return</span> iter_loss, epoch_loss</span><br></pre></td></tr></table></figure>
<p><strong>优化过程可视化</strong>  定义plot_loss函数，用于绘制损失函数变化趋势。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_loss</span>(<span class="params">iter_loss, epoch_loss, fig_name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    可视化损失函数的变化趋势</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">    ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">    ax1.plot(iter_loss, color=<span class="string">&#x27;#e4007f&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;iteration loss&#x27;</span>)</span><br><span class="line">    ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">    ax2.plot(epoch_loss, color=<span class="string">&#x27;#f19ec2&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;epoch loss&#x27;</span>)</span><br><span class="line">    plt.savefig(fig_name)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>对于使用不同优化器的模型训练，保存每一个回合损失的更新情况，并绘制出损失函数的变化趋势，以此验证模型是否收敛。定义train_and_plot函数，调用train和plot_loss函数，训练并展示每个回合和每次迭代(Iteration)的损失变化情况。在模型训练时，使用paddle.nn.MSELoss()计算均方误差。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_plot</span>(<span class="params">optimizer, fig_name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练网络并画出损失函数的变化趋势</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - optimizer：优化器</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 定义均方差损失</span></span><br><span class="line">    mse = nn.MSELoss()</span><br><span class="line">    iter_loss, epoch_loss = train(data, num_epochs=<span class="number">30</span>, batch_size=<span class="number">64</span>, model=model, calculate_loss=mse, optimizer=optimizer)</span><br><span class="line">    plot_loss(iter_loss, epoch_loss, fig_name)</span><br></pre></td></tr></table></figure>
<p>训练网络并可视化损失函数的变化趋势。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 定义网络结构</span></span><br><span class="line">model = Linear(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">opt = SimpleBatchGD(init_lr=<span class="number">0.01</span>, model=model)</span><br><span class="line">train_and_plot(opt, <span class="string">&#x27;opti-loss.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_32_0.png" alt="png"></p>
<p>从输出结果看，loss在不断减小，模型逐渐收敛。</p>
<p><strong>提醒</strong><br>在本小节中，我们定义了两个实验：2D可视化实验和简单拟合实验。这两个实验会在本节介绍的所有优化算法中反复使用，以便进行对比。</p>
<p><strong>与Paddle API对比，验证正确性</strong></p>
<p>分别实例化自定义SimpleBatchGD优化器和调用paddle.optimizer.SGD API, 验证自定义优化器的正确性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x = data[<span class="number">0</span>, :-<span class="number">1</span>].unsqueeze(<span class="number">0</span>)</span><br><span class="line">y = data[<span class="number">0</span>, -<span class="number">1</span>].unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">model1 = Linear(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;model1 parameter W: &#x27;</span>, model1.params[<span class="string">&#x27;W&#x27;</span>].numpy())</span><br><span class="line">opt1 = SimpleBatchGD(init_lr=<span class="number">0.01</span>, model=model1)</span><br><span class="line">output1 = model1(x)</span><br><span class="line"></span><br><span class="line">model2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>, paddle.nn.initializer.Assign(model1.params[<span class="string">&#x27;W&#x27;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;model2 parameter W: &#x27;</span>, model2.state_dict()[<span class="string">&#x27;weight&#x27;</span>].numpy())</span><br><span class="line">output2 = model2(x)</span><br><span class="line"></span><br><span class="line">model1.backward(y)</span><br><span class="line">opt1.step()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;model1 parameter W after train step: &#x27;</span>, model1.params[<span class="string">&#x27;W&#x27;</span>].numpy())</span><br><span class="line"></span><br><span class="line">opt2 = optimizer.SGD(learning_rate=<span class="number">0.01</span>, parameters=model2.parameters())</span><br><span class="line">loss = paddle.nn.functional.mse_loss(output2, y) / <span class="number">2</span></span><br><span class="line">loss.backward()</span><br><span class="line">opt2.step()</span><br><span class="line">opt2.clear_grad()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;model2 parameter W after train step: &#x27;</span>, model2.state_dict()[<span class="string">&#x27;weight&#x27;</span>].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>model1 parameter W:  [[-4.080414 ]
 [-1.3719953]]
model2 parameter W:  [[-4.080414 ]
 [-1.3719953]]
model1 parameter W after train step:  [[-3.196255 ]
 [-1.0747064]]
model2 parameter W after train step:  [[-3.196255 ]
 [-1.0747064]]
</code></pre><p>从输出结果看，在一次梯度更新后，两个模型的参数值保持一致，证明优化器实现正确。</p>
<h3 id="7-3-2-学习率调整"><a href="#7-3-2-学习率调整" class="headerlink" title="7.3.2 学习率调整"></a>7.3.2 学习率调整</h3><p>学习率是神经网络优化时的重要超参数。在梯度下降法中，学习率$\alpha$的取值非常关键，如果取值过大就不会收敛，如果过小则收敛速度太慢。</p>
<p>常用的学习率调整方法包括如下几种方法：</p>
<ul>
<li>学习率衰减：如分段常数衰减（Piecewise Constant Decay）、余弦衰减（Cosine Decay）等；</li>
<li>学习率预热：如逐渐预热(Gradual Warmup) 等；</li>
<li>周期性学习率调整：如循环学习率等；</li>
<li>自适应调整学习率的方法：如AdaGrad、RMSprop、AdaDelta等。自适应学习率方法可以针对每个参数设置不同的学习率。</li>
</ul>
<p><strong>动手练习7.2</strong><br>尝试《神经网络与深度学习》中第7.2.3.1节中定义的不同学习率衰减方法，重复上面的实验并分析实验结果。</p>
<p>下面我们来详细介绍AdaGrad和RMSprop算法。</p>
<h4 id="7-3-2-1-AdaGrad算法"><a href="#7-3-2-1-AdaGrad算法" class="headerlink" title="7.3.2.1 AdaGrad算法"></a>7.3.2.1 AdaGrad算法</h4><p>AdaGrad算法（Adaptive Gradient Algorithm，自适应梯度算法)是借鉴 $\ell_2$ 正则化的思想，每次迭代时自适应地调整每个参数的学习率。在第$t$次迭代时，先计算每个参数梯度平方的累计值。</p>
<script type="math/tex; mode=display">
G_t = \sum^t_{\tau=1} \mathbf g_{\tau} \odot \mathbf g_{\tau},</script><p>其中$\odot$为按元素乘积，$\mathbf g_{\tau} \in \mathbb R^{\mid \theta \mid}$是第$\tau$次迭代时的梯度。</p>
<script type="math/tex; mode=display">
\Delta \theta_t = - \frac{\alpha}{\sqrt{G_t + \epsilon}} \odot \mathbf g_{t},</script><p>其中$\alpha$是初始的学习率，$\epsilon$是为了保持数值稳定性而设置的非常小的常数，一般取值$e^{−7}$到$e^{−10}$。此外，这里的开平方、除、加运算都是按元素进行的操作。</p>
<p><strong>构建优化器</strong>  定义Adagrad类，继承Optimizer类。定义step函数调用adagrad进行参数更新。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl.op <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Adagrad</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_lr, model, epsilon</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Adagrad 优化器初始化</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - init_lr： 初始学习率</span></span><br><span class="line"><span class="string">            - model：模型，model.params存储模型参数值</span></span><br><span class="line"><span class="string">            - epsilon：保持数值稳定性而设置的非常小的常数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Adagrad, self).__init__(init_lr=init_lr, model=model)</span><br><span class="line">        self.G = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.model.params.keys():</span><br><span class="line">            self.G[key] = <span class="number">0</span></span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adagrad</span>(<span class="params">self, x, gradient_x, G, init_lr</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        adagrad算法更新参数，G为参数梯度平方的累计值。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        G += gradient_x ** <span class="number">2</span></span><br><span class="line">        x -= init_lr / paddle.sqrt(G + self.epsilon) * gradient_x</span><br><span class="line">        <span class="keyword">return</span> x, G</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数更新</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.model.params.keys():</span><br><span class="line">            self.model.params[key], self.G[key] = self.adagrad(self.model.params[key], </span><br><span class="line">                                                               self.model.grads[key], </span><br><span class="line">                                                               self.G[key], </span><br><span class="line">                                                               self.init_lr)                </span><br></pre></td></tr></table></figure>
<p><strong>2D可视化实验</strong>  使用被优化函数展示Adagrad算法的参数更新轨迹。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">w = paddle.to_tensor([<span class="number">0.2</span>, <span class="number">2</span>])</span><br><span class="line">model = OptimizedFunction(w)</span><br><span class="line">opt = Adagrad(init_lr=<span class="number">0.5</span>, model=model, epsilon=<span class="number">1e-7</span>)</span><br><span class="line">train_and_plot_f(model, opt, epoch=<span class="number">50</span>, fig_name=<span class="string">&#x27;opti-vis-para2.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>x1 initiate: [3.], x2 initiate: [4.]
</code></pre><p><img src="/img-nndl/output_40_1.png" alt="png"></p>
<p>从输出结果看，AdaGrad算法在前几个回合更新时参数更新幅度较大，随着回合数增加，学习率逐渐缩小，参数更新幅度逐渐缩小。在AdaGrad算法中，如果某个参数的偏导数累积比较大，其学习率相对较小。相反，如果其偏导数累积较小，其学习率相对较大。但整体随着迭代次数的增加，学习率逐渐缩小。该算法的缺点是在经过一定次数的迭代依然没有找到最优点时，由于这时的学习率已经非常小，很难再继续找到最优点。</p>
<p><strong>简单拟合实验</strong>  训练单层线性网络，验证损失是否收敛。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 定义网络结构</span></span><br><span class="line">model = Linear(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">opt = Adagrad(init_lr=<span class="number">0.1</span>, model=model, epsilon=<span class="number">1e-7</span>)</span><br><span class="line">train_and_plot(opt, <span class="string">&#x27;opti-loss2.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_42_0.png" alt="png"></p>
<h4 id="7-3-2-2-RMSprop算法"><a href="#7-3-2-2-RMSprop算法" class="headerlink" title="7.3.2.2 RMSprop算法"></a>7.3.2.2 RMSprop算法</h4><p>RMSprop算法是一种自适应学习率的方法，可以在有些情况下避免AdaGrad算法中学习率不断单调下降以至于过早衰减的缺点。</p>
<p>RMSprop算法首先计算每次迭代梯度平方$\mathbf g_{t}^{2}$的加权移动平均</p>
<script type="math/tex; mode=display">
G_t = \beta G_{t-1} + (1 - \beta) \mathbf g_t \odot \mathbf g_t,</script><p>其中$\beta$为衰减率，一般取值为0.9。</p>
<p>RMSprop算法的参数更新差值为：</p>
<script type="math/tex; mode=display">
\Delta \theta_t = - \frac{\alpha}{\sqrt{G_t + \epsilon}} \odot \mathbf g_t,</script><p>其中$\alpha$是初始的学习率，比如0.001。RMSprop算法和AdaGrad算法的区别在于RMSprop算法中$G_t$的计算由累积方式变成了加权移动平均。在迭代过程中，每个参数的学习率并不是呈衰减趋势，既可以变小也可以变大。</p>
<p><strong>构建优化器</strong>  定义RMSprop类，继承Optimizer类。定义step函数调用rmsprop更新参数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RMSprop</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_lr, model, beta, epsilon</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        RMSprop优化器初始化</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - init_lr：初始学习率</span></span><br><span class="line"><span class="string">            - model：模型，model.params存储模型参数值</span></span><br><span class="line"><span class="string">            - beta：衰减率</span></span><br><span class="line"><span class="string">            - epsilon：保持数值稳定性而设置的常数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(RMSprop, self).__init__(init_lr=init_lr, model=model)</span><br><span class="line">        self.G = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.model.params.keys():</span><br><span class="line">            self.G[key] = <span class="number">0</span></span><br><span class="line">        self.beta = beta</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rmsprop</span>(<span class="params">self, x, gradient_x, G, init_lr</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        rmsprop算法更新参数，G为迭代梯度平方的加权移动平均</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        G = self.beta * G + (<span class="number">1</span> - self.beta) * gradient_x ** <span class="number">2</span></span><br><span class="line">        x -= init_lr / paddle.sqrt(G + self.epsilon) * gradient_x</span><br><span class="line">        <span class="keyword">return</span> x, G</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;参数更新&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.model.params.keys():</span><br><span class="line">            self.model.params[key], self.G[key] = self.rmsprop(self.model.params[key], </span><br><span class="line">                                                               self.model.grads[key],</span><br><span class="line">                                                               self.G[key], </span><br><span class="line">                                                               self.init_lr)</span><br></pre></td></tr></table></figure>
<p><strong>2D可视化实验</strong>  使用被优化函数展示RMSprop算法的参数更新轨迹。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">w = paddle.to_tensor([<span class="number">0.2</span>, <span class="number">2</span>])</span><br><span class="line">model = OptimizedFunction(w)</span><br><span class="line">opt = RMSprop(init_lr=<span class="number">0.1</span>, model=model, beta=<span class="number">0.9</span>, epsilon=<span class="number">1e-7</span>)</span><br><span class="line">train_and_plot_f(model, opt, epoch=<span class="number">50</span>, fig_name=<span class="string">&#x27;opti-vis-para3.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>x1 initiate: [3.], x2 initiate: [4.]
</code></pre><p><img src="/img-nndl/output_46_1.png" alt="png"></p>
<p><strong>简单拟合实验</strong>  训练单层线性网络，进行简单的拟合实验。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 定义网络结构</span></span><br><span class="line">model = Linear(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">opt = RMSprop(init_lr=<span class="number">0.1</span>, model=model, beta=<span class="number">0.9</span>, epsilon=<span class="number">1e-7</span>)</span><br><span class="line">train_and_plot(opt, <span class="string">&#x27;opti-loss3.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_48_0.png" alt="png"></p>
<p><strong>动手练习7.4</strong></p>
<p>动手实现AdaDelta算法。</p>
<h3 id="7-3-3-梯度估计修正"><a href="#7-3-3-梯度估计修正" class="headerlink" title="7.3.3 梯度估计修正"></a>7.3.3 梯度估计修正</h3><p>除了调整学习率之外，还可以进行梯度估计修正。在小批量梯度下降法中，由于每次迭代的样本具有一定的随机性，因此每次迭代的梯度估计和整个训练集上的最优梯度并不一致。如果每次选取样本数量比较小，损失会呈振荡的方式下降。</p>
<p>一种有效地缓解梯度估计随机性的方式是通过使用最近一段时间内的平均梯度来代替当前时刻的随机梯度来作为参数更新的方向，从而提高优化速度。</p>
<h4 id="7-3-3-1-动量法"><a href="#7-3-3-1-动量法" class="headerlink" title="7.3.3.1 动量法"></a>7.3.3.1 动量法</h4><p>动量法（Momentum Method）是用之前积累动量来替代真正的梯度。每次迭代的梯度可以看作加速度。</p>
<p>在第$t$次迭代时，计算负梯度的“加权移动平均”作为参数的更新方向，</p>
<script type="math/tex; mode=display">
\Delta \theta_t = \rho \Delta \theta_{t-1} - \alpha \mathbf g_t = - \alpha \sum_{\tau=1}^t\rho^{t - \tau} \mathbf g_{\tau},</script><p>其中$\rho$为动量因子，通常设为0.9，$\alpha$为学习率。</p>
<p>这样，每个参数的实际更新差值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内的梯度方向不一致时，其真实的参数更新幅度变小。相反，当某个参数在最近一段时间内的梯度方向都一致时，其真实的参数更新幅度变大，起到加速作用。一般而言，在迭代初期，梯度方向都比较一致，动量法会起到加速作用，可以更快地到达最优点。在迭代后期，梯度方向会不一致，在收敛值附近振荡，动量法会起到减速作用，增加稳定性。从某种角度来说，当前梯度叠加上部分的上次梯度，一定程度上可以近似看作二阶梯度。</p>
<p><strong>构建优化器</strong>  定义Momentum类，继承Optimizer类。定义step函数调用momentum进行参数更新。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Momentum</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_lr, model, rho</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Momentum优化器初始化</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - init_lr：初始学习率</span></span><br><span class="line"><span class="string">            - model：模型，model.params存储模型参数值</span></span><br><span class="line"><span class="string">            - rho：动量因子</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Momentum, self).__init__(init_lr=init_lr, model=model)</span><br><span class="line">        self.delta_x = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.model.params.keys():</span><br><span class="line">            self.delta_x[key] = <span class="number">0</span></span><br><span class="line">        self.rho = rho</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">momentum</span>(<span class="params">self, x, gradient_x, delta_x, init_lr</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        momentum算法更新参数，delta_x为梯度的加权移动平均</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        delta_x = self.rho * delta_x - init_lr * gradient_x</span><br><span class="line">        x += delta_x</span><br><span class="line">        <span class="keyword">return</span> x, delta_x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;参数更新&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.model.params.keys():</span><br><span class="line">            self.model.params[key], self.delta_x[key] = self.momentum(self.model.params[key], </span><br><span class="line">                                                                      self.model.grads[key], </span><br><span class="line">                                                                      self.delta_x[key], </span><br><span class="line">                                                                      self.init_lr) </span><br></pre></td></tr></table></figure>
<p><strong>2D可视化实验</strong>  使用被优化函数展示Momentum算法的参数更新轨迹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">w = paddle.to_tensor([<span class="number">0.2</span>, <span class="number">2</span>])</span><br><span class="line">model = OptimizedFunction(w)</span><br><span class="line">opt = Momentum(init_lr=<span class="number">0.01</span>, model=model, rho=<span class="number">0.9</span>)</span><br><span class="line">train_and_plot_f(model, opt, epoch=<span class="number">50</span>, fig_name=<span class="string">&#x27;opti-vis-para4.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>x1 initiate: [3.], x2 initiate: [4.]
</code></pre><p><img src="/img-nndl/output_53_1.png" alt="png"></p>
<p>从输出结果看，在模型训练初期，梯度方向比较一致，参数更新幅度逐渐增大，起加速作用；在迭代后期，参数更新幅度减小，在收敛值附近振荡。</p>
<p><strong>简单拟合实验</strong>  训练单层线性网络，进行简单的拟合实验。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络结构</span></span><br><span class="line">model = Linear(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">opt = Momentum(init_lr=<span class="number">0.01</span>, model=model, rho=<span class="number">0.9</span>)</span><br><span class="line">train_and_plot(opt, <span class="string">&#x27;opti-loss4.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_55_0.png" alt="png"></p>
<h4 id="7-3-3-2-Adam算法"><a href="#7-3-3-2-Adam算法" class="headerlink" title="7.3.3.2 Adam算法"></a>7.3.3.2 Adam算法</h4><p>Adam算法（Adaptive Moment Estimation Algorithm，自适应矩估计算法）可以看作动量法和RMSprop算法的结合，不但使用动量作为参数更新方向，而且可以自适应调整学习率。</p>
<p>Adam算法一方面计算梯度平方$\mathbf g_t^2$的加权移动平均（和RMSprop算法类似），另一方面计算梯度$\mathbf g_t$的加权移动平均（和动量法类似）。</p>
<script type="math/tex; mode=display">
M_t = \beta_1 M_{t-1} + (1 - \beta_1)\mathbf g_t,  \\
G_t = \beta_2 G_{t-1} + (1 - \beta_2)\mathbf g_t \odot \mathbf g_t,</script><p>其中$\beta_1$和$\beta_2$分别为两个移动平均的衰减率，通常取值为$\beta_1 = 0.9, \beta_2 = 0.99$。我们可以把$M_t$和$G_t$分别看作梯度的均值(一阶矩)和未减去均值的方差(二阶矩)。</p>
<p>假设$M_0 = 0, G_0 = 0$，那么在迭代初期$M_t$和$G_t$的值会比真实的均值和方差要小。特别是当$\beta_1$和$\beta_2$都接近于1时，偏差会很大。因此，需要对偏差进行修正。</p>
<script type="math/tex; mode=display">
\hat M_t = \frac{M_t}{1 - \beta^t_1},  \\
\hat G_t = \frac{G_t}{1 - \beta^t_2}。</script><p>Adam算法的参数更新差值为</p>
<script type="math/tex; mode=display">
\Delta \theta_t = - \frac{\alpha}{\sqrt{\hat G_t + \epsilon}}\hat M_t,</script><p>其中学习率$\alpha$通常设为0.001，并且也可以进行衰减，比如$a_t = \frac{a_0}{\sqrt{t}}$。</p>
<p><strong>构建优化器</strong>  定义Adam类，继承Optimizer类。定义step函数调用adam函数更新参数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Adam</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_lr, model, beta1, beta2, epsilon</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Adam优化器初始化</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - init_lr：初始学习率</span></span><br><span class="line"><span class="string">            - model：模型，model.params存储模型参数值</span></span><br><span class="line"><span class="string">            - beta1, beta2：移动平均的衰减率</span></span><br><span class="line"><span class="string">            - epsilon：保持数值稳定性而设置的常数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Adam, self).__init__(init_lr=init_lr, model=model)</span><br><span class="line">        self.beta1 = beta1</span><br><span class="line">        self.beta2 = beta2</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.M, self.G = &#123;&#125;, &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.model.params.keys():</span><br><span class="line">            self.M[key] = <span class="number">0</span></span><br><span class="line">            self.G[key] = <span class="number">0</span></span><br><span class="line">        self.t = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">self, x, gradient_x, G, M, t, init_lr</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        adam算法更新参数</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - x：参数</span></span><br><span class="line"><span class="string">            - G：梯度平方的加权移动平均</span></span><br><span class="line"><span class="string">            - M：梯度的加权移动平均</span></span><br><span class="line"><span class="string">            - t：迭代次数</span></span><br><span class="line"><span class="string">            - init_lr：初始学习率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        M = self.beta1 * M + (<span class="number">1</span> - self.beta1) * gradient_x</span><br><span class="line">        G = self.beta2 * G + (<span class="number">1</span> - self.beta2) * gradient_x ** <span class="number">2</span></span><br><span class="line">        M_hat = M / (<span class="number">1</span> - self.beta1 ** t)</span><br><span class="line">        G_hat = G / (<span class="number">1</span> - self.beta2 ** t)</span><br><span class="line">        t += <span class="number">1</span></span><br><span class="line">        x -= init_lr / paddle.sqrt(G_hat + self.epsilon) * M_hat</span><br><span class="line">        <span class="keyword">return</span> x, G, M, t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;参数更新&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.model.params.keys():</span><br><span class="line">            self.model.params[key], self.G[key], self.M[key], self.t = self.adam(self.model.params[key], </span><br><span class="line">                                                                                 self.model.grads[key],</span><br><span class="line">                                                                                 self.G[key], </span><br><span class="line">                                                                                 self.M[key],</span><br><span class="line">                                                                                 self.t, </span><br><span class="line">                                                                                 self.init_lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                                                                                 </span><br></pre></td></tr></table></figure>
<p><strong>2D可视化实验</strong> 使用被优化函数展示Adam算法的参数更新轨迹。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">w = paddle.to_tensor([<span class="number">0.2</span>, <span class="number">2</span>])</span><br><span class="line">model = OptimizedFunction(w)</span><br><span class="line">opt = Adam(init_lr=<span class="number">0.2</span>, model=model, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.99</span>, epsilon=<span class="number">1e-7</span>)</span><br><span class="line">train_and_plot_f(model, opt, epoch=<span class="number">20</span>, fig_name=<span class="string">&#x27;opti-vis-para5.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>x1 initiate: [3.], x2 initiate: [4.]
</code></pre><p><img src="/img-nndl/output_59_1.png" alt="png"></p>
<p>从输出结果看，Adam算法可以自适应调整学习率，参数更新更加平稳。</p>
<p><strong>简单拟合实验</strong>  训练单层线性网络，进行简单的拟合实验。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 定义网络结构</span></span><br><span class="line">model = Linear(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">opt = Adam(init_lr=<span class="number">0.1</span>, model=model, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.99</span>, epsilon=<span class="number">1e-7</span>)</span><br><span class="line">train_and_plot(opt, <span class="string">&#x27;opti-loss5.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/img-nndl/output_61_0.png" alt="png"></p>
<p><strong>动手练习7.5</strong></p>
<p>学习AdamW算法，通过飞桨 API调用LeNet网络和MNIST数据集，使用paddle.optimizer.AdamW作为优化器训练网络，进行简单的拟合实验。</p>
<h3 id="7-3-4-不同优化器的3D可视化对比"><a href="#7-3-4-不同优化器的3D可视化对比" class="headerlink" title="7.3.4 不同优化器的3D可视化对比"></a>7.3.4 不同优化器的3D可视化对比</h3><h4 id="7-3-4-1-构建一个三维空间中的被优化函数"><a href="#7-3-4-1-构建一个三维空间中的被优化函数" class="headerlink" title="7.3.4.1 构建一个三维空间中的被优化函数"></a>7.3.4.1 构建一个三维空间中的被优化函数</h4><p>定义OptimizedFunction3D算子，表示被优化函数$f(\bm x) = \bm x[0]^2 + \bm x[1]^2 + \bm x[1]^3 + \bm x[0]*\bm x[1]$，其中$\bm x[0]$, $\bm x[1]$代表两个参数。该函数在(0,0)处存在鞍点，即一个既不是极大值点也不是极小值点的临界点。希望训练过程中，优化算法可以使参数离开鞍点，向模型最优解收敛。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">OptimizedFunction3D</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(OptimizedFunction3D, self).__init__()</span><br><span class="line">        self.params = &#123;<span class="string">&#x27;x&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">        self.grads = &#123;<span class="string">&#x27;x&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        self.params[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line">        <span class="keyword">return</span> x[<span class="number">0</span>] ** <span class="number">2</span> + x[<span class="number">1</span>] ** <span class="number">2</span> + x[<span class="number">1</span>] ** <span class="number">3</span> + x[<span class="number">0</span>]*x[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self</span>):</span><br><span class="line">        x = self.params[<span class="string">&#x27;x&#x27;</span>]</span><br><span class="line">        gradient1 = <span class="number">2</span> * x[<span class="number">0</span>] + x[<span class="number">1</span>]</span><br><span class="line">        gradient2 = <span class="number">2</span> * x[<span class="number">1</span>] + <span class="number">3</span> * x[<span class="number">1</span>] ** <span class="number">2</span> + x[<span class="number">0</span>]</span><br><span class="line">        self.grads[<span class="string">&#x27;x&#x27;</span>] = paddle.concat([gradient1, gradient2])</span><br></pre></td></tr></table></figure>
<p>对于相同的被优化函数，分别使用不同的优化器进行参数更新，并保存不同优化器下参数更新的值，用于可视化。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建5个模型，分别配备不同的优化器</span></span><br><span class="line">model1 = OptimizedFunction3D()</span><br><span class="line">opt_gd = SimpleBatchGD(init_lr=<span class="number">0.01</span>, model=model1)</span><br><span class="line"></span><br><span class="line">model2 = OptimizedFunction3D()</span><br><span class="line">opt_adagrad = Adagrad(init_lr=<span class="number">0.5</span>, model=model2, epsilon=<span class="number">1e-7</span>)</span><br><span class="line"></span><br><span class="line">model3 = OptimizedFunction3D()</span><br><span class="line">opt_rmsprop = RMSprop(init_lr=<span class="number">0.1</span>, model=model3, beta=<span class="number">0.9</span>, epsilon=<span class="number">1e-7</span>)</span><br><span class="line"></span><br><span class="line">model4 = OptimizedFunction3D()</span><br><span class="line">opt_momentum = Momentum(init_lr=<span class="number">0.01</span>, model=model4, rho=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">model5 = OptimizedFunction3D()</span><br><span class="line">opt_adam = Adam(init_lr=<span class="number">0.1</span>, model=model5, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.99</span>, epsilon=<span class="number">1e-7</span>)</span><br><span class="line"></span><br><span class="line">models = [model1, model2, model3, model4, model5]</span><br><span class="line">opts = [opt_gd, opt_adagrad, opt_rmsprop, opt_momentum, opt_adam]</span><br><span class="line"></span><br><span class="line">x_all_opts = []</span><br><span class="line">z_all_opts = []</span><br><span class="line">x_init = paddle.to_tensor([<span class="number">2</span>, <span class="number">3</span>], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"><span class="comment"># 使用不同优化器训练</span></span><br><span class="line"><span class="keyword">for</span> model, opt <span class="keyword">in</span> <span class="built_in">zip</span>(models, opts):</span><br><span class="line">    x_one_opt, z_one_opt = train_f(model, opt, x_init, <span class="number">150</span>)</span><br><span class="line">    <span class="comment"># 保存参数值</span></span><br><span class="line">    x_all_opts.append(x_one_opt.numpy())</span><br><span class="line">    z_all_opts.append(np.squeeze(z_one_opt))</span><br></pre></td></tr></table></figure>
<p>定义Visualization3D函数，用于可视化三维的参数更新轨迹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> animation</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> zip_longest</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Visualization3D</span>(animation.FuncAnimation):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    绘制动态图像，可视化参数更新轨迹</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *xy_values, z_values, labels=[], colors=[], fig, ax, interval=<span class="number">60</span>, blit=<span class="literal">True</span>, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化3d可视化类</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            xy_values：三维中x,y维度的值</span></span><br><span class="line"><span class="string">            z_values：三维中z维度的值</span></span><br><span class="line"><span class="string">            labels：每个参数更新轨迹的标签</span></span><br><span class="line"><span class="string">            colors：每个轨迹的颜色</span></span><br><span class="line"><span class="string">            interval：帧之间的延迟（以毫秒为单位）</span></span><br><span class="line"><span class="string">            blit：是否优化绘图</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.fig = fig</span><br><span class="line">        self.ax = ax</span><br><span class="line">        self.xy_values = xy_values</span><br><span class="line">        self.z_values = z_values</span><br><span class="line">        frames = <span class="built_in">max</span>(xy_value.shape[<span class="number">0</span>] <span class="keyword">for</span> xy_value <span class="keyword">in</span> xy_values)</span><br><span class="line">        self.lines = [ax.plot([], [], [], label=label, color=color, lw=<span class="number">2</span>)[<span class="number">0</span>]</span><br><span class="line">                      <span class="keyword">for</span> _, label, color <span class="keyword">in</span> zip_longest(xy_values, labels, colors)]</span><br><span class="line">        <span class="built_in">super</span>(Visualization3D, self).__init__(fig, self.animate, init_func=self.init_animation, frames=frames, interval=interval, blit=blit, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_animation</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 数值初始化</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> self.lines:</span><br><span class="line">            line.set_data([], [])</span><br><span class="line">            line.set_3d_properties([])</span><br><span class="line">        <span class="keyword">return</span> self.lines</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">animate</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="comment"># 将x,y,z三个数据传入，绘制三维图像</span></span><br><span class="line">        <span class="keyword">for</span> line, xy_value, z_value <span class="keyword">in</span> <span class="built_in">zip</span>(self.lines, self.xy_values, self.z_values):</span><br><span class="line">            line.set_data(xy_value[:i, <span class="number">0</span>], xy_value[:i, <span class="number">1</span>])</span><br><span class="line">            line.set_3d_properties(z_value[:i])</span><br><span class="line">        <span class="keyword">return</span> self.lines</span><br></pre></td></tr></table></figure>
<p>绘制出被优化函数的三维图像。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用numpy.meshgrid生成x1,x2矩阵，矩阵的每一行为[-3, 3]，以0.1为间隔的数值</span></span><br><span class="line">x1 = np.arange(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">x2 = np.arange(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">x1, x2 = np.meshgrid(x1, x2)</span><br><span class="line">init_x = paddle.to_tensor([x1, x2])</span><br><span class="line">model = OptimizedFunction3D()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制f_3d函数的三维图像</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = plt.axes(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(init_x[<span class="number">0</span>], init_x[<span class="number">1</span>], model(init_x), color=<span class="string">&#x27;#f19ec2&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&#x27;f(x1,x2)&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;opti-f-3d.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:101: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead
  ret = np.asscalar(ex)
</code></pre><p><img src="/img-nndl/output_70_1.png" alt="png"></p>
<p>可视化不同优化器情况下参数变化轨迹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">&#x27;SGD&#x27;</span>, <span class="string">&#x27;AdaGrad&#x27;</span>, <span class="string">&#x27;RMSprop&#x27;</span>, <span class="string">&#x27;Momentum&#x27;</span>, <span class="string">&#x27;Adam&#x27;</span>]</span><br><span class="line">colors = [<span class="string">&#x27;#9c9d9f&#x27;</span>, <span class="string">&#x27;#f7d2e2&#x27;</span>, <span class="string">&#x27;#f19ec2&#x27;</span>, <span class="string">&#x27;#e86096&#x27;</span>, <span class="string">&#x27;#000000&#x27;</span>]</span><br><span class="line"></span><br><span class="line">anim = Visualization3D(*x_all_opts, z_values=z_all_opts, labels=labels, colors=colors, fig=fig, ax=ax)</span><br><span class="line">ax.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">HTML(anim.to_html5_video())</span><br></pre></td></tr></table></figure>
<pre><code>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/mpl_toolkits/mplot3d/proj3d.py:141: RuntimeWarning: invalid value encountered in true_divide
  txs, tys, tzs = vecw[0]/w, vecw[1]/w, vecw[2]/w
</code></pre><p>从输出结果看，对于我们构建的函数，有些优化器如Momentum在参数更新时成功逃离鞍点，其他优化器在本次实验中收敛到鞍点处没有成功逃离。但这并不证明Momentum优化器是最好的优化器，在模型训练时使用哪种优化器，还要结合具体的场景和数据具体分析。</p>
<p><strong>动手练习7.6</strong><br>通过调用飞桨API, 实验比较不同优化算法在MNIST数据集上的收敛性。</p>
<h1 id="7-4-参数初始化"><a href="#7-4-参数初始化" class="headerlink" title="7.4 参数初始化"></a>7.4 参数初始化</h1><p>神经网络的参数学习是一个非凸优化问题。当使用梯度下降法来进行网络参数优化时，参数初始值的选取十分关键，关系到网络的优化效率和泛化能力。此外，由于神经网络优化时出现的对称权重现象（参见第4.4.1节），神经网络的参数不能初始化为相同的值，需要有一定的差异性。</p>
<p>常用的参数初始化的方式通常有以下三种：</p>
<ul>
<li>随机初始化：最常用的参数初始化策略，通过一个随机采样函数来生成每个参数的初始值。</li>
<li>预训练初始化：一种在实践中经常使用的初始化策略，如果目标任务的训练数据不足，可以使用一个已经在大规模数据上训练过的模型作为参数初始值。预训练模型在目标任务上的学习过程也称为精调Fine-Tuning。</li>
<li>固定值初始化：对于神经网络中的某些重要参数，可以根据先验知识来初始化。比如对于使用ReLU激活函数的全连接层，其偏置通常可以设为比较小的正数（比如0.01），从而确保这一层的神经元的梯度不为0，避免死亡ReLU现象。</li>
</ul>
<p>虽然预训练初始化通常具有更好的收敛性和泛化性，但是灵活性不够，不能在目标任务上任意地调整网络结构。因此，好的随机初始化方法对训练神经网络模型来说依然十分重要。在本节我们主要介绍两种随机初始化方法：基于固定方差的参数初始化和基于方差缩放的参数初始化。</p>
<h3 id="7-4-1-基于固定方差的参数初始化"><a href="#7-4-1-基于固定方差的参数初始化" class="headerlink" title="7.4.1 基于固定方差的参数初始化"></a>7.4.1 基于固定方差的参数初始化</h3><p>一种最简单的随机初始化方法是从一个固定均值(通常为 0)和方差$\sigma^2$的分布中采样来生成参数的初始值。基于固定方差的参数初始化方法主要有高斯分布初始化和均匀分布初始化两种：</p>
<ul>
<li>高斯分布初始化：使用一个高斯分布$\mathscr{N}(0, \sigma^2)$对每个参数进行随机初始化。</li>
<li>均匀分布初始化：在一个给定的区间$[-r, r]$内采用均匀分布来初始化。</li>
</ul>
<p>高斯分布和均匀分布初始化的实现方式可以参考第4.4.1节参数初始化代码。</p>
<h3 id="7-4-2-基于方差缩放的参数初始化"><a href="#7-4-2-基于方差缩放的参数初始化" class="headerlink" title="7.4.2 基于方差缩放的参数初始化"></a>7.4.2 基于方差缩放的参数初始化</h3><p>初始化一个深度网络时，为了缓解梯度消失或爆炸问题，我们尽可能保持每个神经元的输入和输出的方差一致，根据神经元的连接数量来自适应地调整初始化分布的方差，这类方法称为方差缩放（Variance Scaling）。</p>
<p>Xavier初始化是参数初始化中常用的方法，根据每层的神经元数量来自动计算初始化参数方差。<br>在计算出参数的理想方差后，可以通过高斯分布或均匀分布来随机初始化参数。若神经元采用Tanh函数，并采用高斯分布来随机初始化参数，连接权重$w_i^{(l)}$可以按$\mathscr{N}(0, \frac{2}{M_{l-1} + M_l})$的高斯分布进行初始化，其中$M_{l-1}$是第$l-1$层神经元个数。</p>
<p><strong>笔记</strong></p>
<p>Xavier初始化公式推导可参考《神经网络与深度学习》7.3.2.1 Xavier初始化。</p>
<p>本节动手实现Xavier初始化，并观察其效果。</p>
<h4 id="7-4-2-1-模型构建"><a href="#7-4-2-1-模型构建" class="headerlink" title="7.4.2.1 模型构建"></a>7.4.2.1 模型构建</h4><p>首先定义xavier_normal_std函数，根据$l$层和$l-1$层神经元的数量计算理想标准差。值得注意的是，在paddle.normal API中，通过指定标准差的值来生成符合正态分布的张量，因此，这里需要计算标准差。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">xavier_normal_std</span>(<span class="params">input_size, output_size</span>):</span><br><span class="line">    <span class="keyword">return</span> np.sqrt(<span class="number">2</span> / (input_size + output_size))</span><br></pre></td></tr></table></figure>
<p><strong>笔记</strong></p>
<p>Xavier初始化适用于Logistic激活函数和Tanh激活函数，对于不同激活函数，高斯分布的方差和均匀分布的$r$值计算是不同的。xavier_normal_std定义针对Tanh激活函数的情况。</p>
<p>定义一个全连接前馈网络（即多层感知器）MLP算子，实例化网络时可以通过layers_size指定网络每层神经元的数量，通过init_fn_name指定网络中参数初始化方法(Xavier高斯分布初始化、Xavier均匀分布初始化或$\mathscr{N}(0, 1)$高斯分布初始化)，init_fn指定计算初始化时均值或数值范围的函数，act_fn指定激活函数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers_size, init_fn_name, init_fn, act_fn</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        多层网络初始化</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - layers_size: 每层神经元的数量</span></span><br><span class="line"><span class="string">            - init_fn_name: 网络中参数初始化方法，可以为 &#x27;normal&#x27;或&#x27;uniform&#x27;</span></span><br><span class="line"><span class="string">            - init_fn: 函数，用来计算高斯分布标准差或均匀分布r值</span></span><br><span class="line"><span class="string">            - act_fn: 激活函数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        self.linear = nn.Sequential()</span><br><span class="line">        self.num_layers = <span class="built_in">len</span>(layers_size) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            input_size, output_size = layers_size[i], layers_size[i + <span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> init_fn_name == <span class="string">&#x27;normal&#x27;</span>:</span><br><span class="line">                <span class="comment"># Xavier高斯分布初始化，计算方差</span></span><br><span class="line">                self.linear.add_sublayer(<span class="built_in">str</span>(i), nn.Linear(input_size, output_size,</span><br><span class="line">                                           weight_attr=nn.initializer.Normal(mean=<span class="number">0</span>, std=init_fn(input_size, output_size))))</span><br><span class="line">            <span class="keyword">elif</span> init_fn_name == <span class="string">&#x27;uniform&#x27;</span>:</span><br><span class="line">                r = init_fn(input_size, output_size)</span><br><span class="line">                self.linear.add_sublayer(<span class="built_in">str</span>(i), nn.Linear(input_size, output_size, weight_attr=nn.initializer.Uniform(low=-r, high=r)))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.linear.add_sublayer(<span class="built_in">str</span>(i), nn.Linear(input_size, output_size, weight_attr=nn.initializer.Normal()))</span><br><span class="line">        self.act_fn = act_fn()</span><br><span class="line">        self.z = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向计算</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        y = X</span><br><span class="line">        <span class="keyword">for</span> num_layer <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            y = self.linear[num_layer](y)</span><br><span class="line">            <span class="keyword">if</span> num_layer != self.num_layers - <span class="number">1</span>:</span><br><span class="line">                y = self.act_fn(y)</span><br><span class="line">            self.z[num_layer] = y</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h4 id="7-4-2-2-观察模型神经元的方差变化"><a href="#7-4-2-2-观察模型神经元的方差变化" class="headerlink" title="7.4.2.2 观察模型神经元的方差变化"></a>7.4.2.2 观察模型神经元的方差变化</h4><p><strong>高斯分布初始化</strong>  定义网络每层神经元的数量，指定激活函数和参数初始化方式，通过Xavier高斯分布初始化网络。代码实现如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络每层神经元的数量</span></span><br><span class="line">layers_size = [<span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>, <span class="number">300</span>, <span class="number">200</span>, <span class="number">100</span>]</span><br><span class="line"><span class="comment"># 指定激活函数</span></span><br><span class="line">activate_fn = paddle.nn.Tanh</span><br><span class="line"><span class="comment"># 指定参数初始化方式</span></span><br><span class="line">init_fn_name = <span class="string">&#x27;normal&#x27;</span></span><br><span class="line"></span><br><span class="line">model = MLP(layers_size, init_fn_name, init_fn=xavier_normal_std, act_fn=activate_fn)</span><br><span class="line">inputs = paddle.normal(shape=[<span class="number">1</span>, <span class="number">100</span>], std=<span class="number">0.1</span>)</span><br><span class="line">y = model(inputs)</span><br></pre></td></tr></table></figure>
<p>打印每层神经元输出的方差，观察每层的方差值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(model.z) - <span class="number">1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;layer %d: , %f&#x27;</span>%(i, model.z[i].numpy().var()))</span><br></pre></td></tr></table></figure>
<p>从输出结果看，Xavier初始化可以尽量保持每个神经元的输入和输出方差一致。</p>
<p><strong>均匀分布初始化</strong> 若采用区间为$[-r, r]$的均匀分布来初始化$w_i^{(l)}$，则$r$的取值为$\sqrt{\frac{6}{M_{l-1} + M_l}}$。定义xavier_uniform_r，计算均匀分布$r$的值。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">xavier_uniform_r</span>(<span class="params">input_size, output_size</span>):</span><br><span class="line">    <span class="keyword">return</span> np.sqrt(<span class="number">6</span> / (input_size + output_size))</span><br></pre></td></tr></table></figure>
<p>定义网络每层神经元的数量，通过Xavier均匀分布初始化网络。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定激活函数</span></span><br><span class="line">activate_fn = paddle.nn.Tanh</span><br><span class="line"><span class="comment"># 指定参数初始化方式</span></span><br><span class="line">init_fn_name = <span class="string">&#x27;uniform&#x27;</span></span><br><span class="line"></span><br><span class="line">model = MLP(layers_size, init_fn_name, init_fn=xavier_uniform_r, act_fn=activate_fn)</span><br><span class="line">inputs = paddle.normal(shape=[<span class="number">1</span>, <span class="number">100</span>], std=<span class="number">0.1</span>)</span><br><span class="line">y = model(inputs)</span><br></pre></td></tr></table></figure>
<p>打印每层神经元输出的方差，观察每层的方差值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(model.z) - <span class="number">1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;layer %d: , %f&#x27;</span>%(i, model.z[i].numpy().var()))</span><br></pre></td></tr></table></figure>
<h4 id="7-4-2-3-观察模型训练收敛性"><a href="#7-4-2-3-观察模型训练收敛性" class="headerlink" title="7.4.2.3 观察模型训练收敛性"></a>7.4.2.3 观察模型训练收敛性</h4><p>为了进一步验证Xavier初始化的效果，我们在一个简单的二分类任务上来训练MLP模型，并观察模型收敛情况。</p>
<p><strong>构建数据集</strong> 这里使用在第3.1.1中定义的make_moons函数构建一个简单的二分类数据集。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> make_moons</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MoonsDataset</span>(io.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mode=<span class="string">&#x27;train&#x27;</span>, num_samples=<span class="number">300</span>, num_train=<span class="number">200</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MoonsDataset, self).__init__()</span><br><span class="line">        X, y = make_moons(n_samples=num_samples, shuffle=<span class="literal">True</span>, noise=<span class="number">0.5</span>)</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            self.X, self.y = X[:num_train], y[:num_train]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.X, self.y = X[num_train:], y[num_train:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.X[idx], self.y[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br></pre></td></tr></table></figure>
<p>创建训练和验证集，构建DataLoader。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">train_dataset = MoonsDataset(mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">dev_dataset = MoonsDataset(mode=<span class="string">&#x27;dev&#x27;</span>)</span><br><span class="line">train_loader = io.DataLoader(train_dataset, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">dev_loader = io.DataLoader(dev_dataset, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>定义五层MLP，分别以Xavier初始化和标准高斯分布初始化方式对网络进行初始化，训练100回合，对比两个模型的训练损失变化情况。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nndl</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络每层神经元的数量</span></span><br><span class="line">layers_size = [<span class="number">2</span>, <span class="number">300</span>, <span class="number">500</span>, <span class="number">700</span>, <span class="number">400</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 指定激活函数</span></span><br><span class="line">activate_fn = paddle.nn.Tanh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定参数初始化方式为Xavier高斯分布初始化</span></span><br><span class="line">init_fn_name = <span class="string">&#x27;normal&#x27;</span></span><br><span class="line">model1 = MLP(layers_size, init_fn_name, init_fn=xavier_normal_std, act_fn=activate_fn)</span><br><span class="line">opt1 = optimizer.SGD(learning_rate=<span class="number">0.005</span>, parameters=model1.parameters())</span><br><span class="line">loss_fn = F.binary_cross_entropy_with_logits</span><br><span class="line">m = nndl.Accuracy(is_logist=<span class="literal">True</span>)</span><br><span class="line">runner1 = RunnerV3(model1, opt1, loss_fn, m)</span><br><span class="line">runner1.train(train_loader, dev_loader, num_epochs=<span class="number">100</span>, eval_steps=<span class="number">400</span>, log_steps=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定参数初始化方式为N(0, 1)高斯分布初始化</span></span><br><span class="line">init_fn_name = <span class="string">&#x27;basic&#x27;</span></span><br><span class="line">model2 = MLP(layers_size, init_fn_name, <span class="literal">None</span>, act_fn=activate_fn)</span><br><span class="line">opt2 = optimizer.SGD(learning_rate=<span class="number">0.005</span>, parameters=model2.parameters())</span><br><span class="line">runner2 = RunnerV3(model2, opt2, loss_fn, m)</span><br><span class="line">runner2.train(train_loader, dev_loader, num_epochs=<span class="number">100</span>, eval_steps=<span class="number">400</span>, log_steps=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">plt.plot(runner1.train_epoch_losses, label=<span class="string">&#x27;xavier initializer&#x27;</span>, c=<span class="string">&#x27;#e4007f&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.plot(runner2.train_epoch_losses, label=<span class="string">&#x27;N(0,1) initializer&#x27;</span>, c=<span class="string">&#x27;#f19ec2&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.legend(fontsize=<span class="string">&#x27;large&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;opti-xavier.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>从输出结果看，使用Xavier初始化，模型的损失相对较小，模型效果更好。</p>
<p><strong>动手练习7.7</strong></p>
<p>尝试实现He初始化，并将上面MLP算子的激活函数改为ReLU，观察其效果。</p>
<h2 id="7-5-逐层规范化"><a href="#7-5-逐层规范化" class="headerlink" title="7.5 逐层规范化"></a>7.5 逐层规范化</h2><p>逐层规范化（Layer-wise Normalization）是将传统机器学习中的数据规范化方法应用到深度神经网络中，对神经网络中隐藏层的输入进行规范化，从而使得网络更容易训练。</p>
<p>在深度神经网络中，一个神经层的输入是之前神经层的输出。给定一个神经层$l$，它之前的神经层$(1, \cdots, l-1$)的参数变化会导致其输入的分布发生较大的改变。从机器学习角度来看，如果一个神经层的输入分布发生了改变，那么其参数需要重新学习，这种现象叫作内部协变量偏移（Internal Covariate Shift）。 为了缓解这个问题，我们可以对每一个神经层的输入进行规范化操作，使其分布保持稳定。</p>
<p>下面介绍两种比较常用的逐层规范化方法：批量规范化（Batch Normalization）和层规范化（Layer Normalization）。</p>
<h3 id="7-5-1-批量规范化"><a href="#7-5-1-批量规范化" class="headerlink" title="7.5.1 批量规范化"></a>7.5.1 批量规范化</h3><p>对于一个深度神经网络，为了提高优化效率，要使得第$l$层的净输入$\bm z^{(l)}$的分布一致，比如都规范化到标准正态分布。在实践中规范化操作一般应用在线性层和激活函数之间。而为了提高规范化效率，一般使用标准化将净输入$\bm z^{(l)}$的每一维都规范化到标准正态分布。</p>
<script type="math/tex; mode=display">
\hat{\bm z}^{(l)} = \frac{\bm z^{(l)} - \bm \mu_{\mathcal B}}{\sqrt{\bm \sigma_{\mathcal B}^2 + \epsilon}},</script><p>其中$\bm \mu_{\mathcal B}$、$\bm \sigma_{\mathcal B}^2$为小批量样本的均值和方差。</p>
<p>对净输入$\bm z^{(l)}$的标准规范化会使得其取值集中到0附近，如果使用Sigmoid型激活函数时，这个取值区间刚好是接近线性变换的区间，减弱了神经网络的非线性性质。因此，为了使得规范化不对网络的表示能力造成负面影响，可以通过一个附加的缩放和平移变换改变取值区间。则有：</p>
<script type="math/tex; mode=display">
\hat{\bm z}^{(l)} \triangleq BN_{\bm \gamma, v \beta}(\bm z^{(l)}) = \frac{\bm z^{(l)} - \bm \mu_{\mathcal B}}{\sqrt{\bm \sigma_{\mathcal B}^2 + \epsilon}} \odot \bm \gamma + \bm \beta.</script><h4 id="7-5-1-1-BatchNorm算子"><a href="#7-5-1-1-BatchNorm算子" class="headerlink" title="7.5.1.1 BatchNorm算子"></a>7.5.1.1 BatchNorm算子</h4><p>下面定义BatchNorm算子，实现批量规范化。在实现批量规范化时，在训练过程中的均值和方差可以动态计算，但在测试时需要保存固定，否则模型输出就会受到同一批次中其他样本的影响。因此，在训练时需要将每一批次样本的均值和方差以移动平均值的方式记录下来，预测时使用整个训练集上的均值和方差（也就是保存的移动平均值）进行规范化。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, eps=<span class="number">1e-7</span>, momentum=<span class="number">0.9</span>, gamma=<span class="number">1.0</span>, beta=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        批量规范化初始化</span></span><br><span class="line"><span class="string">        输入:</span></span><br><span class="line"><span class="string">            - num_features: 输入特征数</span></span><br><span class="line"><span class="string">            - eps: 保持数值稳定性而设置的常数</span></span><br><span class="line"><span class="string">            - momentum: 用于计算移动平均值</span></span><br><span class="line"><span class="string">            - gamma: 缩放的参数</span></span><br><span class="line"><span class="string">            - beta: 平移的参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(BatchNorm, self).__init__()</span><br><span class="line">        shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        self.gamma = paddle.to_tensor(gamma, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        self.beta = paddle.to_tensor(beta, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        self.moving_mean = paddle.zeros(shape)</span><br><span class="line">        self.moving_variance = paddle.ones(shape)</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.momentum = momentum</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, train_mode=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(X, train_mode)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, train_mode=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> train_mode:</span><br><span class="line">            X = (X - self.moving_mean) / paddle.sqrt(self.moving_variance + self.eps)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 对于Linear层</span></span><br><span class="line">                mean = paddle.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">                var = ((X - mean) ** <span class="number">2</span>).mean(axis=<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 对于卷积层</span></span><br><span class="line">                mean = paddle.mean(X, axis=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>], keepdim=<span class="literal">True</span>)</span><br><span class="line">                var = ((X - mean) ** <span class="number">2</span>).mean(axis=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>], keepdim=<span class="literal">True</span>)</span><br><span class="line">            X = (X - mean) / paddle.sqrt(var, self.eps)</span><br><span class="line">            <span class="comment"># 保存移动平均值</span></span><br><span class="line">            self.moving_mean = self.momentum * self.moving_mean + (<span class="number">1.</span> - self.momentum) * mean</span><br><span class="line">            self.moving_variance = self.momentum * self.moving_variance + (<span class="number">1.</span> - self.momentum) * var</span><br><span class="line">        y = self.gamma * X + self.beta</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h4 id="7-5-1-2-支持逐层规范化的MLP算子"><a href="#7-5-1-2-支持逐层规范化的MLP算子" class="headerlink" title="7.5.1.2 支持逐层规范化的MLP算子"></a>7.5.1.2 支持逐层规范化的MLP算子</h4><p>重新定义MLP算子，加入逐层规范化功能。初始化网络时新增三个参数：norm_name指定使用哪一种逐层规范化（默认为None）、gamma和beta为缩放和平移变换的参数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers_size, init_fn_name, init_fn, act_fn, norm_name=<span class="literal">None</span>, gamma=<span class="literal">None</span>, beta=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        多层网络初始化</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - layers_size: 每层神经元的数量</span></span><br><span class="line"><span class="string">            - init_fn_name: 网络中参数初始化方法</span></span><br><span class="line"><span class="string">            - init_fn: 计算高斯分布标准差或均匀分布r值</span></span><br><span class="line"><span class="string">            - act_fn: 激活函数</span></span><br><span class="line"><span class="string">            - norm_name: 使用哪一种逐层规范化</span></span><br><span class="line"><span class="string">            - gamma、beta: 缩放和平移变换的参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        self.linear = paddle.nn.Sequential()</span><br><span class="line">        self.normalization = &#123;&#125;</span><br><span class="line">        self.num_layers = <span class="built_in">len</span>(layers_size) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            input_size, output_size = layers_size[i], layers_size[i + <span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> init_fn_name == <span class="string">&#x27;normal&#x27;</span>:</span><br><span class="line">                <span class="comment"># Xavier高斯分布初始化，计算方差</span></span><br><span class="line">                self.linear.add_sublayer(<span class="built_in">str</span>(i), nn.Linear(input_size, output_size,</span><br><span class="line">                                           weight_attr=nn.initializer.Normal(mean=<span class="number">0</span>, std=init_fn(input_size, output_size))))</span><br><span class="line">            <span class="keyword">elif</span> init_fn_name == <span class="string">&#x27;uniform&#x27;</span>:</span><br><span class="line">                r = init_fn(input_size, output_size)</span><br><span class="line">                self.linear.add_sublayer(<span class="built_in">str</span>(i), nn.Linear(input_size, output_size, weight_attr=nn.initializer.Uniform(low=-r, high=r)))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.linear.add_sublayer(<span class="built_in">str</span>(i), nn.Linear(input_size, output_size, weight_attr=nn.initializer.Normal()))</span><br><span class="line">            <span class="comment"># 判断是否使用逐层规范化，以及使用哪一种逐层规范化</span></span><br><span class="line">            <span class="keyword">if</span> norm_name == <span class="string">&#x27;bn&#x27;</span>:</span><br><span class="line">                self.normalization[i] = BatchNorm(output_size, gamma=gamma[i], beta=beta[i])</span><br><span class="line">            <span class="keyword">elif</span> norm_name == <span class="string">&#x27;ln&#x27;</span>:</span><br><span class="line">             <span class="comment"># LayerNorm：对一个中间层的所有神经元进行规范化</span></span><br><span class="line">                self.normalization[i] = LayerNorm(gamma=gamma[i], beta=beta[i])</span><br><span class="line">        self.act_fn = act_fn()</span><br><span class="line">        self.norm_name = norm_name</span><br><span class="line">        self.z = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, train_mode=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(X, train_mode)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, train_mode=<span class="literal">True</span></span>):</span><br><span class="line">        y = X</span><br><span class="line">        <span class="keyword">for</span> num_layer <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            y = self.linear[num_layer](y)</span><br><span class="line">            <span class="keyword">if</span> num_layer != self.num_layers - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> self.norm_name == <span class="string">&#x27;bn&#x27;</span>:</span><br><span class="line">                    y = self.normalization[num_layer](y, train_mode)</span><br><span class="line">                <span class="keyword">elif</span> self.norm_name == <span class="string">&#x27;ln&#x27;</span>:</span><br><span class="line">                    y = self.normalization[num_layer](y)</span><br><span class="line">                <span class="comment"># 为了展示逐层规范化后的输出的均值和方差，使用z[num_layer]进行记录</span></span><br><span class="line">                self.z[num_layer] = y</span><br><span class="line">                y = self.act_fn(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<p>因为批量规范化是对一个中间层的单个神经元进行规范化操作，所以要求小批量样本的数量不能太小，否则难以计算单个神经元的统计信息。所以我们使用paddle.randn随机生成一组形状为(200, 100)的数据, 打印数据送入网络前的均值与标准差。再分别定义使用批量规范化和不使用批量规范化的五层线性网络，分别打印网络第四层的均值与标准差，对比结果。</p>
<h4 id="7-5-1-3-内部协变量偏移实验"><a href="#7-5-1-3-内部协变量偏移实验" class="headerlink" title="7.5.1.3 内部协变量偏移实验"></a>7.5.1.3 内部协变量偏移实验</h4><p>下面我们构建两个模型：model1不使用批量规范化，model2使用批量规范化，观察批量规范化是否可以缓解内部协变量偏移问题。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络每层神经元的数量</span></span><br><span class="line">layers_size = [<span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>, <span class="number">300</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">data = paddle.randn(shape=[<span class="number">200</span>, <span class="number">100</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;data mean: &#x27;</span>, data.numpy().mean())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;data std: &#x27;</span>, data.numpy().std())</span><br><span class="line"></span><br><span class="line">activate_fn = paddle.nn.Tanh</span><br><span class="line">model1 = MLP(layers_size, <span class="string">&#x27;basic&#x27;</span>, <span class="literal">None</span>, act_fn=activate_fn)</span><br><span class="line">output = model1(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;no batch normalization: &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;model output mean: &#x27;</span>, model1.z[<span class="number">3</span>].numpy().mean(axis=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;model output std:&#x27;</span>, model1.z[<span class="number">3</span>].numpy().std(axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">gamma = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">beta = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">model2 = MLP(layers_size, <span class="string">&#x27;basic&#x27;</span>, <span class="literal">None</span>, act_fn=activate_fn, norm_name=<span class="string">&#x27;bn&#x27;</span>, gamma=gamma, beta=beta)</span><br><span class="line">output = model2(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;with batch normalization: &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;model output mean: &#x27;</span>, model2.z[<span class="number">3</span>].numpy().mean(axis=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;model output std:&#x27;</span>, model2.z[<span class="number">3</span>].numpy().std(axis=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>从输出结果看，在经过多层网络后，网络输出的均值和标准差已经发生偏移。而当我们指定批量规范化的均值和标准差为0,1时，网络输出的均值和标准差就会变为0,1。</p>
<p>当我们指定$\bm \gamma$和$\bm \beta$时，网络输出的标准差和均值就变为$\bm \gamma$和$\bm \beta$的值。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">gamma = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br><span class="line">beta = [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">model3 = MLP(layers_size, <span class="string">&#x27;basic&#x27;</span>, <span class="literal">None</span>, act_fn=activate_fn, norm_name=<span class="string">&#x27;bn&#x27;</span>, gamma=gamma, beta=beta)</span><br><span class="line">output = model3(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;batch normalization with different gamma and beta for different layer: &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output means with bn 0: &#x27;</span>, model3.z[<span class="number">0</span>].numpy().mean())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output stds with bn 0: &#x27;</span>, model3.z[<span class="number">0</span>].numpy().std())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output means with bn 3: &#x27;</span>, model3.z[<span class="number">3</span>].numpy().mean())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output stds with bn 3: &#x27;</span>, model3.z[<span class="number">3</span>].numpy().std())</span><br></pre></td></tr></table></figure>
<h4 id="7-5-1-4-均值和方差的移动平均计算实验"><a href="#7-5-1-4-均值和方差的移动平均计算实验" class="headerlink" title="7.5.1.4 均值和方差的移动平均计算实验"></a>7.5.1.4 均值和方差的移动平均计算实验</h4><p>下面测试批量规范化中训练样本均值和方差的移动平均值计算。使网络前向迭代50个回合，这个前向计算并不涉及网络训练与梯度更新，只是模拟网络训练时批量规范化中训练样本的均值和方差用移动平均计算的过程。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    inputs = paddle.randn(shape=[<span class="number">200</span>, <span class="number">100</span>])</span><br><span class="line">    output = model3(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印批量规范化中训练样本均值和方差的移动平均值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;batch norm 3 moving mean: &#x27;</span>, model3.normalization[<span class="number">3</span>].moving_mean.numpy())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;batch norm 3 moving variance: &#x27;</span>, model3.normalization[<span class="number">3</span>].moving_variance.numpy())</span><br></pre></td></tr></table></figure>
<p>开启测试模式，使用训练集的移动平均值作为测试集批量规范化的均值和标准差。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">inputs_test = paddle.randn(shape=[<span class="number">5</span>, <span class="number">100</span>])</span><br><span class="line">output = model3(inputs_test, train_mode=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h4 id="7-5-1-5-在MNIST数据集上使用带批量规范化的卷积网络"><a href="#7-5-1-5-在MNIST数据集上使用带批量规范化的卷积网络" class="headerlink" title="7.5.1.5 在MNIST数据集上使用带批量规范化的卷积网络"></a>7.5.1.5 在MNIST数据集上使用带批量规范化的卷积网络</h4><p>批量规范化的提出是为了解决内部协方差偏移问题，但后来发现其主要优点是更平滑的优化地形，以及使梯度变得更加稳定，从而提高收敛速度。</p>
<p>为验证批量规范化的有效性，本节使用飞桨API快速搭建一个多层卷积神经网络。在MNIST数据集上，观察使用批量规范化的网络是否相对于没有使用批量规范化的网络收敛速度更快。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, Linear, BatchNorm2D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多层卷积神经网络实现</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiConvLayerNet</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, use_bn=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiConvLayerNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=<span class="number">1</span>, out_channels=<span class="number">20</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 定义汇聚层，窗口的大小为2，步长为2</span></span><br><span class="line">        self.max_pool1 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2</span></span><br><span class="line">        self.conv2 = Conv2D(in_channels=<span class="number">20</span>, out_channels=<span class="number">20</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 定义汇聚层，窗口的大小为2，步长为2</span></span><br><span class="line">        self.max_pool2 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 定义一层全连接层，输出维度是10</span></span><br><span class="line">        self.fc = Linear(<span class="number">980</span>, <span class="number">10</span>)</span><br><span class="line">        <span class="keyword">if</span> use_bn:</span><br><span class="line">            <span class="comment"># 定义批量规范化层</span></span><br><span class="line">            self.batch_norm1 = BatchNorm2D(num_features=<span class="number">20</span>)</span><br><span class="line">            self.batch_norm2 = BatchNorm2D(num_features=<span class="number">20</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义网络前向计算过程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batch_norm1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.max_pool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batch_norm2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.max_pool2(x)</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], <span class="number">980</span>])</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>实例化网络并进行训练。model1不使用批量规范化，model2使用批量规范化。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> Accuracy</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 确保从paddle.vision.datasets.MNIST中加载的图像数据是np.ndarray类型</span></span><br><span class="line">paddle.vision.image.set_image_backend(<span class="string">&#x27;cv2&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用MNIST数据集</span></span><br><span class="line">train_dataset = MNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=transform)</span><br><span class="line">train_loader = io.DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">dev_dataset = MNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=transform)</span><br><span class="line">dev_loader = io.DataLoader(train_dataset, batch_size=<span class="number">64</span>)</span><br><span class="line">model1 = MultiConvLayerNet(use_bn=<span class="literal">False</span>)</span><br><span class="line">opt1 = paddle.optimizer.Adam(learning_rate=<span class="number">0.01</span>, parameters=model1.parameters())</span><br><span class="line">loss_fn = F.cross_entropy</span><br><span class="line">metric = Accuracy()</span><br><span class="line">runner1 = RunnerV3(model1, opt1, loss_fn, metric)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train network without batch normalization&#x27;</span>)</span><br><span class="line">runner1.train(train_loader, dev_loader, num_epochs=<span class="number">5</span>, log_steps=<span class="number">0</span>, eval_steps=<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">model2 = MultiConvLayerNet(use_bn=<span class="literal">True</span>)</span><br><span class="line">opt2 = paddle.optimizer.Adam(learning_rate=<span class="number">0.01</span>, parameters=model2.parameters())</span><br><span class="line">runner2 = RunnerV3(model2, opt2, loss_fn, metric)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train network with batch normalization&#x27;</span>)</span><br><span class="line">runner2.train(train_loader, dev_loader, num_epochs=<span class="number">5</span>, log_steps=<span class="number">0</span>, eval_steps=<span class="number">300</span>)</span><br></pre></td></tr></table></figure>
<p>对比model1和model2在验证集上损失和准确率的变化情况。从输出结果看，使用批量规范化的网络收敛速度会更好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.plot(np.array(runner1.dev_losses)[:, <span class="number">1</span>], label=<span class="string">&#x27;no bn&#x27;</span>, c=<span class="string">&#x27;#e4007f&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">ax1.plot(np.array(runner2.dev_losses)[:, <span class="number">1</span>], label=<span class="string">&#x27;with bn&#x27;</span>, c=<span class="string">&#x27;#f19ec2&#x27;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">&#x27;step&#x27;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.legend(fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.plot(runner1.dev_scores, label=<span class="string">&#x27;no bn&#x27;</span>, c=<span class="string">&#x27;#e4007f&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">ax2.plot(runner2.dev_scores, label=<span class="string">&#x27;with bn&#x27;</span>, c=<span class="string">&#x27;#f19ec2&#x27;</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">&#x27;step&#x27;</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">plt.legend(fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;opti-acc.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="7-5-2-层规范化"><a href="#7-5-2-层规范化" class="headerlink" title="7.5.2 层规范化"></a>7.5.2 层规范化</h3><p>层规范化（Layer Normalization）和批量规范化是非常类似的方法，它们的区别在于批量规范化对中间层的单个神经元进行规范化操作，而层规范化对一个中间层的所有神经元进行规范化。</p>
<p>层规范化定义为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{\bm z}^{(l)} &= \frac{\bm z^{(l)} - \mu^{(l)}}{\sqrt{\sigma^{(l)^2} + \epsilon}} \odot \bm \gamma + \bm \beta,  \\
&\triangleq LN_{\bm \gamma, \bm \beta}(\bm z^{(l)}),
\end{aligned}</script><p>其中$\bm z^{(l)}$为第$l$层神经元的净输入， $\bm \gamma$和$\bm \beta$分别代表缩放和平移的参数向量，和$\bm z^{(l)}$维数相同。$\mu^{(l)}$和$\sigma^{(l)^2}$分别为$\bm z^{(l)}$的均值和方差。</p>
<p>根据上面的公式可以看出，对于$K$个样本的一个小批量合集$\bm z^{(l)} = [\bm z^{(1, l)}; …; \bm z^{(K, l)}]$，层规范化是对矩阵$\bm z^{(l)}$的每一列进行规范化，而批量规范化是对每一行进行规范化。一般而言，批量规范化是一种更好的选择。当小批量样本数量比较小时，可以选择层规范化。</p>
<h4 id="7-5-2-1-LayerNorm算子"><a href="#7-5-2-1-LayerNorm算子" class="headerlink" title="7.5.2.1 LayerNorm算子"></a>7.5.2.1 LayerNorm算子</h4><p>定义LayerNorm实现层规范化算子。与批量规范化不同，层规范化对每个样本的所有特征进行规范化。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, eps=<span class="number">1e-7</span>,  gamma=<span class="number">1.0</span>, beta=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        层规范化初始化</span></span><br><span class="line"><span class="string">        输入:</span></span><br><span class="line"><span class="string">            - eps: 保持数值稳定性而设置的常数</span></span><br><span class="line"><span class="string">            - gamma: 缩放的参数</span></span><br><span class="line"><span class="string">            - beta: 平移的参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(self.__class__.__name__)</span><br><span class="line">        self.gamma = paddle.to_tensor(gamma, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        self.beta = paddle.to_tensor(beta, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 层规范化对每个样本的每个特征进行规范化</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">4</span>:</span><br><span class="line">            mean = paddle.mean(X, axis=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(axis=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mean = paddle.mean(X, axis=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(axis=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        X = (X - mean) / paddle.sqrt(var, self.eps)</span><br><span class="line">        y = self.gamma * X + self.beta</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h4 id="7-5-2-2-层规范化的验证实验"><a href="#7-5-2-2-层规范化的验证实验" class="headerlink" title="7.5.2.2 层规范化的验证实验"></a>7.5.2.2 层规范化的验证实验</h4><p>随机初始化一组形状为（10，100）的数据，输入带有层规范化的前馈神经网络中，得到网络输出并打印输出的标准差和均值。指定$\bm \gamma$和$\bm \beta$，从输出结果看，网络输出的标准差和均值变为$\bm \gamma$和$\bm \beta$的值。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络每层神经元的数量</span></span><br><span class="line">layers_size = [<span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>, <span class="number">300</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">data = paddle.randn(shape=[<span class="number">10</span>, <span class="number">100</span>])</span><br><span class="line">activate_fn = paddle.nn.Tanh</span><br><span class="line">gamma = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br><span class="line">beta = [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">model = MLP(layers_size, <span class="string">&#x27;basic&#x27;</span>, <span class="literal">None</span>, act_fn=activate_fn, norm_name=<span class="string">&#x27;ln&#x27;</span>, gamma=gamma, beta=beta)</span><br><span class="line">output = model(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;layer normalization with different gamma and beta for different layer: &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output means with ln 0: &#x27;</span>, model.z[<span class="number">0</span>].numpy().mean(axis=-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output stds with ln 0: &#x27;</span>, model.z[<span class="number">0</span>].numpy().std(axis=-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output means with ln 1: &#x27;</span>, model.z[<span class="number">3</span>].numpy().mean(axis=-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output stds with ln 1: &#x27;</span>, model.z[<span class="number">3</span>].numpy().std(axis=-<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>因为层规范化是对每个样本的每个通道做规范化，不需要存储训练数据的均值和方差的移动平均值，所以这里不需要多轮迭代累计移动平均值再做测试。而随机生成测试数据经过带层规范化的神经网络和上述代码实现方式相同，这里不再重复展示。</p>
<p><strong>动手练习7.8</strong> </p>
<p>尝试在MNIST数据集上对比使用层规范化的网络与没有使用层规范化的网络在收敛速度上的区别。</p>
<h2 id="7-6-网络正则化方法"><a href="#7-6-网络正则化方法" class="headerlink" title="7.6 网络正则化方法"></a>7.6 网络正则化方法</h2><p>由于深度神经网络的复杂度比较高，并且拟合能力很强，很容易在训练集上产生过拟合，因此在训练深度神经网络时，也需要通过一定的正则化方法来改进网络的泛化能力。</p>
<p>正则化（Regularization）是一类通过限制模型复杂度，从而避免过拟合、提高泛化能力的方法，比如引入约束、增加先验、提前停止等。</p>
<p>为了展示不同正则化方法的实现方式和效果，本节构建一个小数据集和多层感知器来模拟一个过拟合的实验场景，并实现$\ell_2$正则化、权重衰减和暂退法，观察这些正则化方法是否可以缓解过拟合现象。</p>
<h3 id="7-6-1-数据集构建"><a href="#7-6-1-数据集构建" class="headerlink" title="7.6.1 数据集构建"></a>7.6.1 数据集构建</h3><p>首先使用数据集构建函数make_moons来构建一个小的数据集，生成300个样本，其中200个作为训练数据，100个作为测试数据。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采样300个样本</span></span><br><span class="line">n_samples = <span class="number">300</span></span><br><span class="line">num_train = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据make_moons生成二分类数据集</span></span><br><span class="line">data_X, data_y = make_moons(n_samples=n_samples, shuffle=<span class="literal">True</span>, noise=<span class="number">0.5</span>)</span><br><span class="line">X_train, y_train = data_X[:num_train], data_y[:num_train]</span><br><span class="line">X_test, y_test = data_X[num_train:], data_y[num_train:]</span><br><span class="line"></span><br><span class="line">y_train = y_train.reshape([-<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_test = y_test.reshape([-<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train dataset X shape: &#x27;</span>, X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train dataset y shape: &#x27;</span>, y_train.shape)</span><br><span class="line"><span class="built_in">print</span>(X_train[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<h3 id="7-6-2-模型构建"><a href="#7-6-2-模型构建" class="headerlink" title="7.6.2 模型构建"></a>7.6.2 模型构建</h3><p>为了更好地展示正则化方法的实现机理，本节使用本书自定义的Op类来构建一个全连接前馈网络（即多层感知器）MLP_3L。MLP_3L是一个三层感知器，使用ReLU激活函数，最后一层输出层为线性层，即输出对率。</p>
<p>首先，我们实现ReLU算子，然后复用第4.2.4.4节中定义的Linear算子，组建多层感知器MLP_3L。</p>
<h4 id="7-6-2-1-ReLU算子"><a href="#7-6-2-1-ReLU算子" class="headerlink" title="7.6.2.1 ReLU算子"></a>7.6.2.1 ReLU算子</h4><p>假设一批样本组成的矩阵$\bm Z \in \mathbb{R}^{N\times D}$，每一行表示一个样本，$N$为样本数，$D$为特征维度，ReLU激活函数的前向过程表示为</p>
<script type="math/tex; mode=display">
\bm A=\max(\bm Z,0)\in \mathbb{R}^{N\times D},</script><p>其中$\bm A$为经过ReLU函数后的活性值。</p>
<p>令$\delta_{\bm A}=\frac{\partial \mathcal{R}}{\partial \bm A}\in \mathbb{R}^{N\times D}$表示最终损失$\mathcal{R}$对ReLU算子输出$\bm A$的梯度，ReLU激活函数的反向过程可以写为</p>
<script type="math/tex; mode=display">
\delta_{\bm Z} =  \delta_{\bm A}\odot(\bm A>0) \in \mathbb{R}^{N\times D},</script><p>其中$\delta_{\bm Z}$为ReLU算子反向函数的输出。</p>
<p>下面实现的ReLU算子，并实现前向和反向的计算。由于ReLU函数中没有参数，这里不需要在backward()方法进一步计算该算子参数的梯度。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReLU</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.inputs = <span class="literal">None</span></span><br><span class="line">        self.outputs = <span class="literal">None</span></span><br><span class="line">        self.params = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        self.inputs = inputs</span><br><span class="line">        <span class="keyword">return</span> paddle.multiply(inputs, paddle.to_tensor(inputs &gt; <span class="number">0</span>, dtype=<span class="string">&#x27;float32&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, outputs_grads</span>):</span><br><span class="line">        <span class="comment">#计算ReLU激活函数对输入的导数</span></span><br><span class="line">        <span class="comment"># paddle.multiply是逐元素相乘算子</span></span><br><span class="line">        <span class="keyword">return</span> paddle.multiply(outputs_grads, paddle.to_tensor(self.inputs &gt; <span class="number">0</span>, dtype=<span class="string">&#x27;float32&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h4 id="7-6-2-2-自定义多层感知器"><a href="#7-6-2-2-自定义多层感知器" class="headerlink" title="7.6.2.2 自定义多层感知器"></a>7.6.2.2 自定义多层感知器</h4><p>这里，我们构建一个多层感知器MLP_3L。MLP_3L算子由三层线性网络构成，层与层间加入ReLU激活函数，最后一层输出层为线性层，即输出对率（logits）。复用Linear算子，结合ReLU算子，实现网络的前反向计算。初始化时将模型中每一层的参数$\bW$以标准正态分布的形式进行初始化，参数$\bm b$初始化为0。函数forward进行网络的前向计算，函数backward进行网络的反向计算，将网络中参数梯度保存下来，后续通过优化器进行梯度更新。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nndl.op <span class="keyword">as</span> op</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP_3L</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers_size</span>):</span><br><span class="line">        self.fc1 = op.Linear(layers_size[<span class="number">0</span>], layers_size[<span class="number">1</span>], name=<span class="string">&#x27;fc1&#x27;</span>)</span><br><span class="line">        <span class="comment"># ReLU激活函数</span></span><br><span class="line">        self.act_fn1 = ReLU()</span><br><span class="line">        self.fc2 = op.Linear(layers_size[<span class="number">1</span>], layers_size[<span class="number">2</span>], name=<span class="string">&#x27;fc2&#x27;</span>)</span><br><span class="line">        self.act_fn2 = ReLU()</span><br><span class="line">        self.fc3 = op.Linear(layers_size[<span class="number">2</span>], layers_size[<span class="number">3</span>], name=<span class="string">&#x27;fc3&#x27;</span>)</span><br><span class="line">        self.layers = [self.fc1, self.act_fn1, self.fc2, self.act_fn2, self.fc3]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        z1 = self.fc1(X)</span><br><span class="line">        a1 = self.act_fn1(z1)</span><br><span class="line">        z2 = self.fc2(a1)</span><br><span class="line">        a2 = self.act_fn2(z2)</span><br><span class="line">        z3 = self.fc3(a2)</span><br><span class="line">        <span class="keyword">return</span> z3</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, loss_grad_z3</span>):</span><br><span class="line">        loss_grad_a2 = self.fc3.backward(loss_grad_z3)</span><br><span class="line">        loss_grad_z2 = self.act_fn2.backward(loss_grad_a2)</span><br><span class="line">        loss_grad_a1 = self.fc2.backward(loss_grad_z2)</span><br><span class="line">        loss_grad_z1 = self.act_fn1.backward(loss_grad_a1)</span><br><span class="line">        loss_grad_inputs = self.fc1.backward(loss_grad_z1)</span><br></pre></td></tr></table></figure>
<h4 id="7-6-2-3-损失函数算子"><a href="#7-6-2-3-损失函数算子" class="headerlink" title="7.6.2.3 损失函数算子"></a>7.6.2.3 损失函数算子</h4><p>使用交叉熵函数作为损失函数。这里MLP_3L模型的输出是对率而不是概率，因此不能直接使用第4.2.3节实现的BinaryCrossEntropyLoss算子。我们这里对交叉熵函数进行完善，使其可以直接接收对率计算交叉熵。</p>
<p>对二分类交叉熵损失进行改写，令向量$\bm y\in \{0,1\}^N$表示$N$个样本的标签构成的向量，向量$\bm o\in \mathbb{R}^N$表示$N$个样本的模型输出的对率，二分类的交叉熵损失为</p>
<script type="math/tex; mode=display">
\mathcal{R}(\bm y,\bm o) = -\frac{1}{N}(\bm y^{T} \log \sigma(\bm o) + (1 - \bm y)^{T} \log(1-\sigma(\bm o))),</script><p>其中$\sigma$为Logistic函数。</p>
<p>二分类交叉熵损失函数的输入是神经网络的输出$\bm o$。最终的损失$\mathcal{R}$对$\bm o$的偏导数为:</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal R}{\partial \bm o} =  -\frac{1}{N}(\bm y-\sigma(\bm o)).</script><p>损失函数BinaryCrossEntropyWithLogits的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BinaryCrossEntropyWithLogits</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.predicts = <span class="literal">None</span></span><br><span class="line">        self.labels = <span class="literal">None</span></span><br><span class="line">        self.data_size = <span class="literal">None</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.logistic = op.Logistic()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, logits, labels</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(logits, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, logits, labels</span>):</span><br><span class="line">        self.predicts = self.logistic(logits)</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.data_size = self.predicts.shape[<span class="number">0</span>]</span><br><span class="line">        loss = -<span class="number">1.</span> / self.data_size * (paddle.matmul(self.labels.t(), paddle.log(self.predicts)) + paddle.matmul((<span class="number">1</span> - self.labels.t()), paddle.log(<span class="number">1</span> - self.predicts)))</span><br><span class="line">        loss = paddle.squeeze(loss, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self</span>):</span><br><span class="line">        inputs_grads = <span class="number">1.</span>/ self.data_size * (self.predicts - self.labels)</span><br><span class="line">        self.model.backward(inputs_grads)</span><br></pre></td></tr></table></figure>
<p>定义accuracy_logits函数，输入为logits和labels。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy_logits</span>(<span class="params">logits, labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    输入:</span></span><br><span class="line"><span class="string">        - logits: 预测值，二分类时，shape=[N, 1]，N为样本数量; 多分类时，shape=[N, C]，C为类别数量</span></span><br><span class="line"><span class="string">        - labels: 真实标签，shape=[N, 1]</span></span><br><span class="line"><span class="string">    输出:</span></span><br><span class="line"><span class="string">        - 准确率: shape=[1]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 判断是二分类任务还是多分类任务，preds.shape[1]=1时为二分类任务，preds.shape[1]&gt;1时为多分类任务</span></span><br><span class="line">    <span class="keyword">if</span> logits.shape[<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 二分类时，判断每个logits是否大于0，当大于0时类别为1，否则类别为0</span></span><br><span class="line">        <span class="comment">#使用&#x27;paddle.cast&#x27;将preds的数据类型转换为float32类型</span></span><br><span class="line">        preds = paddle.cast((logits &gt; <span class="number">0</span>), dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 多分类时，使用&#x27;paddle.argmax&#x27;计算最大元素索引作为类别</span></span><br><span class="line">        preds = paddle.argmax(logits, axis=<span class="number">1</span>, dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> paddle.mean(paddle.cast(paddle.equal(preds, labels), dtype=<span class="string">&#x27;float32&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h4 id="7-6-2-4-模型训练"><a href="#7-6-2-4-模型训练" class="headerlink" title="7.6.2.4 模型训练"></a>7.6.2.4 模型训练</h4><p>使用train_model函数指定训练集数据和测试集数据、网络、优化器、损失函数、训练迭代次数等参数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">X_train, y_train, X_test, y_test, model, optimizer, loss_fn, num_iters, *args</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练模型</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - X_train, y_train: 训练集数据</span></span><br><span class="line"><span class="string">        - X_test, y_test: 测试集数据</span></span><br><span class="line"><span class="string">        - model: 定义网络</span></span><br><span class="line"><span class="string">        - optimizer: 优化器</span></span><br><span class="line"><span class="string">        - loss_fn: 损失函数</span></span><br><span class="line"><span class="string">        - num_iters: 训练迭代次数</span></span><br><span class="line"><span class="string">        - args: 在dropout中指定模型为训练模式或评价模式</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="comment"># 前向计算</span></span><br><span class="line">        train_logits = model(X_train)</span><br><span class="line">        loss = loss_fn(train_logits, y_train)</span><br><span class="line">        <span class="comment"># 反向计算</span></span><br><span class="line">        loss_fn.backward()</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            losses.append(loss)</span><br><span class="line"></span><br><span class="line">    train_logits = model(X_train, *args)</span><br><span class="line">    acc_train = accuracy_logits(train_logits, y_train)</span><br><span class="line">    test_logits = model(X_test, *args)</span><br><span class="line">    acc_test = accuracy_logits(test_logits, y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;train accuracy:&#x27;</span>, acc_train.numpy())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;test accuracy:&#x27;</span>, acc_test.numpy())</span><br><span class="line">    <span class="keyword">return</span> losses</span><br></pre></td></tr></table></figure>
<p>复用第4.2.4.6节中的BatchGD定义梯度下降优化器。进行50 000次训练迭代，观察模型在训练集和测试集上的准确率。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl.op <span class="keyword">import</span> BatchGD</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">layers_size = [X_train.shape[<span class="number">1</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">model = MLP_3L(layers_size)</span><br><span class="line">opt = BatchGD(init_lr=<span class="number">0.2</span>, model=model)</span><br><span class="line">loss_fn = BinaryCrossEntropyWithLogits(model)</span><br><span class="line">losses = train_model(X_train, y_train, X_test, y_test, model, opt, loss_fn, <span class="number">50000</span>)</span><br></pre></td></tr></table></figure>
<p>从输出结果看，模型在训练集上的准确率为91%，在测试集上的准确率为71%，推断模型出现了过拟合现象。为了更好地观察模型，我们通过可视化分类界面来确认模型是否发生了过拟合。</p>
<p>可视化函数show_class_boundary的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_class_boundary</span>(<span class="params">model, X_train, y_train, *args, fig_name</span>):</span><br><span class="line">    <span class="comment">#均匀生成40 000个数据点</span></span><br><span class="line">    x1, x2 = paddle.meshgrid(paddle.linspace(-<span class="number">2</span>, <span class="number">3</span>, <span class="number">200</span>), paddle.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">200</span>))</span><br><span class="line">    x = paddle.stack([paddle.flatten(x1), paddle.flatten(x2)], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#预测对应类别</span></span><br><span class="line">    y = model(x, *args)</span><br><span class="line">    y = paddle.cast((y&gt;<span class="number">0</span>), dtype=<span class="string">&#x27;int32&#x27;</span>).squeeze()</span><br><span class="line"></span><br><span class="line">    bg_colors = [<span class="string">&#x27;#f5f5f5&#x27;</span> <span class="keyword">if</span> y==<span class="number">1</span> <span class="keyword">else</span> <span class="string">&#x27;#f19ec2&#x27;</span> <span class="keyword">for</span> y <span class="keyword">in</span> y]</span><br><span class="line">    label_colors = [<span class="string">&#x27;#000000&#x27;</span> <span class="keyword">if</span> train_label==<span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;#e4007f&#x27;</span> <span class="keyword">for</span> train_label <span class="keyword">in</span> y_train]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#绘制类别区域</span></span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br><span class="line">    plt.scatter(x[:, <span class="number">0</span>].numpy(), x[:, <span class="number">1</span>].numpy(), c=bg_colors)</span><br><span class="line">    plt.scatter(X_train[:, <span class="number">0</span>].numpy(), X_train[:, <span class="number">1</span>].numpy(), marker=<span class="string">&#x27;*&#x27;</span>, c=label_colors)</span><br><span class="line">    plt.savefig(fig_name)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_class_boundary(model, X_train, y_train, fig_name=<span class="string">&#x27;opti-regularization.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>图中两种颜色的点代表两种类别的分类标签，不同颜色的区域是模型学习到的两个分类区域。从输出结果看，交界处的点被极细致地进行了区域分割，说明模型存在过拟合现象。</p>
<h3 id="7-6-3-ℓ-1-和-ℓ-2-正则化"><a href="#7-6-3-ℓ-1-和-ℓ-2-正则化" class="headerlink" title="7.6.3 $ℓ_1$和$ℓ_2$正则化"></a>7.6.3 $ℓ_1$和$ℓ_2$正则化</h3><p>$\ell_1$和$\ell_2$正则化是机器学习中最常用的正则化方法，通过约束参数的$\ell_1$和$\ell_2$范数来减小模型在训练数据集上的过拟合现象。通过加入$\ell_1$和$\ell_2$正则化，优化问题可以写为</p>
<script type="math/tex; mode=display">
\theta^{*} = \mathop{\arg\min}\limits_{\theta} \frac{1}{B} \sum_{n=1}^{B} \mathcal{L}(y^{(n)}, f(\bm x^{(n)};\theta)) + \lambda \ell_p(\theta),</script><p>其中$\mathcal{L}(\cdot)$为损失函数，$B$为批量大小，$f(\cdot)$为待学习的神经网络，$\theta$为其参数，$\ell_p$为范数函数，$p$的取值通常为1,2代表$\ell_1$和$\ell_2$范数，$\lambda$为正则化系数。</p>
<p>下面通过实验来验证$\ell_2$正则化缓解过拟合的效果。在交叉熵损失基础上增加$\ell_2$正则化，相当于前向计算时，损失加上$\frac{1}{2}\|\theta\|^2$。而反向计算时，所有参数的梯度再额外加上$\lambda\theta$。</p>
<p>完善算子BinaryCrossEntropyWithLogits，使其支持带$\ell_2$正则化的损失函数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BinaryCrossEntropyWithLogits</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, lambd</span>):</span><br><span class="line">        self.predicts = <span class="literal">None</span></span><br><span class="line">        self.labels = <span class="literal">None</span></span><br><span class="line">        self.data_size = <span class="literal">None</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.logistic = op.Logistic()</span><br><span class="line">        self.lambd = lambd</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, logits, labels</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(logits, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, logits, labels</span>):</span><br><span class="line">        self.predicts = self.logistic(logits)</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.data_size = self.predicts.shape[<span class="number">0</span>]</span><br><span class="line">        loss = -<span class="number">1.</span> / self.data_size * (paddle.matmul(self.labels.t(), paddle.log(self.predicts)) + paddle.matmul((<span class="number">1</span> - self.labels.t()), paddle.log(<span class="number">1</span> - self.predicts)))</span><br><span class="line">        loss = paddle.squeeze(loss, axis=<span class="number">1</span>)</span><br><span class="line">        regularization_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.model.layers:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, op.Linear):</span><br><span class="line">                regularization_loss += paddle.<span class="built_in">sum</span>(paddle.square(layer.params[<span class="string">&#x27;W&#x27;</span>]))</span><br><span class="line">        loss += self.lambd * regularization_loss / (<span class="number">2</span> * self.data_size)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self</span>):</span><br><span class="line">        inputs_grads = <span class="number">1.</span>/ self.data_size * (self.predicts - self.labels)</span><br><span class="line">        self.model.backward(inputs_grads)</span><br><span class="line">        <span class="comment">#更新正则化项对应的梯度</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.model.layers:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, op.Linear) <span class="keyword">and</span> <span class="built_in">isinstance</span>(layer.grads, <span class="built_in">dict</span>):</span><br><span class="line">                layer.grads[<span class="string">&#x27;W&#x27;</span>] += self.lambd * layer.params[<span class="string">&#x27;W&#x27;</span>] / self.data_size</span><br></pre></td></tr></table></figure>
<p>重新训练网络，增加$\ell_2$正则化后再进行50 000迭代。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">model = MLP_3L(layers_size)</span><br><span class="line">opt = BatchGD(init_lr=<span class="number">0.2</span>, model=model)</span><br><span class="line">loss_fn = BinaryCrossEntropyWithLogits(model, lambd=<span class="number">0.7</span>)</span><br><span class="line">losses = train_model(X_train, y_train, X_test, y_test, model, opt, loss_fn, num_iters=<span class="number">50000</span>)</span><br></pre></td></tr></table></figure>
<p>从输出结果看，在训练集上的准确率为86%，测试集上的准确率为77%。从输出结果看，猜测过拟合现象得到缓解。</p>
<p>再通过可视化分类界面证实猜测结果，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_class_boundary(model, X_train, y_train, fig_name=<span class="string">&#x27;opti-regularization2.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>从输出结果看，过拟合现象有所缓解，说明$\ell_2$正则化可以缓解过拟合现象。</p>
<h3 id="7-6-4-权重衰减"><a href="#7-6-4-权重衰减" class="headerlink" title="7.6.4 权重衰减"></a>7.6.4 权重衰减</h3><p>权重衰减(Weight Decay)是一种有效的正则化方法，在每次参数更新时引入一个衰减系数。</p>
<script type="math/tex; mode=display">
\theta_t \leftarrow (1 - \beta)\theta_{t-1} - \alpha \mathbf g_t,</script><p>其中$\mathbf g_t$为第$t$步更新时的梯度，$\alpha$为学习率，$\beta$为权重衰减系数，一般取值比较小，比如0.0005。</p>
<p>完善BatchGD优化器，增加权重衰减系数。定义gradient_descent函数，在参数更新时增加衰减系数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchGD</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_lr, model, weight_decay</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        小批量梯度下降优化器初始化</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - init_lr: 初始学习率</span></span><br><span class="line"><span class="string">            - model：模型，model.params字典存储模型参数值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(BatchGD, self).__init__(init_lr=init_lr, model=model)</span><br><span class="line">        self.weight_decay = weight_decay</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">self, x, gradient_x, init_lr</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        梯度下降更新一次参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = (<span class="number">1</span> - self.weight_decay) * x - init_lr * gradient_x</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数更新</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - gradient：梯度字典，存储每个参数的梯度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.model.layers:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer.params, <span class="built_in">dict</span>):</span><br><span class="line">                <span class="keyword">for</span> key <span class="keyword">in</span> layer.params.keys():</span><br><span class="line">                    layer.params[key] = self.gradient_descent(layer.params[key], layer.grads[key], self.init_lr)</span><br></pre></td></tr></table></figure>
<p>设置权重衰减系数为0.001。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">model = MLP_3L(layers_size)</span><br><span class="line">opt = BatchGD(init_lr=<span class="number">0.2</span>, model=model, weight_decay=<span class="number">0.001</span>)</span><br><span class="line">loss_fn = BinaryCrossEntropyWithLogits(model, lambd=<span class="number">0</span>)</span><br><span class="line">losses = train_model(X_train, y_train, X_test, y_test, model, opt, loss_fn, num_iters=<span class="number">50000</span>)</span><br></pre></td></tr></table></figure>
<p>从输出结果看，训练集上的准确率为84.5%，测试集上的准确率为75%，猜测仍存在过拟合现象，但是现象得到缓解。</p>
<p>下面通过可视化分类界面证猜测试结果。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_class_boundary(model, X_train, y_train, fig_name=<span class="string">&#x27;opti-regularization3.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>从输出结果看，权重衰减也可以有效缓解过拟合现象</p>
<h3 id="7-6-5-暂退法"><a href="#7-6-5-暂退法" class="headerlink" title="7.6.5 暂退法"></a>7.6.5 暂退法</h3><p>当训练一个深度神经网络时，我们可以随机暂退一部分神经元（即置为0）来避免过拟合，这种方法称为暂退法(Dropout Method)。每次选择暂退的神经元是随机的，最简单的方法是设置一个固定的概率$p$，对每一个神经元都以概率$p$来判定要不要保留。</p>
<p>假设一批样本的某个神经层为$\bm X\in \mathbb{R}^{B\times D}$，其中$B$为批大小，$D$为该层神经元数量，引入一个掩码矩阵$\bm M \in \mathbb{R}^{B\times D}$，每个元素的值以$p$的概率置为0，$1-p$的概率置为1。</p>
<p>由于掩蔽某些神经元后，该神经层的活性值的分布会发生变化。而在测试阶段时不使用暂退，这会使得训练和测试两个阶段该层神经元的活性值的分布不一致，并对之后的神经层产生影响，发生协变量偏移现象。<br>因此，为了在使用暂退法时不改变活性值$\bm X$的方差，将暂退后保留的神经元活性值放大原来的$1/(1-p)$倍。这样可以保证下一个神经层的输入在训练和测试阶段的方差基本一致。</p>
<p>暂退函数$\mathrm{dropout}$定义为</p>
<script type="math/tex; mode=display">
\tilde{\bm X}=\mathrm{dropout}(\bm X) \triangleq
\begin{cases}
(\bm X \odot \bm M)/(1-p) & \text{当训练阶段时},  \\
\bm X & \text{当测试阶段时}.
\end{cases}</script><p><strong>提醒</strong></p>
<p>和《神经网络与深度学习》中公式(7.74)不同。两者都可以解决使用暂退法带来的协变量偏移问题，但本书的方法在实践中更常见。</p>
<p>在反向计算梯度时，令$\delta_{\tilde{\bm X}}=\frac{\partial \mathcal L}{\partial \tilde{\bm X}}$，<br>则有</p>
<script type="math/tex; mode=display">
\delta_{\bm X}  = \delta_{\tilde{\bm X}} \odot \bm M /(1-p).</script><p>这里可以看出，暂退神经元的梯度也为$0$。</p>
<h4 id="7-6-5-1-Dropout算子"><a href="#7-6-5-1-Dropout算子" class="headerlink" title="7.6.5.1 Dropout算子"></a>7.6.5.1 Dropout算子</h4><p>定义Dropout算子，实现前向和反向的计算。注意，Dropout需要区分训练和评价模型。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dropout</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_rate</span>):</span><br><span class="line">        self.mask = <span class="literal">None</span></span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># 生成一个丢弃掩码</span></span><br><span class="line">        mask = paddle.cast(paddle.rand(inputs.shape) &gt; self.drop_rate, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        self.mask = mask</span><br><span class="line">        <span class="comment"># 随机使一些神经元失效</span></span><br><span class="line">        inputs = paddle.multiply(inputs, mask)</span><br><span class="line">        <span class="comment"># 使输入的方差保持不变</span></span><br><span class="line">        inputs /= (<span class="number">1</span> - self.drop_rate)</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, outputs_grad</span>):</span><br><span class="line">        <span class="keyword">return</span> paddle.multiply(outputs_grad, self.mask) / (<span class="number">1</span> - self.drop_rate)</span><br></pre></td></tr></table></figure>
<p>定义MLP_3L_dropout模型，，实现带暂退法的网络前反向计算。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl.op <span class="keyword">import</span> MLP_3L</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP_3L_dropout</span>(<span class="title class_ inherited__">MLP_3L</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers_size, drop_rate</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP_3L_dropout, self).__init__(layers_size)</span><br><span class="line">        self.dropout1 = Dropout(drop_rate)</span><br><span class="line">        self.dropout2 = Dropout(drop_rate)</span><br><span class="line">        self.layers = [self.fc1, self.act_fn1, self.fc2, self.act_fn2, self.fc3]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, mode=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(X, mode)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, mode=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">        self.mode = mode</span><br><span class="line">        z1 = self.fc1(X)</span><br><span class="line">        a1 = self.act_fn1(z1)</span><br><span class="line">        <span class="keyword">if</span> self.mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            a1 = self.dropout1(a1)</span><br><span class="line">        z2 = self.fc2(a1)</span><br><span class="line">        a2 = self.act_fn2(z2)</span><br><span class="line">        <span class="keyword">if</span> self.mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            a2 = self.dropout2(a2)</span><br><span class="line">        z3 = self.fc3(a2)</span><br><span class="line">        <span class="keyword">return</span> z3</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, loss_grad_z3</span>):</span><br><span class="line">        loss_grad_a2 = self.fc3.backward(loss_grad_z3)</span><br><span class="line">        <span class="keyword">if</span> self.mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            loss_grad_a2 = self.dropout2.backward(loss_grad_a2)</span><br><span class="line">        loss_grad_z2 = self.act_fn2.backward(loss_grad_a2)</span><br><span class="line">        loss_grad_a1 = self.fc2.backward(loss_grad_z2)</span><br><span class="line">        <span class="keyword">if</span> self.mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            loss_grad_a1 = self.dropout1.backward(loss_grad_a1)</span><br><span class="line">        loss_grad_z1 = self.act_fn1.backward(loss_grad_a1)</span><br><span class="line">        loss_grad_inputs = self.fc1.backward(loss_grad_z1)</span><br></pre></td></tr></table></figure>
<p>设置丢弃概率为0.5。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">model = MLP_3L_dropout(layers_size, drop_rate=<span class="number">0.3</span>)</span><br><span class="line">opt = BatchGD(init_lr=<span class="number">0.2</span>, model=model, weight_decay=<span class="number">0</span>)</span><br><span class="line">loss_fn = BinaryCrossEntropyWithLogits(model, lambd=<span class="number">0</span>)</span><br><span class="line">losses = train_model(X_train, y_train, X_test, y_test, model, opt, loss_fn, <span class="number">50000</span>, <span class="string">&#x27;dev&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>从输出结果看，训练集上的准确率为85.5%，测试集上的准确率为76%，猜测仍存在过拟合现象，但是现象得到缓解。</p>
<p>通过可视化分类界面证实猜想结果。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_class_boundary(model, X_train, y_train, <span class="string">&#x27;dev&#x27;</span>, fig_name=<span class="string">&#x27;opti-regularization4.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>从输出结果看，暂退法可以有效缓解过拟合，但缓解效果不如正则化或权重衰减明显。</p>
<p><strong>动手练习7.9</strong></p>
<p>尝试修改正则化系数、权重衰减系数和暂退概率，观察如果三种参数过大会产生什么现象。</p>
<h2 id="7-7-小结"><a href="#7-7-小结" class="headerlink" title="7.7 小结"></a>7.7 小结</h2><p>本章通过动手实现不同的优化器和正则化方法来加深对神经网络优化和正则化的理解。</p>
<p>在网络优化方面，首先从影响神经网络优化的三个主要因素（批大小、学习率、梯度计算）进行实验比较来看它们对神经网络优化的影响。为了更好地可视化，我们还进行2D和3D的优化过程展示。除了上面的三个因素外，我们还动手实现了基于随机采样的参数初始化方法以及逐层规范化方法来进一步提高网络的优化效率。</p>
<p>在网络正则化方面，我们动手实现了$\ell_2$正则化、权重衰减以及暂退法，并展示了它们在缓解过拟合方面的效果。</p>
<div id="reference"></div>

<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] 邱锡鹏，神经网络与深度学习，机械工业出版社，<a target="_blank" rel="noopener" href="https://nndl.github.io/">https://nndl.github.io/</a>, 2020.<br>[2] Test functions for optimization. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Test_functions_for_optimization">https://en.wikipedia.org/wiki/Test_functions_for_optimization</a><br>[3] P. Goyal, P. Dolla ́r, R. B. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017.<br>[4] Duchi J, Hazan E, Singer Y, 2011. Adaptive subgradient methods for online learning and stochastic<br>optimization[J]. The Journal of Machine Learning Research, 12:2121-2159.<br>[5] Tieleman T, Hinton G, 2012. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude[Z].<br>[6] Kingma D, Ba J, 2015. Adam: A method for stochastic optimization[C]//Proceedings of Interna-<br>tional Conference on Learning Representations.<br>[7] Ioffe S, Szegedy C, 2015. Batch normalization: Accelerating deep network training by reducing inter-<br>nal covariate shift[C]//Proceedings of the 32nd International Conference on Machine Learning.<br>448-456.<br>[8] Ba L J, Kiros R, Hinton G E, 2016. Layer normalization[J/OL]. CoRR, abs/1607.06450. <a target="_blank" rel="noopener" href="http://arxiv">http://arxiv</a>. org/abs/1607.06450.<br>[9] Hanson S J, Pratt L Y, 1989. Comparing biases for minimal network construction with back-<br>propagation[C]//Advances in neural information processing systems. 177-185.</p>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><p>在实际训练中，我们通过调用飞桨API指定优化器，并通过设置优化器的参数weight_decay=paddle.regularizer.L2Decay(coeff)来实现l2权重衰减正则化。通过飞桨API重复上述实验，构建过拟合情况，实现l2权重衰减正则化，观察过拟合缓解情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP_3L_Paddle</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP_3L_Paddle, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(in_features, <span class="number">20</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">20</span>, <span class="number">3</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">3</span>, out_features)</span><br><span class="line">        self.act_fn = nn.ReLU()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        z1 = self.fc1(X)</span><br><span class="line">        a1 = self.act_fn(z1)</span><br><span class="line">        z2 = self.fc2(a1)</span><br><span class="line">        a2 = self.act_fn(z2)</span><br><span class="line">        z3 = self.fc3(a2)</span><br><span class="line">        <span class="keyword">return</span> z3</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model_paddle</span>(<span class="params">X_train, y_train, X_test, y_test, model, optimizer, loss_fn, num_iters</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练模型</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">        - X_train, y_train: 训练集数据</span></span><br><span class="line"><span class="string">        - X_test, y_test: 测试集数据</span></span><br><span class="line"><span class="string">        - model: 定义网络</span></span><br><span class="line"><span class="string">        - optimizer: 优化器</span></span><br><span class="line"><span class="string">        - loss_fn: 损失函数</span></span><br><span class="line"><span class="string">        - num_iters: 训练迭代次数</span></span><br><span class="line"><span class="string">        - args: 在dropout中指定模型为训练模式或评价模式</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="comment"># 前向计算</span></span><br><span class="line">        train_logits = model(X_train)</span><br><span class="line">        loss = loss_fn(train_logits, y_train)</span><br><span class="line">        <span class="comment"># 反向计算</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.clear_grad()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            losses.append(loss)</span><br><span class="line"></span><br><span class="line">    train_logits = model(X_train)</span><br><span class="line">    acc_train = accuracy_logits(train_logits, y_train)</span><br><span class="line">    test_logits = model(X_test)</span><br><span class="line">    acc_test = accuracy_logits(test_logits, y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;train accuracy:&#x27;</span>, acc_train.numpy())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;test accuracy:&#x27;</span>, acc_test.numpy())</span><br><span class="line">    <span class="keyword">return</span> losses</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">model = MLP_3L_Paddle(in_features=X_train.shape[<span class="number">1</span>], out_features=<span class="number">1</span>)</span><br><span class="line">opt = paddle.optimizer.SGD(learning_rate=<span class="number">0.2</span>, parameters=model.parameters())</span><br><span class="line">loss_fn = F.binary_cross_entropy_with_logits</span><br><span class="line">losses = train_model_paddle(X_train, y_train, X_test, y_test, model, opt, loss_fn, <span class="number">50000</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_class_boundary(model, X_train, y_train, fig_name=<span class="string">&#x27;opti-regularization4.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.regularizer <span class="keyword">as</span> regularizer</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">0</span>)</span><br><span class="line">model = MLP_3L_Paddle(in_features=X_train.shape[<span class="number">1</span>], out_features=<span class="number">1</span>)</span><br><span class="line">opt = paddle.optimizer.SGD(learning_rate=<span class="number">0.2</span>, parameters=model.parameters(), weight_decay=regularizer.L2Decay(coeff=<span class="number">0.01</span>))</span><br><span class="line">loss_fn = F.binary_cross_entropy_with_logits</span><br><span class="line">losses = train_model_paddle(X_train, y_train, X_test, y_test, model, opt, loss_fn, <span class="number">50000</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_class_boundary(model, X_train, y_train, fig_name=<span class="string">&#x27;opti-regularization5.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/08/12/nndl/chapter8A/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          第8章（上）：注意力机制的理论解读
        
      </div>
    </a>
  
  
    <a href="/2022/08/12/nndl/chapter6B/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">第6章（下）：基于双向LSTM实现文本分类</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






    <section id="comments" style="margin: 60px;">
      <div id="gitalk-container"></div>
      <script>
      var gitalk = new Gitalk({
          clientID: 'ecf230c60af3ed1123fa',
          clientSecret: '1e1daf2e6dd957a3f7d6b22b524387d14aafdb13',
          repo: 'hfut-zyw.github.io',
          owner: 'hfut-zyw',
          admin: ['hfut-zyw'],
          id: md5(location.pathname),      // Ensure uniqueness and length less than 50
          distractionFreeMode: false  // Facebook-like distraction free mode
        })
      gitalk.render('gitalk-container')
      </script>
      
    </section>

</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        <a href="https://space.bilibili.com/782159" target="_blank">访问我的哔哩哔哩</a> 
      </div>
        <div class="footer-right">
        &copy; 2022 LabmemNo.001
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: ,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":170,"height":340},"mobile":{"show":false},"log":false});</script></body>
</html>