<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第5章（上）：卷积神经网络理论解读 | Homepage | 张有为</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="第5章 卷积神经网络卷积神经网络（Convolutional Neural Network，CNN）是受生物学上感受野机制的启发而提出的。目前的卷积神经网络一般是由卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络，有三个结构上的特性：局部连接、权重共享以及汇聚。">
<meta property="og:type" content="article">
<meta property="og:title" content="第5章（上）：卷积神经网络理论解读">
<meta property="og:url" content="http://example.com/2022/08/12/nndl/chapter5A/index.html">
<meta property="og:site_name" content="Homepage | 张有为">
<meta property="og:description" content="第5章 卷积神经网络卷积神经网络（Convolutional Neural Network，CNN）是受生物学上感受野机制的启发而提出的。目前的卷积神经网络一般是由卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络，有三个结构上的特性：局部连接、权重共享以及汇聚。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/4e6c4b60ff9b4de98db14a30c6ef153c8035f39ab8f7421189a4b96bc811ed7e">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/1b057e53beac41e8b721c34ba04df3e8e1420668e90c4793944e9da0eb18a962">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/18faf1c2dfbb48659d2ccb72cd16e917f0af57b4640142ed8f98f9ceaa8b5ae8">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/1021536721524f4d8f4c1aefa89693c4b0fd388f21a347b583d413b3ac41241b">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/ac14916db81e40a48a25ab894d7a95e33fa0eece71d44a55af7bffab462fb7a7">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/67bdbc3a81e945f4a6e469a40db5d41ac7f5e47c801f4ec9b9f4950d3d908160">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/deae0e6e83ba48968b153a2cc549c5bc8fc86d78c224441fb2e5700f3fea76ac">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/c08f9a5599ae4f9d8eedb0131c54af859fcfaae003224bc6b247d60391b2b102">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/d4d1434e62154505a56259d09c9b6e072f8331830c2b4ffc97b5d7bd0181b075">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/7a75788ae8d54f22882590a0923f5dd28bca6e120b1a4431abe122980d06224f">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/d55501bb93914955b1c262a00201f8bcc6abf632085e48e3b37aac5f064a4e1c">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/53774cf008e941d3b93ce2b8731a1089fc64539bebd347e3a255c2d8fb5ec1a9">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/448b1f120a2e4890bae4589d213c0ab36f71b8d9d6204998a6cceddaabbe9ab5">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/cf14626bb6c54eb9840a7253975fdc995f165146c6324bdb8f9b943b96f57394">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/16cb9fe0d0704940a829be1722fd6273b85209f198d143efb221f760d5f87f64">
<meta property="article:published_time" content="2022-08-12T01:27:28.000Z">
<meta property="article:modified_time" content="2022-09-03T22:13:15.457Z">
<meta property="article:author" content="Pokemon Master">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ai-studio-static-online.cdn.bcebos.com/4e6c4b60ff9b4de98db14a30c6ef153c8035f39ab8f7421189a4b96bc811ed7e">
  
    <link rel="alternative" href="/atom.xml" title="Homepage | 张有为" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/icon.gif">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>


  
<meta name="generator" content="Hexo 6.2.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/face.png" class="js-avatar show" style="width: 100%;height: 100%;opacity: 1;">
		</a>

		<hgroup>
			<h1 class="header-author"><a href="/">Pokemon Master</a></h1>
		</hgroup>

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/2022/08/09/me/aboutme">个人简介</a></li>
				        
							<li><a href="/categories/notes">随写</a></li>
				        
							<li><a href="/categories/analysis">实变泛函</a></li>
				        
							<li><a href="/categories/opt">优化笔记</a></li>
				        
							<li><a href="/categories/pytorch">Pytorch</a></li>
				        
							<li><a href="/categories/nndl">nndl案例与实践</a></li>
				        
						</ul>
					</nav>
					<nav class="half-header-menu">
						<a class="hide">Home</a>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
					        
						</div>
						<!-- music -->
						
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/face.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/2022/08/09/me/aboutme">个人简介</a></li>
		        
					<li><a href="/categories/notes">随写</a></li>
		        
					<li><a href="/categories/analysis">实变泛函</a></li>
		        
					<li><a href="/categories/opt">优化笔记</a></li>
		        
					<li><a href="/categories/pytorch">Pytorch</a></li>
		        
					<li><a href="/categories/nndl">nndl案例与实践</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-nndl/chapter5A" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/08/12/nndl/chapter5A/" class="article-date">
  	<time datetime="2022-08-12T01:27:28.000Z" itemprop="datePublished">2022-08-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第5章（上）：卷积神经网络理论解读
      
          <span class="title-pop-out"></a>
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/nndl/">nndl</a>
	</div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第5章-卷积神经网络"><a href="#第5章-卷积神经网络" class="headerlink" title="第5章 卷积神经网络"></a>第5章 卷积神经网络</h1><p>卷积神经网络（Convolutional Neural Network，CNN）是受生物学上感受野机制的启发而提出的。目前的卷积神经网络一般是由卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络，有三个结构上的特性：局部连接、权重共享以及汇聚。<span id="more"></span>这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数更少。卷积神经网络主要应用在图像和视频分析的任务上，其准确率一般也远远超出了其他的神经网络模型。近年来卷积神经网络也广泛地应用到自然语言处理、推荐系统等领域。</p>
<p>在学习本章内容前，建议您先阅读《神经网络与深度学习》第5章：卷积神经网络的相关内容，关键知识点如 <strong>图5.1</strong> 所示，以便更好的理解和掌握书中的理论知识在实践中的应用方法。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/4e6c4b60ff9b4de98db14a30c6ef153c8035f39ab8f7421189a4b96bc811ed7e" width=500></center>



<p><br><center>图5.1：《神经网络与深度学习》关键知识点回顾</center></br></p>
<p>本实践基于 <strong>《神经网络与深度学习》第5章：卷积神经网络</strong> 相关内容进行设计，主要包含两部分：</p>
<ul>
<li><strong>模型解读</strong>：介绍卷积的原理、卷积神经网络的网络结构、残差连接的原理以及残差网络的网络结构，并使用简单卷积神经网络和残差网络，完成手写数字识别任务；</li>
<li><strong>案例与实践</strong>：基于残差网络ResNet18完成CIFAR-10图像分类任务。</li>
</ul>
<h2 id="5-1-卷积"><a href="#5-1-卷积" class="headerlink" title="5.1 卷积"></a>5.1 卷积</h2><p>考虑到使用全连接前馈网络来处理图像时，会出现如下问题：</p>
<ol>
<li><p><strong>模型参数过多，容易发生过拟合。</strong> 在全连接前馈网络中，隐藏层的每个神经元都要跟该层所有输入的神经元相连接。随着隐藏层神经元数量的增多，参数的规模也会急剧增加，导致整个神经网络的训练效率非常低，也很容易发生过拟合。</p>
</li>
<li><p><strong>难以提取图像中的局部不变性特征。</strong> 自然图像中的物体都具有局部不变性特征，比如尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈网络很难提取这些局部不变性特征。</p>
</li>
</ol>
<p>卷积神经网络有三个结构上的特性：局部连接、权重共享和汇聚。这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数也更少。因此，通常会使用卷积神经网络来处理图像信息。</p>
<p><strong>卷积</strong>是分析数学中的一种重要运算，常用于信号处理或图像处理任务。本节以二维卷积为例来进行实践。</p>
<h3 id="5-1-1-二维卷积运算"><a href="#5-1-1-二维卷积运算" class="headerlink" title="5.1.1 二维卷积运算"></a>5.1.1 二维卷积运算</h3><p>在机器学习和图像处理领域，卷积的主要功能是在一个图像（或特征图）上滑动一个卷积核，通过卷积操作得到一组新的特征。在计算卷积的过程中，需要进行卷积核的翻转，而这也会带来一些不必要的操作和开销。因此，在具体实现上，一般会以数学中的<strong>互相关</strong>（Cross-Correlatio）运算来代替卷积。<br>在神经网络中，卷积运算的主要作用是抽取特征，卷积核是否进行翻转并不会影响其特征抽取的能力。特别是当卷积核是可学习的参数时，卷积和互相关在能力上是等价的。因此，很多时候，为方便起见，会直接用互相关来代替卷积。</p>
<hr>
<p><strong>说明：</strong></p>
<p>在本案例之后的描述中，除非特别声明，卷积一般指“互相关”。</p>
<hr>
<p>对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个滤波器$\mathbf W \in\Bbb{R}^{U\times V}$，它们的卷积为</p>
<script type="math/tex; mode=display">y_{i,j}=\sum_{u=0}^{U-1} \sum_{v=0}^{V-1} w_{uv}x_{i+u,j+v}。（5.1）</script><hr>
<p><strong>说明：</strong></p>
<p>这里和《神经网络与深度学习》中的定义区别是矩阵的下标从0开始。</p>
<hr>
<p><strong>图5.2</strong> 给出了卷积计算的示例。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/1b057e53beac41e8b721c34ba04df3e8e1420668e90c4793944e9da0eb18a962" width = "700"></center>
<center><br>图5.2：卷积操作的计算过程</br></center>

<p>经过卷积运算后，最终输出矩阵大小则为</p>
<script type="math/tex; mode=display">M' = M - U + 1,（5.2）</script><script type="math/tex; mode=display">N' = N - V + 1.（5.3）</script><p>可以发现，使用卷积处理图像，会有以下两个特性：</p>
<ol>
<li>在卷积层(假设是第$l$层)中的每一个神经元都只和前一层(第$l-1$层)中某个局部窗口内的神经元相连，构成一个局部连接网络，这也就是卷积神经网络的<strong>局部连接</strong>特性。</li>
<li>由于卷积的主要功能是在一个图像（或特征图）上滑动一个卷积核，所以作为参数的卷积核$\mathbf W \in\Bbb{R}^{U\times V}$对于第$l$层的所有的神经元都是相同的，这也就是卷积神经网络的<strong>权重共享</strong>特性。</li>
</ol>
<h3 id="5-1-2-二维卷积算子"><a href="#5-1-2-二维卷积算子" class="headerlink" title="5.1.2 二维卷积算子"></a>5.1.2 二维卷积算子</h3><hr>
<p>在本书后面的实现中，算子都继承<code>paddle.nn.Layer</code>，并使用支持反向传播的飞桨API进行实现，这样我们就可以不用手工写<code>backword()</code>的代码实现。</p>
<hr>
<p>根据公式（5.1），我们首先实现一个简单的二维卷积算子，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size, </span></span><br><span class="line"><span class="params">                    weight_attr=paddle.ParamAttr(<span class="params">initializer=nn.initializer.Assign(<span class="params">value=[[<span class="number">0.</span>, <span class="number">1.</span>],[<span class="number">2.</span>, <span class="number">3.</span>]]</span>)</span>)</span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv2D, self).__init__()</span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.create_parameter&#x27;创建卷积核</span></span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.ParamAttr&#x27;进行参数初始化</span></span><br><span class="line">        self.weight = paddle.create_parameter(shape=[kernel_size,kernel_size],</span><br><span class="line">                                                dtype=<span class="string">&#x27;float32&#x27;</span>,</span><br><span class="line">                                                attr=weight_attr)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - X：输入矩阵，shape=[B, M, N]，B为样本数量</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - output：输出矩阵</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        u, v = self.weight.shape</span><br><span class="line">        output = paddle.zeros([X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>] - u + <span class="number">1</span>, X.shape[<span class="number">2</span>] - v + <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(output.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(output.shape[<span class="number">2</span>]):</span><br><span class="line">                output[:, i, j] = paddle.<span class="built_in">sum</span>(X[:, i:i+u, j:j+v]*self.weight, axis=[<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机构造一个二维输入矩阵</span></span><br><span class="line">paddle.seed(<span class="number">100</span>)</span><br><span class="line">inputs = paddle.to_tensor([[[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>],[<span class="number">7.</span>,<span class="number">8.</span>,<span class="number">9.</span>]]])</span><br><span class="line"></span><br><span class="line">conv2d = Conv2D(kernel_size=<span class="number">2</span>)</span><br><span class="line">outputs = conv2d(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input: &#123;&#125;, \noutput: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(inputs, outputs))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="5-1-3-二维卷积的参数量和计算量"><a href="#5-1-3-二维卷积的参数量和计算量" class="headerlink" title="5.1.3 二维卷积的参数量和计算量"></a>5.1.3 二维卷积的参数量和计算量</h3><p><strong>参数量</strong></p>
<p>由于二维卷积的运算方式为在一个图像（或特征图）上滑动一个卷积核，通过卷积操作得到一组新的特征。所以参数量仅仅与卷积核的尺寸有关，对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个滤波器$\mathbf W \in\Bbb{R}^{U\times V}$，卷积核的参数量为$U\times V$。</p>
<p>假设有一幅大小为$32\times 32$的图像，如果使用全连接前馈网络进行处理，即便第一个隐藏层神经元个数为1，此时该层的参数量也高达$1025$个，此时该层的计算过程如 <strong>图5.3</strong> 所示。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/18faf1c2dfbb48659d2ccb72cd16e917f0af57b4640142ed8f98f9ceaa8b5ae8" width = "800"></center>
<center><br>图5.3：使用全连接前馈网络处理图像数据的计算过程 </br></center>

<p><br></br></p>
<p>可以想像，随着隐藏层神经元数量的变多以及层数的加深，使用全连接前馈网络处理图像数据时，参数量会急剧增加。</p>
<p>如果使用卷积进行图像处理，当卷积核为$3\times 3$时，参数量仅为$9$，相较于全连接前馈网络，参数量少了非常多。</p>
<p><strong>计算量</strong></p>
<p>在卷积神经网络中运算时，通常会统计网络总的乘加运算次数作为计算量（FLOPs，floating point of operations），来衡量整个网络的运算速度。对于单个二维卷积，计算量的统计方式为：</p>
<script type="math/tex; mode=display">FLOPs=M'\times N'\times U\times V。（5.4）</script><p>其中$M’\times N’$表示输出特征图的尺寸，即输出特征图上每个点都要与卷积核$\mathbf W \in\Bbb{R}^{U\times V}$进行$U\times V$次乘加运算。对于一幅大小为$32\times 32$的图像，使用$3\times 3$的卷积核进行运算可以得到以下的输出特征图尺寸：</p>
<script type="math/tex; mode=display">M' = M - U + 1 = 30</script><script type="math/tex; mode=display">N' = N - V + 1 = 30</script><p>此时，计算量为：</p>
<script type="math/tex; mode=display">FLOPs=M'\times N'\times U\times V=30\times 30\times 3\times 3=8100</script><h3 id="5-1-4-感受野"><a href="#5-1-4-感受野" class="headerlink" title="5.1.4 感受野"></a>5.1.4 感受野</h3><p>输出特征图上每个点的数值，是由输入图片上大小为$U\times V$的区域的元素与卷积核每个元素相乘再相加得到的，所以输入图像上$U\times V$区域内每个元素数值的改变，都会影响输出点的像素值。我们将这个区域叫做输出特征图上对应点的感受野。感受野内每个元素数值的变动，都会影响输出点的数值变化。比如$3\times3$卷积对应的感受野大小就是$3\times3$，如 <strong>图5.4</strong> 所示。</p>
<p><br></br></p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/1021536721524f4d8f4c1aefa89693c4b0fd388f21a347b583d413b3ac41241b" width = "800"></center>
<center><br>图5.4：感受野为3×3的卷积 </br></center>

<p><br></br></p>
<p>而当通过两层$3\times3$的卷积之后，感受野的大小将会增加到$5\times5$，如 <strong>图5.5</strong> 所示。</p>
<p><br></br></p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/ac14916db81e40a48a25ab894d7a95e33fa0eece71d44a55af7bffab462fb7a7" width = "800"></center>
<center><br>图5.5：感受野为5×5的卷积 </br></center>

<p><br></br></p>
<p>因此，当增加卷积网络深度的同时，感受野将会增大，输出特征图中的一个像素点将会包含更多的图像语义信息。</p>
<h3 id="5-1-5-卷积的变种"><a href="#5-1-5-卷积的变种" class="headerlink" title="5.1.5 卷积的变种"></a>5.1.5 卷积的变种</h3><p>在卷积的标准定义基础上，还可以引入卷积核的滑动步长和零填充来增加卷积的多样性，从而更灵活地进行特征抽取。</p>
<h4 id="5-1-5-1-步长（Stride）"><a href="#5-1-5-1-步长（Stride）" class="headerlink" title="5.1.5.1 步长（Stride）"></a>5.1.5.1 步长（Stride）</h4><p>在卷积运算的过程中，有时会希望跳过一些位置来降低计算的开销，也可以把这一过程看作是对标准卷积运算输出的<strong>下采样</strong>。</p>
<p>在计算卷积时，可以在所有维度上每间隔$S$个元素计算一次，$S$称为卷积运算的<strong>步长</strong>（Stride），也就是卷积核在滑动时的间隔。</p>
<p>此时，对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个滤波器$\mathbf W \in\Bbb{R}^{U\times V}$，它们的卷积为</p>
<script type="math/tex; mode=display">y_{i,j}=\sum_{u=0}^{U-1} \sum_{v=0}^{V-1} w_{uv}x_{i\times S+u,j\times S+v}，（5.5）</script><p>在二维卷积运算中，当步长$S=2$时，计算过程如 <strong>图5.6</strong> 所示。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/67bdbc3a81e945f4a6e469a40db5d41ac7f5e47c801f4ec9b9f4950d3d908160" width = "800"></center>
<center><br>图5.6：步长为2的二维卷积计算过程 </br></center>

<p><br></br></p>
<h4 id="5-1-5-2-零填充（Zero-Padding）"><a href="#5-1-5-2-零填充（Zero-Padding）" class="headerlink" title="5.1.5.2 零填充（Zero Padding）"></a>5.1.5.2 零填充（Zero Padding）</h4><p>在卷积运算中，还可以对输入用零进行填充使得其尺寸变大。根据卷积的定义，如果不进行填充，当卷积核尺寸大于1时，输出特征会缩减。对输入进行零填充则可以对卷积核的宽度和输出的大小进行独立的控制。</p>
<p>在二维卷积运算中，<strong>零填充</strong>（Zero Padding）是指在输入矩阵周围对称地补上$P$个$0$。<strong>图5.7</strong> 为使用零填充的示例。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/deae0e6e83ba48968b153a2cc549c5bc8fc86d78c224441fb2e5700f3fea76ac" width = "200"></center>
<center><br>图5.7：padding=1的零填充 </br></center>

<p><br></br></p>
<p>对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个滤波器$\mathbf W \in\Bbb{R}^{U\times V}$，，步长为$S$，对输入矩阵进行零填充，那么最终输出矩阵大小则为</p>
<script type="math/tex; mode=display">M' = \frac{M + 2P - U}{S} + 1,（5.6）</script><script type="math/tex; mode=display">N' = \frac{N + 2P - V}{S} + 1.（5.7）</script><p>引入步长和零填充后的卷积，参数量和计算量的统计方式与之前一致，参数量与卷积核的尺寸有关，为：$U\times V$，计算量与输出特征图和卷积核的尺寸有关，为：</p>
<script type="math/tex; mode=display">FLOPs=M'\times N'\times U\times V=(\frac{M + 2P - U}{S} + 1)\times (\frac{N + 2P - V}{S} + 1)\times U\times V。（5.8）</script><p>一般常用的卷积有以下三类：</p>
<ol>
<li><strong>窄卷积</strong>：步长$S=1$，两端不补零$P=0$，卷积后输出尺寸为：</li>
</ol>
<script type="math/tex; mode=display">M' = M - U + 1,（5.9）</script><script type="math/tex; mode=display">N' = N - V + 1.（5.10）</script><ol>
<li><strong>宽卷积</strong>：步长$S=1$，两端补零$P=U-1=V-1$，卷积后输出尺寸为：</li>
</ol>
<script type="math/tex; mode=display">M' = M + U - 1,（5.11）</script><script type="math/tex; mode=display">N' = N + V - 1.（5.12）</script><ol>
<li><strong>等宽卷积</strong>：步长$S=1$，两端补零$P=\frac{(U-1)}{2}=\frac{(V-1)}{2}$，卷积后输出尺寸为：</li>
</ol>
<script type="math/tex; mode=display">M' = M,（5.13）</script><script type="math/tex; mode=display">N' = N.（5.14）</script><p>通常情况下，在层数较深的卷积神经网络，比如：VGG、ResNet中，会使用等宽卷积保证输出特征图的大小不会随着层数的变深而快速缩减。例如：当卷积核的大小为$3\times 3$时，会将步长设置为$S=1$，两端补零$P=1$，此时，卷积后的输出尺寸就可以保持不变。在本章后续的案例中，会使用ResNet进行实验。</p>
<h3 id="5-1-6-带步长和零填充的二维卷积算子"><a href="#5-1-6-带步长和零填充的二维卷积算子" class="headerlink" title="5.1.6 带步长和零填充的二维卷积算子"></a>5.1.6 带步长和零填充的二维卷积算子</h3><p>引入步长和零填充后，二维卷积算子代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, </span></span><br><span class="line"><span class="params">                    weight_attr=paddle.ParamAttr(<span class="params">initializer=nn.initializer.Constant(<span class="params">value=<span class="number">1.0</span></span>)</span>)</span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv2D, self).__init__()</span><br><span class="line">        self.weight = paddle.create_parameter(shape=[kernel_size,kernel_size], </span><br><span class="line">                                                dtype=<span class="string">&#x27;float32&#x27;</span>, </span><br><span class="line">                                                attr=weight_attr)</span><br><span class="line">        <span class="comment"># 步长</span></span><br><span class="line">        self.stride = stride</span><br><span class="line">        <span class="comment"># 零填充</span></span><br><span class="line">        self.padding = padding</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 零填充</span></span><br><span class="line">        new_X = paddle.zeros([X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]+<span class="number">2</span>*self.padding, X.shape[<span class="number">2</span>]+<span class="number">2</span>*self.padding])</span><br><span class="line">        new_X[:, self.padding:X.shape[<span class="number">1</span>]+self.padding, self.padding:X.shape[<span class="number">2</span>]+self.padding] = X</span><br><span class="line">        u, v = self.weight.shape</span><br><span class="line">        output_w = (new_X.shape[<span class="number">1</span>] - u) // self.stride + <span class="number">1</span></span><br><span class="line">        output_h = (new_X.shape[<span class="number">2</span>] - v) // self.stride + <span class="number">1</span></span><br><span class="line">        output = paddle.zeros([X.shape[<span class="number">0</span>], output_w, output_h])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, output.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, output.shape[<span class="number">2</span>]):</span><br><span class="line">                output[:, i, j] = paddle.<span class="built_in">sum</span>(</span><br><span class="line">                    new_X[:, self.stride*i:self.stride*i+u, self.stride*j:self.stride*j+v]*self.weight,</span><br><span class="line">                    axis=[<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">inputs = paddle.randn(shape=[<span class="number">2</span>, <span class="number">8</span>, <span class="number">8</span>])</span><br><span class="line">conv2d_padding = Conv2D(kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">outputs = conv2d_padding(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;When kernel_size=3, padding=1 stride=1, input&#x27;s shape: &#123;&#125;, output&#x27;s shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(inputs.shape, outputs.shape))</span><br><span class="line">conv2d_stride = Conv2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">outputs = conv2d_stride(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;When kernel_size=3, padding=1 stride=2, input&#x27;s shape: &#123;&#125;, output&#x27;s shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(inputs.shape, outputs.shape))</span><br></pre></td></tr></table></figure>
<p>从输出结果看出，使用$3\times3$大小卷积，<code>padding</code>为1，当<code>stride</code>=1时，模型的输出特征图可以与输入特征图保持一致；当<code>stride</code>=2时，输出特征图的宽和高都缩小一倍。</p>
<h3 id="5-1-7-使用卷积运算完成图像边缘检测任务"><a href="#5-1-7-使用卷积运算完成图像边缘检测任务" class="headerlink" title="5.1.7 使用卷积运算完成图像边缘检测任务"></a>5.1.7 使用卷积运算完成图像边缘检测任务</h3><p>在图像处理任务中，常用<strong>拉普拉斯算子</strong>对物体边缘进行提取，拉普拉斯算子为一个大小为$3 \times 3$的卷积核，中心元素值是$8$，其余元素值是$-1$。</p>
<hr>
<p>考虑到边缘其实就是图像上像素值变化很大的点的集合，因此可以通过计算二阶微分得到，当二阶微分为0时，像素值的变化最大。此时，对$x$方向和$y$方向分别求取二阶导数：</p>
<script type="math/tex; mode=display">\frac{\delta^2 I}{\delta x^2} = I(i, j+1) - 2I(i,j) + I(i,j-1),（5.15）</script><script type="math/tex; mode=display">\frac{\delta^2 I}{\delta y^2} = I(i+1, j) - 2I(i,j) + I(i-1,j).（5.16）</script><p>完整的二阶微分公式为：</p>
<script type="math/tex; mode=display">\nabla^2I = \frac{\delta^2 I}{\delta x^2} + \frac{\delta^2 I}{\delta y^2} =  - 4I(i,j) + I(i,j-1) + I(i, j+1) + I(i+1, j) + I(i-1,j),（5.17）</script><p>上述公式也被称为<strong>拉普拉斯算子</strong>，对应的二阶微分卷积核为：</p>
<script type="math/tex; mode=display">\begin{bmatrix}
0      & 1   &  0    \\
1      & -4 &  1 \\
0      &  1 &  0 \\
\end{bmatrix}</script><p>对上述算子全部求反也可以起到相同的作用，此时，该算子可以表示为：</p>
<script type="math/tex; mode=display">\begin{bmatrix}
0      & -1   &  0    \\
-1      & 4 &  -1 \\
0      &  -1 &  0 \\
\end{bmatrix}</script><p>也就是一个点的四邻域拉普拉斯的算子计算结果是自己像素值的四倍减去上下左右的像素的和，将这个算子旋转$45°$后与原算子相加，就变成八邻域的拉普拉斯算子，也就是一个像素自己值的八倍减去周围一圈八个像素值的和，做为拉普拉斯计算结果，此时，该算子可以表示为：</p>
<script type="math/tex; mode=display">\begin{bmatrix}
-1      & -1   &  -1    \\
-1      & 8 &  -1 \\
-1      &  -1 &  -1 \\
\end{bmatrix}</script><hr>
<p>下面我们利用上面定义的<code>Conv2D</code>算子，构造一个简单的拉普拉斯算子，并对一张输入的灰度图片进行边缘检测，提取出目标的外形轮廓。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;./number.jpg&#x27;</span>).resize((<span class="number">256</span>,<span class="number">256</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置卷积核参数</span></span><br><span class="line">w = np.array([[-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>], [-<span class="number">1</span>,<span class="number">8</span>,-<span class="number">1</span>], [-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"><span class="comment"># 创建卷积算子，卷积核大小为3x3，并使用上面的设置好的数值作为卷积核权重的初始化参数</span></span><br><span class="line">conv = Conv2D(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, </span><br><span class="line">                weight_attr=paddle.ParamAttr(initializer=nn.initializer.Assign(value=w)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将读入的图片转化为float32类型的numpy.ndarray</span></span><br><span class="line">inputs = np.array(img).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bf to_tensor, inputs:&quot;</span>,inputs)</span><br><span class="line"><span class="comment"># 将图片转为Tensor</span></span><br><span class="line">inputs = paddle.to_tensor(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bf unsqueeze, inputs:&quot;</span>,inputs)</span><br><span class="line">inputs = paddle.unsqueeze(inputs, axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;af unsqueeze, inputs:&quot;</span>,inputs)</span><br><span class="line">outputs = conv(inputs)</span><br><span class="line">outputs = outputs.numpy()</span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">f = plt.subplot(<span class="number">121</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;input image&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">f = plt.subplot(<span class="number">122</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;output feature map&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.imshow(outputs.squeeze(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;conv-vis.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>从输出结果看，使用拉普拉斯算子，目标的边缘可以成功被检测出来。</p>
<h2 id="5-2-卷积神经网络的基础算子"><a href="#5-2-卷积神经网络的基础算子" class="headerlink" title="5.2 卷积神经网络的基础算子"></a>5.2 卷积神经网络的基础算子</h2><p>卷积神经网络是目前计算机视觉中使用最普遍的模型结构，如<strong>图5.8</strong> 所示，由$M$个卷积层和$b$个汇聚层组合作用在输入图片上，在网络的最后通常会加入$K$个全连接层。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/c08f9a5599ae4f9d8eedb0131c54af859fcfaae003224bc6b247d60391b2b102" width = "1000"></center>
<center><br>图5.8：卷积神经网络经典结构</br></center>

<p>从上图可以看出，卷积网络是由多个基础的算子组合而成。下面我们先实现卷积网络的两个基础算子：卷积层算子和汇聚层算子。</p>
<h3 id="5-2-1-卷积算子"><a href="#5-2-1-卷积算子" class="headerlink" title="5.2.1 卷积算子"></a>5.2.1 卷积算子</h3><p>卷积层是指用卷积操作来实现神经网络中一层。为了提取不同种类的特征，通常会使用多个卷积核一起进行特征提取。</p>
<h4 id="5-2-1-1-多通道卷积"><a href="#5-2-1-1-多通道卷积" class="headerlink" title="5.2.1.1 多通道卷积"></a>5.2.1.1 多通道卷积</h4><p>在前面介绍的二维卷积运算中，卷积的输入数据是二维矩阵。但实际应用中，一幅大小为$M\times N$的图片中的每个像素的特征表示不仅仅只有灰度值的标量，通常有多个特征，可以表示为$D$维的向量，比如RGB三个通道的特征向量。因此，图像上的卷积操作的输入数据通常是一个三维张量，分别对应了图片的高度$M$、宽度$N$和深度$D$，其中深度$D$通常也被称为<strong>输入通道数</strong>$D$。如果输入如果是灰度图像，则输入通道数为1；如果输入是彩色图像，分别有$R、G、B$三个通道，则输入通道数为3。</p>
<p>此外，由于具有单个核的卷积每次只能提取一种类型的特征，即输出一张大小为$U\times V$的<strong>特征图</strong>（Feature Map）。而在实际应用中，我们也希望每一个卷积层能够提取多种不同类型的特征，所以一个卷积层通常会组合多个不同的卷积核来提取特征，经过卷积运算后会输出多张特征图，不同的特征图对应不同类型的特征。输出特征图的个数通常将其称为<strong>输出通道数</strong>$P$。</p>
<hr>
<p><strong>说明：</strong></p>
<p>《神经网络与深度学习》将Feature Map翻译为“特征映射”，这里翻译为“特征图”。</p>
<hr>
<p>假设一个卷积层的输入特征图$\mathbf X\in \mathbb{R}^{D\times M\times N}$，其中$(M,N)$为特征图的尺寸，$D$代表通道数；卷积核为$\mathbf W\in \mathbb{R}^{P\times D\times U\times V}$，其中$(U,V)$为卷积核的尺寸，$D$代表输入通道数，$P$代表输出通道数。</p>
<hr>
<p><strong>说明：</strong></p>
<p>在实践中，根据目前深度学习框架中张量的组织和运算性质，这里特征图的大小为$D\times M\times N$，和《神经网络与深度学习》中$M\times N \times D$的定义并不一致。<br>相应地，卷积核$W$的大小为$\mathbb{R}^{P\times D\times U\times V}$。</p>
<hr>
<p><strong>一张输出特征图的计算</strong></p>
<p>对于$D$个输入通道，分别对每个通道的特征图$\mathbf X^d$设计一个二维卷积核$\mathbf W^{p,d}$，并与对应的输入特征图$\mathbf X^d$进行卷积运算，再将得到的$D$个结果进行加和，得到一张输出特征图$\mathbf Z^p$。计算方式如下：</p>
<script type="math/tex; mode=display">
\mathbf Z^p = \sum_{d=1}^D \mathbf W^{p,d} \otimes \mathbf X^d + b^p，（5.18）</script><script type="math/tex; mode=display">
\mathbf Y^p = f(\mathbf Z^p)。（5.19）</script><p>其中$p$表示输出特征图的索引编号，$\mathbf W^{p,d} \in \mathbb{R}^{U\times V}$为二维卷积核，$b^p$为标量偏置，$f(·)$为非线性激活函数，一般用ReLU函数。</p>
<hr>
<p>说明：</p>
<p>在代码实现时，通常将非线性激活函数放在卷积层算子外部。</p>
<hr>
<p>公式（5.13）对应的可视化如<strong>图5.9</strong>所示。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/d4d1434e62154505a56259d09c9b6e072f8331830c2b4ffc97b5d7bd0181b075" width = "600"></center>
<center><br>图5.9：多输入通道的卷积运算 </br></center>

<p><br></br></p>
<p><strong>多张输出特征图的计算</strong></p>
<p>对于大小为$D\times M\times N$的输入特征图，每一个输出特征图都需要一组大小为$\mathbf W\in \mathbb{R}^{D\times U\times V}$的卷积核进行卷积运算。使用$P$组卷积核分布进行卷积运算，得到$P$个输出特征图$\mathbf Y^1, \mathbf Y^2,\cdots,\mathbf Y^P$。然后将$P$个输出特征图进行拼接，获得大小为$P\times M’ \times N’$的多通道输出特征图。上面计算方式的可视化如下<strong>图5.10</strong>所示。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/7a75788ae8d54f22882590a0923f5dd28bca6e120b1a4431abe122980d06224f" width = "600"></center>
<center><br>图5.10：多输出通道的卷积运算 </br></center>

<p><br></br></p>
<h4 id="5-2-1-2-多通道卷积层算子"><a href="#5-2-1-2-多通道卷积层算子" class="headerlink" title="5.2.1.2 多通道卷积层算子"></a>5.2.1.2 多通道卷积层算子</h4><p>根据上面的公式，多通道卷积卷积层的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                    weight_attr=paddle.ParamAttr(<span class="params">initializer=nn.initializer.Constant(<span class="params">value=<span class="number">1.0</span></span>)</span>),</span></span><br><span class="line"><span class="params">                    bias_attr=paddle.ParamAttr(<span class="params">initializer=nn.initializer.Constant(<span class="params">value=<span class="number">0.0</span></span>)</span>)</span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv2D, self).__init__()</span><br><span class="line">        <span class="comment"># 创建卷积核</span></span><br><span class="line">        self.weight = paddle.create_parameter(shape=[out_channels, in_channels, kernel_size,kernel_size],</span><br><span class="line">                                                dtype=<span class="string">&#x27;float32&#x27;</span>,</span><br><span class="line">                                                attr=weight_attr)</span><br><span class="line">        <span class="comment"># 创建偏置</span></span><br><span class="line">        self.bias = paddle.create_parameter(shape=[out_channels, <span class="number">1</span>],</span><br><span class="line">                                                dtype=<span class="string">&#x27;float32&#x27;</span>,</span><br><span class="line">                                                attr=bias_attr)</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.padding = padding</span><br><span class="line">        <span class="comment"># 输入通道数</span></span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        <span class="comment"># 输出通道数</span></span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基础卷积运算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">single_forward</span>(<span class="params">self, X, weight</span>):</span><br><span class="line">        <span class="comment"># 零填充</span></span><br><span class="line">        new_X = paddle.zeros([X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]+<span class="number">2</span>*self.padding, X.shape[<span class="number">2</span>]+<span class="number">2</span>*self.padding])</span><br><span class="line">        new_X[:, self.padding:X.shape[<span class="number">1</span>]+self.padding, self.padding:X.shape[<span class="number">2</span>]+self.padding] = X</span><br><span class="line">        u, v = weight.shape</span><br><span class="line">        output_w = (new_X.shape[<span class="number">1</span>] - u) // self.stride + <span class="number">1</span></span><br><span class="line">        output_h = (new_X.shape[<span class="number">2</span>] - v) // self.stride + <span class="number">1</span></span><br><span class="line">        output = paddle.zeros([X.shape[<span class="number">0</span>], output_w, output_h])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, output.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, output.shape[<span class="number">2</span>]):</span><br><span class="line">                output[:, i, j] = paddle.<span class="built_in">sum</span>(</span><br><span class="line">                    new_X[:, self.stride*i:self.stride*i+u, self.stride*j:self.stride*j+v]*weight, </span><br><span class="line">                    axis=[<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - inputs：输入矩阵，shape=[B, D, M, N]</span></span><br><span class="line"><span class="string">            - weights：P组二维卷积核，shape=[P, D, U, V]</span></span><br><span class="line"><span class="string">            - bias：P个偏置，shape=[P, 1]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        feature_maps = []</span><br><span class="line">        <span class="comment"># 进行多次多输入通道卷积运算</span></span><br><span class="line">        p=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> w, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.weight, self.bias): <span class="comment"># P个(w,b),每次计算一个特征图Zp</span></span><br><span class="line">            multi_outs = []</span><br><span class="line">            <span class="comment"># 循环计算每个输入特征图对应的卷积结果</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.in_channels):</span><br><span class="line">                single = self.single_forward(inputs[:,i,:,:], w[i])</span><br><span class="line">                multi_outs.append(single)</span><br><span class="line">                <span class="comment"># print(&quot;Conv2D in_channels:&quot;,self.in_channels,&quot;i:&quot;,i,&quot;single:&quot;,single.shape)</span></span><br><span class="line">            <span class="comment"># 将所有卷积结果相加</span></span><br><span class="line">            feature_map = paddle.<span class="built_in">sum</span>(paddle.stack(multi_outs), axis=<span class="number">0</span>) + b <span class="comment">#Zp</span></span><br><span class="line">            feature_maps.append(feature_map)</span><br><span class="line">            <span class="comment"># print(&quot;Conv2D out_channels:&quot;,self.out_channels, &quot;p:&quot;,p,&quot;feature_map:&quot;,feature_map.shape)</span></span><br><span class="line">            p+=<span class="number">1</span></span><br><span class="line">        <span class="comment"># 将所有Zp进行堆叠</span></span><br><span class="line">        out = paddle.stack(feature_maps, <span class="number">1</span>) </span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">inputs = paddle.to_tensor([[[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]]])</span><br><span class="line">conv2d = Conv2D(in_channels=<span class="number">2</span>, out_channels=<span class="number">3</span>, kernel_size=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;inputs shape:&quot;</span>,inputs.shape)</span><br><span class="line">outputs = conv2d(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Conv2D outputs shape:&quot;</span>,outputs.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较与paddle API运算结果</span></span><br><span class="line">conv2d_paddle = nn.Conv2D(in_channels=<span class="number">2</span>, out_channels=<span class="number">3</span>, kernel_size=<span class="number">2</span>,</span><br><span class="line">                            weight_attr=paddle.ParamAttr(initializer=nn.initializer.Constant(value=<span class="number">1.0</span>)),</span><br><span class="line">                            bias_attr=paddle.ParamAttr(initializer=nn.initializer.Constant(value=<span class="number">0.0</span>)))</span><br><span class="line">outputs_paddle = conv2d_paddle(inputs)</span><br><span class="line"><span class="comment"># 自定义算子运算结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Conv2D outputs:&#x27;</span>, outputs)</span><br><span class="line"><span class="comment"># paddle API运算结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;nn.Conv2D outputs:&#x27;</span>, outputs_paddle)</span><br></pre></td></tr></table></figure>
<h4 id="5-2-1-3-卷积算子的参数量和计算量"><a href="#5-2-1-3-卷积算子的参数量和计算量" class="headerlink" title="5.2.1.3 卷积算子的参数量和计算量"></a>5.2.1.3 卷积算子的参数量和计算量</h4><p><strong>参数量</strong></p>
<p>对于大小为$D\times M\times N$的输入特征图，使用$P$组大小为$\mathbf W\in \mathbb{R}^{D\times U\times V}$的卷积核进行卷积运算，参数量计算方式为：</p>
<script type="math/tex; mode=display">
parameters = P \times D \times U \times V + P.（5.20）</script><p>其中，最后的$P$代表偏置个数。例如：输入特征图大小为$3\times 32\times 32$，使用$6$组大小为$3\times 3\times 3$的卷积核进行卷积运算，参数量为：</p>
<script type="math/tex; mode=display">
parameters = 6 \times 3 \times 3 \times 3 + 6= 168.</script><p><strong>计算量</strong></p>
<p>对于大小为$D\times M\times N$的输入特征图，使用$P$组大小为$\mathbf W\in \mathbb{R}^{D\times U\times V}$的卷积核进行卷积运算，计算量计算方式为：</p>
<script type="math/tex; mode=display">FLOPs=M'\times N'\times P\times D\times U\times V + M'\times N'\times P。（5.21）</script><p>其中$M’\times N’\times P$代表加偏置的计算量，即输出特征图上每个点都要与$P$组卷积核$\mathbf W\in \mathbb{R}^{D\times U\times V}$进行$U\times V\times D$次乘法运算后再加上偏置。比如对于输入特征图大小为$3\times 32\times 32$，使用$6$组大小为$3\times 3\times 3$的卷积核进行卷积运算，计算量为：</p>
<script type="math/tex; mode=display">FLOPs=M'\times N'\times P\times D\times U\times V + M'\times N'\times P= 30\times 30\times 3\times 3\times 6\times 3 + 30\times 30\times 6= 151200</script><h3 id="5-2-2-汇聚层算子"><a href="#5-2-2-汇聚层算子" class="headerlink" title="5.2.2 汇聚层算子"></a>5.2.2 汇聚层算子</h3><p><strong>汇聚层</strong>的作用是进行特征选择，降低特征数量，从而减少参数数量。由于汇聚之后特征图会变得更小，如果后面连接的是全连接层，可以有效地减小神经元的个数，节省存储空间并提高计算效率。</p>
<p>常用的汇聚方法有两种，分别是：平均汇聚和最大汇聚。</p>
<ul>
<li>平均汇聚：将输入特征图划分为$2\times2$大小的区域，对每个区域内的神经元活性值取平均值作为这个区域的表示；</li>
<li>最大汇聚：使用输入特征图的每个子区域内所有神经元的最大活性值作为这个区域的表示。</li>
</ul>
<p><strong>图5.11</strong> 给出了两种汇聚层的示例。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/d55501bb93914955b1c262a00201f8bcc6abf632085e48e3b37aac5f064a4e1c" width = "600"></center>
<center><br>图5.11：汇聚层 </br></center>

<p><br></br></p>
<p>汇聚层输出的计算尺寸与卷积层一致，对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个运算区域大小为$U\times V$的汇聚层，步长为$S$，对输入矩阵进行零填充，那么最终输出矩阵大小则为</p>
<script type="math/tex; mode=display">M' = \frac{M + 2P - U}{S} + 1,（5.20）</script><script type="math/tex; mode=display">N' = \frac{N + 2P - V}{S} + 1.（5.21）</script><p>由于过大的采样区域会急剧减少神经元的数量，也会造成过多的信息丢失。目前，在卷积神经网络中比较典型的汇聚层是将每个输入特征图划分为$2\times2$大小的不重叠区域，然后使用最大汇聚的方式进行下采样。</p>
<p>由于汇聚是使用某一位置的相邻输出的总体统计特征代替网络在该位置的输出，所以其好处是当输入数据做出少量平移时，经过汇聚运算后的大多数输出还能保持不变。比如：当识别一张图像是否是人脸时，我们需要知道人脸左边有一只眼睛，右边也有一只眼睛，而不需要知道眼睛的精确位置，这时候通过汇聚某一片区域的像素点来得到总体统计特征会显得很有用。这也就体现了汇聚层的<strong>平移不变</strong>特性。</p>
<p><strong>汇聚层的参数量和计算量</strong></p>
<p>由于汇聚层中没有参数，所以参数量为$0$；最大汇聚中，没有乘加运算，所以计算量为$0$，而平均汇聚中，输出特征图上每个点都对应了一次求平均运算。</p>
<p>使用飞桨实现一个简单的汇聚层，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Pool2D</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size=(<span class="params"><span class="number">2</span>,<span class="number">2</span></span>), mode=<span class="string">&#x27;max&#x27;</span>, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Pool2D, self).__init__()</span><br><span class="line">        <span class="comment"># 汇聚方式</span></span><br><span class="line">        self.mode = mode</span><br><span class="line">        self.h, self.w = size</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output_w = (x.shape[<span class="number">2</span>] - self.w) // self.stride + <span class="number">1</span></span><br><span class="line">        output_h = (x.shape[<span class="number">3</span>] - self.h) // self.stride + <span class="number">1</span></span><br><span class="line">        output = paddle.zeros([x.shape[<span class="number">0</span>], x.shape[<span class="number">1</span>], output_w, output_h])</span><br><span class="line">        <span class="comment"># 汇聚</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(output.shape[<span class="number">2</span>]):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(output.shape[<span class="number">3</span>]):</span><br><span class="line">                <span class="comment"># 最大汇聚</span></span><br><span class="line">                <span class="keyword">if</span> self.mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                    output[:, :, i, j] = paddle.<span class="built_in">max</span>(</span><br><span class="line">                        x[:, :, self.stride*i:self.stride*i+self.w, self.stride*j:self.stride*j+self.h], </span><br><span class="line">                        axis=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">                <span class="comment"># 平均汇聚</span></span><br><span class="line">                <span class="keyword">elif</span> self.mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                    output[:, :, i, j] = paddle.mean(</span><br><span class="line">                        x[:, :, self.stride*i:self.stride*i+self.w, self.stride*j:self.stride*j+self.h], </span><br><span class="line">                        axis=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">inputs = paddle.to_tensor([[[[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>,<span class="number">4.</span>],[<span class="number">5.</span>,<span class="number">6.</span>,<span class="number">7.</span>,<span class="number">8.</span>],[<span class="number">9.</span>,<span class="number">10.</span>,<span class="number">11.</span>,<span class="number">12.</span>],[<span class="number">13.</span>,<span class="number">14.</span>,<span class="number">15.</span>,<span class="number">16.</span>]]]])</span><br><span class="line">pool2d = Pool2D(stride=<span class="number">2</span>)</span><br><span class="line">outputs = pool2d(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input: &#123;&#125;, \noutput: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(inputs.shape, outputs.shape))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较Maxpool2D与paddle API运算结果</span></span><br><span class="line">maxpool2d_paddle = nn.MaxPool2D(kernel_size=(<span class="number">2</span>,<span class="number">2</span>), stride=<span class="number">2</span>)</span><br><span class="line">outputs_paddle = maxpool2d_paddle(inputs)</span><br><span class="line"><span class="comment"># 自定义算子运算结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Maxpool2D outputs:&#x27;</span>, outputs)</span><br><span class="line"><span class="comment"># paddle API运算结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;nn.Maxpool2D outputs:&#x27;</span>, outputs_paddle)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较Avgpool2D与paddle API运算结果</span></span><br><span class="line">avgpool2d_paddle = nn.AvgPool2D(kernel_size=(<span class="number">2</span>,<span class="number">2</span>), stride=<span class="number">2</span>)</span><br><span class="line">outputs_paddle = avgpool2d_paddle(inputs)</span><br><span class="line">pool2d = Pool2D(mode=<span class="string">&#x27;avg&#x27;</span>, stride=<span class="number">2</span>)</span><br><span class="line">outputs = pool2d(inputs)</span><br><span class="line"><span class="comment"># 自定义算子运算结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Avgpool2D outputs:&#x27;</span>, outputs)</span><br><span class="line"><span class="comment"># paddle API运算结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;nn.Avgpool2D outputs:&#x27;</span>, outputs_paddle)</span><br></pre></td></tr></table></figure>
<h2 id="5-3-基于LeNet实现手写体数字识别实验"><a href="#5-3-基于LeNet实现手写体数字识别实验" class="headerlink" title="5.3 基于LeNet实现手写体数字识别实验"></a>5.3 基于LeNet实现手写体数字识别实验</h2><p>在本节中，我们实现经典卷积网络LeNet-5，并进行手写体数字识别任务。</p>
<h3 id="5-3-1-数据"><a href="#5-3-1-数据" class="headerlink" title="5.3.1 数据"></a>5.3.1 数据</h3><p>手写体数字识别是计算机视觉中最常用的图像分类任务，让计算机识别出给定图片中的手写体数字（0-9共10个数字）。由于手写体风格差异很大，因此手写体数字识别是具有一定难度的任务。</p>
<p>我们采用常用的手写数字识别数据集：<strong>MNIST数据集</strong>。MNIST数据集是计算机视觉领域的经典入门数据集，包含了60,000个训练样本和10,000个测试样本。这些数字已经过尺寸标准化并位于图像中心，图像是固定大小($28\times28$像素)。<strong>图5.12</strong>给出了部分样本的示例。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/53774cf008e941d3b93ce2b8731a1089fc64539bebd347e3a255c2d8fb5ec1a9" width="600" hegiht="" ></center>
<center><br>图5.12：MNIST数据集示例</br></center>

<p><br></br></p>
<p>为了节省训练时间，本节选取MNIST数据集的一个子集进行后续实验，数据集的划分为：</p>
<ul>
<li>训练集：1,000条样本</li>
<li>验证集：200条样本</li>
<li>测试集：200条样本</li>
</ul>
<p>MNIST数据集分为train_set、dev_set和test_set三个数据集，每个数据集含两个列表分别存放了图片数据以及标签数据。比如train_set包含：</p>
<ul>
<li><strong>图片数据</strong>：[1 000, 784]的二维列表，包含1 000张图片。每张图片用一个长度为784的向量表示，内容是 $28\times 28$ 尺寸的像素灰度值（黑白图片）。</li>
<li><strong>标签数据</strong>：[1 000, 1]的列表，表示这些图片对应的分类标签，即0~9之间的数字。</li>
</ul>
<p>观察数据集分布情况，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印并观察数据集分布情况</span></span><br><span class="line">train_set, dev_set, test_set = json.load(gzip.<span class="built_in">open</span>(<span class="string">&#x27;./mnist.json.gz&#x27;</span>))</span><br><span class="line">train_images, train_labels = train_set[<span class="number">0</span>][:<span class="number">1000</span>], train_set[<span class="number">1</span>][:<span class="number">1000</span>]</span><br><span class="line">dev_images, dev_labels = dev_set[<span class="number">0</span>][:<span class="number">200</span>], dev_set[<span class="number">1</span>][:<span class="number">200</span>]</span><br><span class="line">test_images, test_labels = test_set[<span class="number">0</span>][:<span class="number">200</span>], test_set[<span class="number">1</span>][:<span class="number">200</span>]</span><br><span class="line">train_set, dev_set, test_set = [train_images, train_labels], [dev_images, dev_labels], [test_images, test_labels]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Length of train/dev/test set:&#123;&#125;/&#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_set[<span class="number">0</span>]), <span class="built_in">len</span>(dev_set[<span class="number">0</span>]), <span class="built_in">len</span>(test_set[<span class="number">0</span>])))</span><br></pre></td></tr></table></figure>
<p>可视化观察其中的一张样本以及对应的标签，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">image, label = train_set[<span class="number">0</span>][<span class="number">0</span>], train_set[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">image, label = np.array(image).astype(<span class="string">&#x27;float32&#x27;</span>), <span class="built_in">int</span>(label)</span><br><span class="line"><span class="comment"># 原始图像数据为长度784的行向量，需要调整为[28,28]大小的图像</span></span><br><span class="line">image = np.reshape(image, [<span class="number">28</span>,<span class="number">28</span>])</span><br><span class="line">image = Image.fromarray(image.astype(<span class="string">&#x27;uint8&#x27;</span>), mode=<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The number in the picture is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(label))</span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.imshow(image)</span><br><span class="line">plt.savefig(<span class="string">&#x27;conv-number5.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="5-3-1-1-数据预处理"><a href="#5-3-1-1-数据预处理" class="headerlink" title="5.3.1.1 数据预处理"></a>5.3.1.1 数据预处理</h4><p>图像分类网络对输入图片的格式、大小有一定的要求，数据输入模型前，需要对数据进行预处理操作，使图片满足网络训练以及预测的需要。本实验主要应用了如下方法：</p>
<ul>
<li>调整图片大小：LeNet网络对输入图片大小的要求为 $32\times 32$ ，而MNIST数据集中的原始图片大小却是 $28\times 28$ ，这里为了符合网络的结构设计，将其调整为$32 \times 32$；</li>
<li>规范化： 通过规范化手段，把输入图像的分布改变成均值为0，标准差为1的标准正态分布，使得最优解的寻优过程明显会变得平缓，训练过程更容易收敛。</li>
</ul>
<hr>
<p>在飞桨中，提供了部分视觉领域的高层API，可以直接调用API实现简单的图像处理操作。通过调用<code>paddle.vision.transforms.Resize</code>调整大小；调用<code>paddle.vision.transforms.Normalize</code>进行标准化处理；使用<code>paddle.vision.transforms.Compose</code>将两个预处理操作进行拼接。</p>
<hr>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.vision.transforms <span class="keyword">import</span> Compose, Resize, Normalize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transforms = Compose([Resize(<span class="number">32</span>), Normalize(mean=[<span class="number">127.5</span>], std=[<span class="number">127.5</span>], data_format=<span class="string">&#x27;CHW&#x27;</span>)])</span><br></pre></td></tr></table></figure>
<p>将原始的数据集封装为Dataset类，以便DataLoader调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> paddle.io <span class="keyword">as</span> io</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MNIST_dataset</span>(io.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, transforms, mode=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">        self.mode = mode</span><br><span class="line">        self.transforms =transforms</span><br><span class="line">        self.dataset = dataset</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 获取图像和标签</span></span><br><span class="line">        image, label = self.dataset[<span class="number">0</span>][idx], self.dataset[<span class="number">1</span>][idx]</span><br><span class="line">        image, label = np.array(image).astype(<span class="string">&#x27;float32&#x27;</span>), <span class="built_in">int</span>(label)</span><br><span class="line">        image = np.reshape(image, [<span class="number">28</span>,<span class="number">28</span>])</span><br><span class="line">        image = Image.fromarray(image.astype(<span class="string">&#x27;uint8&#x27;</span>), mode=<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">        image = self.transforms(image)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dataset[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 加载 mnist 数据集</span></span><br><span class="line">train_dataset = MNIST_dataset(dataset=train_set, transforms=transforms, mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">test_dataset = MNIST_dataset(dataset=test_set, transforms=transforms, mode=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">dev_dataset = MNIST_dataset(dataset=dev_set, transforms=transforms, mode=<span class="string">&#x27;dev&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-3-2-模型构建"><a href="#5-3-2-模型构建" class="headerlink" title="5.3.2 模型构建"></a>5.3.2 模型构建</h3><p>LeNet-5虽然提出的时间比较早，但它是一个非常成功的神经网络模型。基于LeNet-5的手写数字识别系统在20世纪90年代被美国很多银行使用，用来识别支票上面的手写数字。LeNet-5的网络结构如<strong>图5.13</strong>所示。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/448b1f120a2e4890bae4589d213c0ab36f71b8d9d6204998a6cceddaabbe9ab5" width="600" hegiht="" ></center>
<center><br>图5.13：LeNet-5网络结构</br></center>

<p><br></br></p>
<p>我们使用上面定义的卷积层算子和汇聚层算子构建一个LeNet-5模型。</p>
<hr>
<p>  这里的LeNet-5和原始版本有4点不同：</p>
<ol>
<li>C3层没有使用连接表来减少卷积数量。</li>
<li>汇聚层使用了简单的平均汇聚，没有引入权重和偏置参数以及非线性激活函数。</li>
<li>卷积层的激活函数使用ReLU函数。</li>
<li>最后的输出层为一个全连接线性层。</li>
</ol>
<hr>
<p>网络共有7层，包含3个卷积层、2个汇聚层以及2个全连接层的简单卷积神经网络接，受输入图像大小为$32\times 32=1\, 024$，输出对应10个类别的得分。<br>具体实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model_LeNet</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Model_LeNet, self).__init__()</span><br><span class="line">        <span class="comment"># 卷积层：输出通道数为6，卷积核大小为5×5</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=in_channels, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, weight_attr=paddle.ParamAttr())</span><br><span class="line">        <span class="comment"># 汇聚层：汇聚窗口为2×2，步长为2</span></span><br><span class="line">        self.pool2 = Pool2D(size=(<span class="number">2</span>,<span class="number">2</span>), mode=<span class="string">&#x27;max&#x27;</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 卷积层：输入通道数为6，输出通道数为16，卷积核大小为5×5，步长为1</span></span><br><span class="line">        self.conv3 = Conv2D(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, weight_attr=paddle.ParamAttr())</span><br><span class="line">        <span class="comment"># 汇聚层：汇聚窗口为2×2，步长为2</span></span><br><span class="line">        self.pool4 = Pool2D(size=(<span class="number">2</span>,<span class="number">2</span>), mode=<span class="string">&#x27;avg&#x27;</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 卷积层：输入通道数为16，输出通道数为120，卷积核大小为5×5</span></span><br><span class="line">        self.conv5 = Conv2D(in_channels=<span class="number">16</span>, out_channels=<span class="number">120</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, weight_attr=paddle.ParamAttr())</span><br><span class="line">        <span class="comment"># 全连接层：输入神经元为120，输出神经元为84</span></span><br><span class="line">        self.linear6 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 全连接层：输入神经元为84，输出神经元为类别数</span></span><br><span class="line">        self.linear7 = nn.Linear(<span class="number">84</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># C1：卷积层+激活函数</span></span><br><span class="line">        output = F.relu(self.conv1(x))</span><br><span class="line">        <span class="comment"># S2：汇聚层</span></span><br><span class="line">        output = self.pool2(output)</span><br><span class="line">        <span class="comment"># C3：卷积层+激活函数</span></span><br><span class="line">        output = F.relu(self.conv3(output))</span><br><span class="line">        <span class="comment"># S4：汇聚层</span></span><br><span class="line">        output = self.pool4(output)</span><br><span class="line">        <span class="comment"># C5：卷积层+激活函数</span></span><br><span class="line">        output = F.relu(self.conv5(output))</span><br><span class="line">        <span class="comment"># 输入层将数据拉平[B,C,H,W] -&gt; [B,CxHxW]</span></span><br><span class="line">        output = paddle.squeeze(output, axis=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">        <span class="comment"># F6：全连接层</span></span><br><span class="line">        output = F.relu(self.linear6(output))</span><br><span class="line">        <span class="comment"># F7：全连接层</span></span><br><span class="line">        output = self.linear7(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>下面测试一下上面的LeNet-5模型，构造一个形状为 [1,1,32,32]的输入数据送入网络，观察每一层特征图的形状变化。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里用np.random创建一个随机数组作为输入数据</span></span><br><span class="line">inputs = np.random.randn(*[<span class="number">1</span>,<span class="number">1</span>,<span class="number">32</span>,<span class="number">32</span>])</span><br><span class="line">inputs = inputs.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Model_LeNet类的实例，指定模型名称和分类的类别数目</span></span><br><span class="line">model = Model_LeNet(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 通过调用LeNet从基类继承的sublayers()函数，查看LeNet中所包含的子层</span></span><br><span class="line"><span class="built_in">print</span>(model.sublayers())</span><br><span class="line">x = paddle.to_tensor(inputs)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> model.sublayers():</span><br><span class="line">    <span class="comment"># item是LeNet类中的一个子层</span></span><br><span class="line">    <span class="comment"># 查看经过子层之后的输出数据形状</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="comment"># 如果是最后一个卷积层输出，需要展平后才可以送入全连接层</span></span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(item.parameters())==<span class="number">2</span>:</span><br><span class="line">        <span class="comment"># 查看卷积和全连接层的数据和参数的形状，</span></span><br><span class="line">        <span class="comment"># 其中item.parameters()[0]是权重参数w，item.parameters()[1]是偏置参数b</span></span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape, item.parameters()[<span class="number">0</span>].shape, </span><br><span class="line">                item.parameters()[<span class="number">1</span>].shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 汇聚层没有参数</span></span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape)</span><br></pre></td></tr></table></figure>
<p>从输出结果看，</p>
<ul>
<li>对于大小为$32 \times32$的单通道图像，先用6个大小为$5 \times5$的卷积核对其进行卷积运算，输出为6个$28 \times28$大小的特征图；</li>
<li>6个$28 \times28$大小的特征图经过大小为$2 \times2$，步长为2的汇聚层后，输出特征图的大小变为$14 \times14$；</li>
<li>6个$14 \times14$大小的特征图再经过16个大小为$5 \times5$的卷积核对其进行卷积运算，得到16个$10 \times10$大小的输出特征图；</li>
<li>16个$10 \times10$大小的特征图经过大小为$2 \times2$，步长为2的汇聚层后，输出特征图的大小变为$5 \times5$；</li>
<li>16个$5 \times5$大小的特征图再经过120个大小为$5 \times5$的卷积核对其进行卷积运算，得到120个$1 \times1$大小的输出特征图；</li>
<li>此时，将特征图展平成1维，则有120个像素点，经过输入神经元个数为120，输出神经元个数为84的全连接层后，输出的长度变为84。</li>
<li>再经过一个全连接层的计算，最终得到了长度为类别数的输出结果。</li>
</ul>
<p>考虑到自定义的<code>Conv2D</code>和<code>Pool2D</code>算子中包含多个<code>for</code>循环，所以运算速度比较慢。飞桨框架中，针对卷积层算子和汇聚层算子进行了速度上的优化，这里基于<code>paddle.nn.Conv2D</code>、<code>paddle.nn.MaxPool2D</code>和<code>paddle.nn.AvgPool2D</code>构建LeNet-5模型，对比与上边实现的模型的运算速度。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Paddle_LeNet</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Paddle_LeNet, self).__init__()</span><br><span class="line">        <span class="comment"># 卷积层：输出通道数为6，卷积核大小为5*5</span></span><br><span class="line">        self.conv1 = nn.Conv2D(in_channels=in_channels, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 汇聚层：汇聚窗口为2*2，步长为2</span></span><br><span class="line">        self.pool2 = nn.MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 卷积层：输入通道数为6，输出通道数为16，卷积核大小为5*5</span></span><br><span class="line">        self.conv3 = nn.Conv2D(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 汇聚层：汇聚窗口为2*2，步长为2</span></span><br><span class="line">        self.pool4 = nn.AvgPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 卷积层：输入通道数为16，输出通道数为120，卷积核大小为5*5</span></span><br><span class="line">        self.conv5 = nn.Conv2D(in_channels=<span class="number">16</span>, out_channels=<span class="number">120</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 全连接层：输入神经元为120，输出神经元为84</span></span><br><span class="line">        self.linear6 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 全连接层：输入神经元为84，输出神经元为类别数</span></span><br><span class="line">        self.linear7 = nn.Linear(in_features=<span class="number">84</span>, out_features=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># C1：卷积层+激活函数</span></span><br><span class="line">        output = F.relu(self.conv1(x))</span><br><span class="line">        <span class="comment"># S2：汇聚层</span></span><br><span class="line">        output = self.pool2(output)</span><br><span class="line">        <span class="comment"># C3：卷积层+激活函数</span></span><br><span class="line">        output = F.relu(self.conv3(output))</span><br><span class="line">        <span class="comment"># S4：汇聚层</span></span><br><span class="line">        output = self.pool4(output)</span><br><span class="line">        <span class="comment"># C5：卷积层+激活函数</span></span><br><span class="line">        output = F.relu(self.conv5(output))</span><br><span class="line">        <span class="comment"># 输入层将数据拉平[B,C,H,W] -&gt; [B,CxHxW]</span></span><br><span class="line">        output = paddle.squeeze(output, axis=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">        <span class="comment"># F6：全连接层</span></span><br><span class="line">        output = F.relu(self.linear6(output))</span><br><span class="line">        <span class="comment"># F7：全连接层</span></span><br><span class="line">        output = self.linear7(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>测试两个网络的运算速度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里用np.random创建一个随机数组作为测试数据</span></span><br><span class="line">inputs = np.random.randn(*[<span class="number">1</span>,<span class="number">1</span>,<span class="number">32</span>,<span class="number">32</span>])</span><br><span class="line">inputs = inputs.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = paddle.to_tensor(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Model_LeNet类的实例，指定模型名称和分类的类别数目</span></span><br><span class="line">model = Model_LeNet(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 创建Paddle_LeNet类的实例，指定模型名称和分类的类别数目</span></span><br><span class="line">paddle_model = Paddle_LeNet(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算Model_LeNet类的运算速度</span></span><br><span class="line">model_time = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):</span><br><span class="line">    strat_time = time.time()</span><br><span class="line">    out = model(x)</span><br><span class="line">    end_time = time.time()</span><br><span class="line">    <span class="comment"># 预热10次运算，不计入最终速度统计</span></span><br><span class="line">    <span class="keyword">if</span> i &lt; <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    model_time += (end_time - strat_time)</span><br><span class="line">avg_model_time = model_time / <span class="number">50</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Model_LeNet speed:&#x27;</span>, avg_model_time, <span class="string">&#x27;s&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算Paddle_LeNet类的运算速度</span></span><br><span class="line">paddle_model_time = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):</span><br><span class="line">    strat_time = time.time()</span><br><span class="line">    paddle_out = paddle_model(x)</span><br><span class="line">    end_time = time.time()</span><br><span class="line">    <span class="comment"># 预热10次运算，不计入最终速度统计</span></span><br><span class="line">    <span class="keyword">if</span> i &lt; <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    paddle_model_time += (end_time - strat_time)</span><br><span class="line">avg_paddle_model_time = paddle_model_time / <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Paddle_LeNet speed:&#x27;</span>, avg_paddle_model_time, <span class="string">&#x27;s&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>这里还可以令两个网络加载同样的权重，测试一下两个网络的输出结果是否一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里用np.random创建一个随机数组作为测试数据</span></span><br><span class="line">inputs = np.random.randn(*[<span class="number">1</span>,<span class="number">1</span>,<span class="number">32</span>,<span class="number">32</span>])</span><br><span class="line">inputs = inputs.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = paddle.to_tensor(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Model_LeNet类的实例，指定模型名称和分类的类别数目</span></span><br><span class="line">model = Model_LeNet(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 获取网络的权重</span></span><br><span class="line">params = model.state_dict()</span><br><span class="line"><span class="comment"># 自定义Conv2D算子的bias参数形状为[out_channels, 1]</span></span><br><span class="line"><span class="comment"># paddle API中Conv2D算子的bias参数形状为[out_channels]</span></span><br><span class="line"><span class="comment"># 需要进行调整后才可以赋值</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> params:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;bias&#x27;</span> <span class="keyword">in</span> key:</span><br><span class="line">        params[key] = params[key].squeeze()</span><br><span class="line"><span class="comment"># 创建Paddle_LeNet类的实例，指定模型名称和分类的类别数目</span></span><br><span class="line">paddle_model = Paddle_LeNet(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 将Model_LeNet的权重参数赋予给Paddle_LeNet模型，保持两者一致</span></span><br><span class="line">paddle_model.set_state_dict(params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果保留小数点后6位</span></span><br><span class="line">paddle.set_printoptions(<span class="number">6</span>)</span><br><span class="line"><span class="comment"># 计算Model_LeNet的结果</span></span><br><span class="line">output = model(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Model_LeNet output: &#x27;</span>, output)</span><br><span class="line"><span class="comment"># 计算Paddle_LeNet的结果</span></span><br><span class="line">paddle_output = paddle_model(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Paddle_LeNet output: &#x27;</span>, paddle_output)</span><br></pre></td></tr></table></figure>
<p>可以看到，输出结果是一致的。</p>
<p>这里还可以统计一下LeNet-5模型的参数量和计算量。</p>
<p><strong>参数量</strong></p>
<p>按照公式(5.18)进行计算，可以得到：</p>
<ul>
<li>第一个卷积层的参数量为：$6 \times 1 \times 5 \times 5 + 6 = 156$；</li>
<li>第二个卷积层的参数量为：$16 \times 6 \times 5 \times 5 + 16 = 2416$；</li>
<li>第三个卷积层的参数量为：$120 \times 16 \times 5 \times 5 + 120= 48120$；</li>
<li>第一个全连接层的参数量为：$120 \times 84 + 84= 10164$；</li>
<li>第二个全连接层的参数量为：$84 \times 10 + 10= 850$；</li>
</ul>
<p>所以，LeNet-5总的参数量为$61706$。</p>
<p>在飞桨中，还可以使用<code>paddle.summary</code>API自动计算参数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Paddle_LeNet(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>)</span><br><span class="line">params_info = paddle.summary(model, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"><span class="built_in">print</span>(params_info)</span><br></pre></td></tr></table></figure>
<p>可以看到，结果与公式推导一致。</p>
<p><strong>计算量</strong></p>
<p>按照公式(5.19)进行计算，可以得到：</p>
<ul>
<li>第一个卷积层的计算量为：$28\times 28\times 5\times 5\times 6\times 1 + 28\times 28\times 6=122304$；</li>
<li>第二个卷积层的计算量为：$10\times 10\times 5\times 5\times 16\times 6 + 10\times 10\times 16=241600$；</li>
<li>第三个卷积层的计算量为：$1\times 1\times 5\times 5\times 120\times 16 + 1\times 1\times 120=48120$；</li>
<li>平均汇聚层的计算量为：$16\times 5\times 5=400$</li>
<li>第一个全连接层的计算量为：$120 \times 84 = 10080$；</li>
<li>第二个全连接层的计算量为：$84 \times 10 = 840$；</li>
</ul>
<p>所以，LeNet-5总的计算量为$423344$。</p>
<p>在飞桨中，还可以使用<code>paddle.flops</code>API自动统计计算量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FLOPs = paddle.flops(model, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>), print_detail=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(FLOPs)</span><br></pre></td></tr></table></figure>
<p>可以看到，结果与公式推导一致。</p>
<h3 id="5-3-3-模型训练"><a href="#5-3-3-模型训练" class="headerlink" title="5.3.3 模型训练"></a>5.3.3 模型训练</h3><p>使用交叉熵损失函数，并用随机梯度下降法作为优化器来训练LeNet-5网络。<br>用RunnerV3在训练集上训练5个epoch，并保存准确率最高的模型作为最佳模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.optimizer <span class="keyword">as</span> opt</span><br><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> RunnerV3, metric</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 学习率大小</span></span><br><span class="line">lr = <span class="number">0.1</span>    </span><br><span class="line"><span class="comment"># 批次大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)</span><br><span class="line">test_loader = io.DataLoader(test_dataset, batch_size=batch_size)</span><br><span class="line"><span class="comment"># 定义LeNet网络</span></span><br><span class="line"><span class="comment"># 自定义算子实现的LeNet-5</span></span><br><span class="line">model = Model_LeNet(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 飞桨API实现的LeNet-5</span></span><br><span class="line"><span class="comment"># model = Paddle_LeNet(in_channels=1, num_classes=10)</span></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss_fn = F.cross_entropy</span><br><span class="line"><span class="comment"># 定义评价指标</span></span><br><span class="line">metric = metric.Accuracy(is_logist=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 实例化 RunnerV3 类，并传入训练配置。</span></span><br><span class="line">runner = RunnerV3(model, optimizer, loss_fn, metric)</span><br><span class="line"><span class="comment"># 启动训练</span></span><br><span class="line">log_steps = <span class="number">15</span></span><br><span class="line">eval_steps = <span class="number">15</span></span><br><span class="line">runner.train(train_loader, dev_loader, num_epochs=<span class="number">5</span>, log_steps=log_steps, </span><br><span class="line">                eval_steps=eval_steps, save_path=<span class="string">&quot;best_model.pdparams&quot;</span>)          </span><br></pre></td></tr></table></figure>
<p>可视化观察训练集与验证集的损失变化情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> plot_training_loss_acc</span><br><span class="line">plot_training_loss_acc(runner, <span class="string">&#x27;cnn-loss1.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-3-4-模型评价"><a href="#5-3-4-模型评价" class="headerlink" title="5.3.4 模型评价"></a>5.3.4 模型评价</h3><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率以及损失变化情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载最优模型</span></span><br><span class="line">runner.load_model(<span class="string">&#x27;best_model.pdparams&#x27;</span>)</span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line">score, loss = runner.evaluate(test_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(score, loss))</span><br></pre></td></tr></table></figure>
<h3 id="5-3-5-模型预测"><a href="#5-3-5-模型预测" class="headerlink" title="5.3.5 模型预测"></a>5.3.5 模型预测</h3><p>同样地，我们也可以使用保存好的模型，对测试集中的某一个数据进行模型预测，观察模型效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取测试集中第一条数据</span></span><br><span class="line">X, label = <span class="built_in">next</span>(test_loader())</span><br><span class="line">logits = runner.predict(X)</span><br><span class="line"><span class="comment"># 多分类，使用softmax计算预测概率</span></span><br><span class="line">pred = F.softmax(logits)</span><br><span class="line"><span class="comment"># 获取概率最大的类别</span></span><br><span class="line">pred_class = paddle.argmax(pred[<span class="number">1</span>]).numpy()</span><br><span class="line">label = label[<span class="number">1</span>][<span class="number">0</span>].numpy()</span><br><span class="line"><span class="comment"># 输出真实类别与预测类别</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The true category is &#123;&#125; and the predicted category is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(label[<span class="number">0</span>], pred_class[<span class="number">0</span>]))</span><br><span class="line"><span class="comment"># 可视化图片</span></span><br><span class="line">plt.figure(figsize=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">image, label = test_set[<span class="number">0</span>][<span class="number">1</span>], test_set[<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">image= np.array(image).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">image = np.reshape(image, [<span class="number">28</span>,<span class="number">28</span>])</span><br><span class="line">image = Image.fromarray(image.astype(<span class="string">&#x27;uint8&#x27;</span>), mode=<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">plt.imshow(image)</span><br><span class="line">plt.savefig(<span class="string">&#x27;cnn-number2.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="5-4-基于残差网络的手写体数字识别实验"><a href="#5-4-基于残差网络的手写体数字识别实验" class="headerlink" title="5.4 基于残差网络的手写体数字识别实验"></a>5.4 基于残差网络的手写体数字识别实验</h2><p><strong>残差网络</strong>（Residual Network，ResNet）是在神经网络模型中给非线性层增加直连边的方式来缓解梯度消失问题，从而使训练深度神经网络变得更加容易。</p>
<p>在残差网络中，最基本的单位为<strong>残差单元</strong>。</p>
<p>假设$f(\mathbf x;\theta)$为一个或多个神经层，残差单元在$f()$的输入和输出之间加上一个<strong>直连边</strong>。</p>
<p>不同于传统网络结构中让网络$f(x;\theta)$去逼近一个目标函数$h(x)$，在残差网络中，将目标函数$h(x)$拆为了两个部分：恒等函数$x$和残差函数$h(x)-x$</p>
<script type="math/tex; mode=display">
\mathrm{ResBlock}_f(\mathbf x) = f(\mathbf x;\theta) + \mathbf x,（5.22）</script><p>其中$\theta$为可学习的参数。</p>
<p>一个典型的残差单元如<strong>图5.14</strong>所示，由多个级联的卷积层和一个跨层的直连边组成。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/cf14626bb6c54eb9840a7253975fdc995f165146c6324bdb8f9b943b96f57394" width = "200"></center>
<center><br>图5.14：残差单元结构</br></center>


<p>一个残差网络通常有很多个残差单元堆叠而成。下面我们来构建一个在计算机视觉中非常典型的残差网络：ResNet18，并重复上一节中的手写体数字识别任务。</p>
<h3 id="5-4-1-模型构建"><a href="#5-4-1-模型构建" class="headerlink" title="5.4.1 模型构建"></a>5.4.1 模型构建</h3><p>在本节中，我们先构建ResNet18的残差单元，然后在组建完整的网络。</p>
<h4 id="5-4-1-1-残差单元"><a href="#5-4-1-1-残差单元" class="headerlink" title="5.4.1.1 残差单元"></a>5.4.1.1 残差单元</h4><p>这里，我们实现一个算子<code>ResBlock</code>来构建残差单元，其中定义了<code>use_residual</code>参数，用于在后续实验中控制是否使用残差连接。</p>
<hr>
<p>残差单元包裹的非线性层的输入和输出形状大小应该一致。如果一个卷积层的输入特征图和输出特征图的通道数不一致，则其输出与输入特征图无法直接相加。为了解决上述问题，我们可以使用$1 \times 1$大小的卷积将输入特征图的通道数映射为与级联卷积输出特征图的一致通道数。</p>
<p>$1 \times 1$卷积：与标准卷积完全一样，唯一的特殊点在于卷积核的尺寸是$1 \times 1$，也就是不去考虑输入数据局部信息之间的关系，而把关注点放在不同通道间。通过使用$1 \times 1$卷积，可以起到如下作用：</p>
<ul>
<li>实现信息的跨通道交互与整合。考虑到卷积运算的输入输出都是3个维度（宽、高、多通道），所以$1 \times 1$卷积实际上就是对每个像素点，在不同的通道上进行线性组合，从而整合不同通道的信息；</li>
<li>对卷积核通道数进行降维和升维，减少参数量。经过$1 \times 1$卷积后的输出保留了输入数据的原有平面结构，通过调控通道数，从而完成升维或降维的作用；</li>
<li>利用$1 \times 1$卷积后的非线性激活函数，在保持特征图尺寸不变的前提下，大幅增加非线性。</li>
</ul>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResBlock</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, stride=<span class="number">1</span>, use_residual=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        残差单元</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - in_channels：输入通道数</span></span><br><span class="line"><span class="string">            - out_channels：输出通道数</span></span><br><span class="line"><span class="string">            - stride：残差单元的步长，通过调整残差单元中第一个卷积层的步长来控制</span></span><br><span class="line"><span class="string">            - use_residual：用于控制是否使用残差连接</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ResBlock, self).__init__()</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.use_residual = use_residual</span><br><span class="line">        <span class="comment"># 第一个卷积层，卷积核大小为3×3，可以设置不同输出通道数以及步长</span></span><br><span class="line">        self.conv1 = nn.Conv2D(in_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>, stride=self.stride, bias_attr=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第二个卷积层，卷积核大小为3×3，不改变输入特征图的形状，步长为1</span></span><br><span class="line">        self.conv2 = nn.Conv2D(out_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>, bias_attr=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果conv2的输出和此残差块的输入数据形状不一致，则use_1x1conv = True</span></span><br><span class="line">        <span class="comment"># 当use_1x1conv = True，添加1个1x1的卷积作用在输入数据上，使其形状变成跟conv2一致</span></span><br><span class="line">        <span class="keyword">if</span> in_channels != out_channels <span class="keyword">or</span> stride != <span class="number">1</span>:</span><br><span class="line">            self.use_1x1conv = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.use_1x1conv = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 当残差单元包裹的非线性层输入和输出通道数不一致时，需要用1×1卷积调整通道数后再进行相加运算</span></span><br><span class="line">        <span class="keyword">if</span> self.use_1x1conv:</span><br><span class="line">            self.shortcut = nn.Conv2D(in_channels, out_channels, <span class="number">1</span>, stride=self.stride, bias_attr=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个卷积层后会接一个批量规范化层，批量规范化的内容在7.5.1中会进行详细介绍</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2D(out_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2D(out_channels)</span><br><span class="line">        <span class="keyword">if</span> self.use_1x1conv:</span><br><span class="line">            self.bn3 = nn.BatchNorm2D(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        y = F.relu(self.bn1(self.conv1(inputs)))</span><br><span class="line">        y = self.bn2(self.conv2(y))</span><br><span class="line">        <span class="keyword">if</span> self.use_residual:</span><br><span class="line">            <span class="keyword">if</span> self.use_1x1conv:  <span class="comment"># 如果为真，对inputs进行1×1卷积，将形状调整成跟conv2的输出y一致</span></span><br><span class="line">                shortcut = self.shortcut(inputs)</span><br><span class="line">                shortcut = self.bn3(shortcut)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 否则直接将inputs和conv2的输出y相加</span></span><br><span class="line">                shortcut = inputs</span><br><span class="line">            y = paddle.add(shortcut, y)</span><br><span class="line">        out = F.relu(y)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="5-4-1-2-残差网络的整体结构"><a href="#5-4-1-2-残差网络的整体结构" class="headerlink" title="5.4.1.2 残差网络的整体结构"></a>5.4.1.2 残差网络的整体结构</h4><p>残差网络就是将很多个残差单元串联起来构成的一个非常深的网络。ResNet18 的网络结构如<strong>图5.16</strong>所示。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/16cb9fe0d0704940a829be1722fd6273b85209f198d143efb221f760d5f87f64" width = "700"></center>
<center><br>图5.16：残差网络</br></center>

<p>其中为了便于理解，可以将ResNet18网络划分为6个模块：</p>
<ul>
<li>第一模块：包含了一个步长为2，大小为$7 \times 7$的卷积层，卷积层的输出通道数为64，卷积层的输出经过批量归一化、ReLU激活函数的处理后，接了一个步长为2的$3 \times 3$的最大汇聚层；</li>
<li>第二模块：包含了两个残差单元，经过运算后，输出通道数为64，特征图的尺寸保持不变；</li>
<li>第三模块：包含了两个残差单元，经过运算后，输出通道数为128，特征图的尺寸缩小一半；</li>
<li>第四模块：包含了两个残差单元，经过运算后，输出通道数为256，特征图的尺寸缩小一半；</li>
<li>第五模块：包含了两个残差单元，经过运算后，输出通道数为512，特征图的尺寸缩小一半；</li>
<li>第六模块：包含了一个全局平均汇聚层，将特征图变为$1 \times 1$的大小，最终经过全连接层计算出最后的输出。</li>
</ul>
<p>ResNet18模型的代码实现如下：</p>
<p>定义模块一。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_first_module</span>(<span class="params">in_channels</span>):</span><br><span class="line">    <span class="comment"># 模块一：7*7卷积、批量规范化、汇聚</span></span><br><span class="line">    m1 = nn.Sequential(nn.Conv2D(in_channels, <span class="number">64</span>, <span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                    nn.BatchNorm2D(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                    nn.MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> m1</span><br></pre></td></tr></table></figure>
<p>定义模块二到模块五。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_module</span>(<span class="params">input_channels, out_channels, num_res_blocks, stride=<span class="number">1</span>, use_residual=<span class="literal">True</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="comment"># 根据num_res_blocks，循环生成残差单元</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>: <span class="comment"># 创建模块中的第一个残差单元</span></span><br><span class="line">            blk.append(ResBlock(input_channels, out_channels,</span><br><span class="line">                                stride=stride, use_residual=use_residual))</span><br><span class="line">        <span class="keyword">else</span>:      <span class="comment"># 创建模块中的其他残差单元</span></span><br><span class="line">            blk.append(ResBlock(out_channels, out_channels, use_residual=use_residual))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<p>封装模块二到模块五。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_modules</span>(<span class="params">use_residual</span>):</span><br><span class="line">    <span class="comment"># 模块二：包含两个残差单元，输入通道数为64，输出通道数为64，步长为1，特征图大小保持不变</span></span><br><span class="line">    m2 = nn.Sequential(*resnet_module(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, stride=<span class="number">1</span>, use_residual=use_residual))</span><br><span class="line">    <span class="comment"># 模块三：包含两个残差单元，输入通道数为64，输出通道数为128，步长为2，特征图大小缩小一半。</span></span><br><span class="line">    m3 = nn.Sequential(*resnet_module(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>, stride=<span class="number">2</span>, use_residual=use_residual))</span><br><span class="line">    <span class="comment"># 模块四：包含两个残差单元，输入通道数为128，输出通道数为256，步长为2，特征图大小缩小一半。</span></span><br><span class="line">    m4 = nn.Sequential(*resnet_module(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>, stride=<span class="number">2</span>, use_residual=use_residual))</span><br><span class="line">    <span class="comment"># 模块五：包含两个残差单元，输入通道数为256，输出通道数为512，步长为2，特征图大小缩小一半。</span></span><br><span class="line">    m5 = nn.Sequential(*resnet_module(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>, stride=<span class="number">2</span>, use_residual=use_residual))</span><br><span class="line">    <span class="keyword">return</span> m2, m3, m4, m5</span><br></pre></td></tr></table></figure>
<p>定义完整网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义完整网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model_ResNet18</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels=<span class="number">3</span>, num_classes=<span class="number">10</span>, use_residual=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Model_ResNet18,self).__init__()</span><br><span class="line">        m1 = make_first_module(in_channels)</span><br><span class="line">        m2, m3, m4, m5 = make_modules(use_residual)</span><br><span class="line">        <span class="comment"># 封装模块一到模块6</span></span><br><span class="line">        self.net = nn.Sequential(m1, m2, m3, m4, m5,</span><br><span class="line">                        <span class="comment"># 模块六：汇聚层、全连接层</span></span><br><span class="line">                        nn.AdaptiveAvgPool2D(<span class="number">1</span>), nn.Flatten(), nn.Linear(<span class="number">512</span>, num_classes) )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure>
<p>这里同样可以使用<code>paddle.summary</code>统计模型的参数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Model_ResNet18(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>, use_residual=<span class="literal">True</span>)</span><br><span class="line">params_info = paddle.summary(model, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"><span class="built_in">print</span>(params_info)</span><br></pre></td></tr></table></figure>
<p>使用<code>paddle.flops</code>统计模型的计算量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FLOPs = paddle.flops(model, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>), print_detail=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(FLOPs)</span><br></pre></td></tr></table></figure>
<p>为了验证残差连接对深层卷积神经网络的训练可以起到促进作用，接下来先使用ResNet18（use_residual设置为False）进行手写数字识别实验，再添加残差连接（use_residual设置为True），观察实验对比效果。</p>
<h3 id="5-4-2-没有残差连接的ResNet18"><a href="#5-4-2-没有残差连接的ResNet18" class="headerlink" title="5.4.2 没有残差连接的ResNet18"></a>5.4.2 没有残差连接的ResNet18</h3><p>为了验证残差连接的效果，先使用没有残差连接的ResNet18进行实验。</p>
<h4 id="5-4-2-1-模型训练"><a href="#5-4-2-1-模型训练" class="headerlink" title="5.4.2.1 模型训练"></a>5.4.2.1 模型训练</h4><p>使用训练集和验证集进行模型训练，共训练5个epoch。在实验中，保存准确率最高的模型作为最佳模型。代码实现如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> plot</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 学习率大小</span></span><br><span class="line">lr = <span class="number">0.005</span>  </span><br><span class="line"><span class="comment"># 批次大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)</span><br><span class="line">test_loader = io.DataLoader(test_dataset, batch_size=batch_size)</span><br><span class="line"><span class="comment"># 定义网络，不使用残差结构的深层网络</span></span><br><span class="line">model = Model_ResNet18(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>, use_residual=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())</span><br><span class="line"><span class="comment"># 实例化RunnerV3</span></span><br><span class="line">runner = RunnerV3(model, optimizer, loss_fn, metric)</span><br><span class="line"><span class="comment"># 启动训练</span></span><br><span class="line">log_steps = <span class="number">15</span></span><br><span class="line">eval_steps = <span class="number">15</span></span><br><span class="line">runner.train(train_loader, dev_loader, num_epochs=<span class="number">5</span>, log_steps=log_steps, </span><br><span class="line">            eval_steps=eval_steps, save_path=<span class="string">&quot;best_model.pdparams&quot;</span>)</span><br><span class="line"><span class="comment"># 可视化观察训练集与验证集的Loss变化情况</span></span><br><span class="line">plot_training_loss_acc(runner, <span class="string">&#x27;cnn-loss2.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="5-4-2-2-模型评价"><a href="#5-4-2-2-模型评价" class="headerlink" title="5.4.2.2 模型评价"></a>5.4.2.2 模型评价</h4><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率以及损失情况。代码实现如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载最优模型</span></span><br><span class="line">runner.load_model(<span class="string">&#x27;best_model.pdparams&#x27;</span>)</span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line">score, loss = runner.evaluate(test_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(score, loss))</span><br></pre></td></tr></table></figure>
<p>从输出结果看，对比LeNet-5模型评价实验结果，网络层级加深后，训练效果不升反降。</p>
<h3 id="5-4-3-带残差连接的ResNet18"><a href="#5-4-3-带残差连接的ResNet18" class="headerlink" title="5.4.3 带残差连接的ResNet18"></a>5.4.3 带残差连接的ResNet18</h3><h4 id="5-4-3-1-模型训练"><a href="#5-4-3-1-模型训练" class="headerlink" title="5.4.3.1 模型训练"></a>5.4.3.1 模型训练</h4><p>使用带残差连接的ResNet18重复上面的实验，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 学习率大小</span></span><br><span class="line">lr = <span class="number">0.01</span>  </span><br><span class="line"><span class="comment"># 批次大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)</span><br><span class="line">test_loader = io.DataLoader(test_dataset, batch_size=batch_size)</span><br><span class="line"><span class="comment"># 定义网络，通过指定use_residual为True，使用残差结构的深层网络</span></span><br><span class="line">model = Model_ResNet18(in_channels=<span class="number">1</span>, num_classes=<span class="number">10</span>, use_residual=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())</span><br><span class="line"><span class="comment"># 实例化RunnerV3</span></span><br><span class="line">runner = RunnerV3(model, optimizer, loss_fn, metric)</span><br><span class="line"><span class="comment"># 启动训练</span></span><br><span class="line">log_steps = <span class="number">15</span></span><br><span class="line">eval_steps = <span class="number">15</span></span><br><span class="line">runner.train(train_loader, dev_loader, num_epochs=<span class="number">5</span>, log_steps=log_steps, </span><br><span class="line">            eval_steps=eval_steps, save_path=<span class="string">&quot;best_model.pdparams&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化观察训练集与验证集的Loss变化情况</span></span><br><span class="line">plot_training_loss_acc(runner, <span class="string">&#x27;cnn-loss3.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="5-4-3-2-模型评价"><a href="#5-4-3-2-模型评价" class="headerlink" title="5.4.3.2 模型评价"></a>5.4.3.2 模型评价</h4><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率以及损失情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载最优模型</span></span><br><span class="line">runner.load_model(<span class="string">&#x27;best_model.pdparams&#x27;</span>)</span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line">score, loss = runner.evaluate(test_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(score, loss))</span><br></pre></td></tr></table></figure>
<p>添加了残差连接后，模型收敛曲线更平滑。<br>从输出结果看，和不使用残差连接的ResNet相比，添加了残差连接后，模型效果有了一定的提升。</p>
<h3 id="5-4-4-与高层API实现版本的对比实验"><a href="#5-4-4-与高层API实现版本的对比实验" class="headerlink" title="5.4.4 与高层API实现版本的对比实验"></a>5.4.4 与高层API实现版本的对比实验</h3><p>对于Reset18这种比较经典的图像分类网络，飞桨高层API中都为大家提供了实现好的版本，大家可以不再从头开始实现。这里为高层API版本的resnet18模型和自定义的resnet18模型赋予相同的权重，并使用相同的输入数据，观察输出结果是否一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.vision.models <span class="keyword">import</span> resnet18</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="comment">#warnings.filterwarnings(&quot;ignore&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用飞桨HAPI中实现的resnet18模型，该模型默认输入通道数为3，输出类别数1000</span></span><br><span class="line">hapi_model = resnet18()</span><br><span class="line"><span class="comment"># 自定义的resnet18模型</span></span><br><span class="line">model = Model_ResNet18(in_channels=<span class="number">3</span>, num_classes=<span class="number">1000</span>, use_residual=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取网络的权重</span></span><br><span class="line">params = hapi_model.state_dict()</span><br><span class="line"><span class="comment"># 用来保存参数名映射后的网络权重</span></span><br><span class="line">new_params = &#123;&#125;</span><br><span class="line"><span class="comment"># 将参数名进行映射</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> params:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;layer&#x27;</span> <span class="keyword">in</span> key:</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;downsample.0&#x27;</span> <span class="keyword">in</span> key:</span><br><span class="line">            new_params[<span class="string">&#x27;net.&#x27;</span> + key[<span class="number">5</span>:<span class="number">8</span>] + <span class="string">&#x27;.shortcut&#x27;</span> + key[-<span class="number">7</span>:]] = params[key]</span><br><span class="line">        <span class="keyword">elif</span> <span class="string">&#x27;downsample.1&#x27;</span> <span class="keyword">in</span> key:</span><br><span class="line">            new_params[<span class="string">&#x27;net.&#x27;</span> + key[<span class="number">5</span>:<span class="number">8</span>] + <span class="string">&#x27;.shorcutt&#x27;</span> + key[<span class="number">23</span>:]] = params[key]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_params[<span class="string">&#x27;net.&#x27;</span> + key[<span class="number">5</span>:]] = params[key]</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;conv1.weight&#x27;</span> == key:</span><br><span class="line">        new_params[<span class="string">&#x27;net.0.0.weight&#x27;</span>] = params[key]</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;bn1&#x27;</span> <span class="keyword">in</span> key:</span><br><span class="line">        new_params[<span class="string">&#x27;net.0.1&#x27;</span> + key[<span class="number">3</span>:]] = params[key]</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;fc&#x27;</span> <span class="keyword">in</span> key:</span><br><span class="line">        new_params[<span class="string">&#x27;net.7&#x27;</span> + key[<span class="number">2</span>:]] = params[key]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将飞桨HAPI中实现的resnet18模型的权重参数赋予自定义的resnet18模型，保持两者一致</span></span><br><span class="line">model.set_state_dict(new_params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里用np.random创建一个随机数组作为测试数据</span></span><br><span class="line">inputs = np.random.randn(*[<span class="number">1</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>])</span><br><span class="line">inputs = inputs.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = paddle.to_tensor(inputs)</span><br><span class="line"></span><br><span class="line">output = model(x)</span><br><span class="line">hapi_out = hapi_model(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算两个模型输出的差异</span></span><br><span class="line">diff = output - hapi_out</span><br><span class="line"><span class="comment"># 取差异最大的值</span></span><br><span class="line">max_diff = paddle.<span class="built_in">max</span>(diff)</span><br><span class="line"><span class="built_in">print</span>(max_diff)</span><br></pre></td></tr></table></figure>
<p>可以看到，高层API版本的resnet18模型和自定义的resnet18模型输出结果是一致的，也就说明两个模型的实现完全一样。</p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/08/12/nndl/chapter5B/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          第5章（下）：基于残差网络完成图像分类任务
        
      </div>
    </a>
  
  
    <a href="/2022/08/12/nndl/chapter4B/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">第4章（下）：基于前馈神经网络完成鸢尾花分类任务</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2022 Pokemon Master
      </div>
        <div class="footer-right">
          <a href="https://space.bilibili.com/782159" target="_blank">访问我的哔哩哔哩</a> 
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: ,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":170,"height":340},"mobile":{"show":false},"log":false});</script></body>
</html>