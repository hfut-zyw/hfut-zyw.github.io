<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第8章（下）：基于自注意力机制完成LCQMC文本语义匹配任务 | Homepage | 张有为</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="8.3 基于自注意力模型的文本语义匹配文本语义匹配 {Text Semantic Matching}是一个十分常见的自然语言处理任务。比如在信息检索、问答系统，文本蕴含等任务中都需要用于文本语义匹配的技术。">
<meta property="og:type" content="article">
<meta property="og:title" content="第8章（下）：基于自注意力机制完成LCQMC文本语义匹配任务">
<meta property="og:url" content="http://example.com/2022/08/12/nndl/chapter8B/index.html">
<meta property="og:site_name" content="Homepage | 张有为">
<meta property="og:description" content="8.3 基于自注意力模型的文本语义匹配文本语义匹配 {Text Semantic Matching}是一个十分常见的自然语言处理任务。比如在信息检索、问答系统，文本蕴含等任务中都需要用于文本语义匹配的技术。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/1b81fa33279a430a972cb30fb5b91143bcd2275f5931437a917a28e0a0e80c88">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/9256f30d032045f2a1fec3c44bd4e7747e4d331a04774d05938db566896aee96">
<meta property="og:image" content="http://example.com/img-nndl/output_22_0.png">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/0ec699f6bdb04e7db58c18be2bbfd41d7b8def8ac9a847ccb7ca75b6726ed538">
<meta property="article:published_time" content="2022-08-12T01:28:00.000Z">
<meta property="article:modified_time" content="2022-09-03T22:14:39.114Z">
<meta property="article:author" content="宝可梦训练师">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ai-studio-static-online.cdn.bcebos.com/1b81fa33279a430a972cb30fb5b91143bcd2275f5931437a917a28e0a0e80c88">
  
    <link rel="alternative" href="/atom.xml" title="Homepage | 张有为" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/icon.gif">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>


  
<meta name="generator" content="Hexo 6.2.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/face.png" class="js-avatar show" style="width: 100%;height: 100%;opacity: 1;">
		</a>

		<hgroup>
			<h1 class="header-author"><a href="/">宝可梦训练师</a></h1>
		</hgroup>

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/2022/08/09/me/aboutme">个人简介</a></li>
				        
							<li><a href="/categories/notes">随写</a></li>
				        
							<li><a href="/categories/pytorch">Pytorch</a></li>
				        
							<li><a href="/categories/opt">优化笔记</a></li>
				        
							<li><a href="/categories/nndl">nndl案例与实践</a></li>
				        
						</ul>
					</nav>
					<nav class="half-header-menu">
						<a class="hide">Home</a>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
					        
						</div>
						<!-- music -->
						
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/face.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/2022/08/09/me/aboutme">个人简介</a></li>
		        
					<li><a href="/categories/notes">随写</a></li>
		        
					<li><a href="/categories/pytorch">Pytorch</a></li>
		        
					<li><a href="/categories/opt">优化笔记</a></li>
		        
					<li><a href="/categories/nndl">nndl案例与实践</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-nndl/chapter8B" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/08/12/nndl/chapter8B/" class="article-date">
  	<time datetime="2022-08-12T01:28:00.000Z" itemprop="datePublished">2022-08-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第8章（下）：基于自注意力机制完成LCQMC文本语义匹配任务
      
          <span class="title-pop-out"></a>
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/nndl/">nndl</a>
	</div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="8-3-基于自注意力模型的文本语义匹配"><a href="#8-3-基于自注意力模型的文本语义匹配" class="headerlink" title="8.3 基于自注意力模型的文本语义匹配"></a>8.3 基于自注意力模型的文本语义匹配</h1><p><strong>文本语义匹配</strong> {Text Semantic Matching}是一个十分常见的自然语言处理任务。比如在信息检索、问答系统，文本蕴含等任务中都需要用于文本语义匹配的技术。<span id="more"></span></p>
<p>比如下面三个句子，句子A和句子C的语义更相似，但是如果通过字符串匹配的方式，A和B更容易被判断为相似。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A. 什么花一年四季都开？</span><br><span class="line">B. 什么花生一年四季都开？</span><br><span class="line">C. 哪些花可以全年开放？</span><br></pre></td></tr></table></figure>
<p>文本语义匹配任务就是希望能从语义上更准确第判断两个句子之间的关系，而不是仅仅通过字符串匹配。<br>令1表示“相似”，0表示“不相似”，我们希望文本语义匹配模型能够达到这样的效果：<br>输入A和B两句话，输出为0,；输入A和C两句话，输出为1。</p>
<p>本实践基于自注意力机制来进行文本语义匹配任务。和前两节不同，这里只是用自注意力模型，而不是将自注意力模型叠加在LSTM模型的上面。<br>这里使用一个非常流行的网络结构Transformer。由于语义匹配是一个分类任务，因此只需要用到Transformer模型的编码器。</p>
<p>基于Transformer编码器的文本语义匹配的整个模型结构如图所示：</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/1b81fa33279a430a972cb30fb5b91143bcd2275f5931437a917a28e0a0e80c88" width="700px"></center>

<p><br><center>图8.15 基于Transformer编码器的文本语义匹配模型结构</center></br></p>
<p>我们首先将两个句子“什么花一年四季都开”“哪些花可以全年开放”进行拼接，构建“[CLS]什么花一年四季都开[SEP]哪些花可以全年开放[SEP]”一个句子，其中“[CLS]”表示整个句子的特征，“[SEP]”表示两个句子的分割。这样我们可以将两个句子的匹配问题转换为一个句子的分类问题。</p>
<p>然后，我们将合并后的句子转换成稠密的特征向量（图中的输入编码），然后加入对应的位置编码和分段编码，位置编码和分段编码也是稠密的特征向量，然后输入编码，位置编码分段编码按照位置相加，最终得到每个字符级别的特征表示，字符级别的表示包含了语义信息（输入编码），当前字符的位置信息(位置编码)和句子的信息（分段编码）。然后经过编码器得到多头自注意力机制后的输出，然后取第0个位置的句子向量接入线性层进行分类，得到最终的分类结果”相似”。</p>
<h2 id="8-3-1-数据准备"><a href="#8-3-1-数据准备" class="headerlink" title="8.3.1 数据准备"></a>8.3.1 数据准备</h2><p>LCQMC是百度知道领域的中文问题匹配数据集，该数据集是从不同领域的用户中提取出来。LCQMC的训练集的数量是    238766条，验证集的大小是4401条，测试集的大小是4401条。其目录结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lcqmc</span><br><span class="line">├── dev.csv  # 验证集</span><br><span class="line">├── test.csv # 测试集</span><br><span class="line">└── train.tsv # 训练集</span><br></pre></td></tr></table></figure>
<p>下面看一下其中的一条数据，数据有三部分组成，前两句是文本，表示的是两句话，第三部分是标签，其中1表示的是两文本是相似的，0表示的是两文本是不相似的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">什么花一年四季都开      什么花一年四季都是开的  1</span><br></pre></td></tr></table></figure>
<h3 id="8-3-1-1-数据加载"><a href="#8-3-1-1-数据加载" class="headerlink" title="8.3.1.1  数据加载"></a>8.3.1.1  数据加载</h3><p>首先加载数据和词表，利用词表是把句子中的每个中文字符转换成ID。注意由于数据集不同，这里使用的词表跟上面文本分类实验里面的的词表不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> utils.data <span class="keyword">import</span> load_vocab,load_lcqmc_data</span><br><span class="line"><span class="comment"># 加载训练集，验证集，测试集</span></span><br><span class="line">train_data, dev_data, test_data=load_lcqmc_data(<span class="string">&#x27;lcqmc&#x27;</span>)</span><br><span class="line"><span class="comment"># 加载词表</span></span><br><span class="line">word2id_dict = load_vocab()</span><br></pre></td></tr></table></figure>
<h3 id="8-3-1-2-构建DataSet"><a href="#8-3-1-2-构建DataSet" class="headerlink" title="8.3.1.2 构建DataSet"></a>8.3.1.2 构建DataSet</h3><p>构造一个LCQMCDataset类，LCQMCDataset继承paddle.io.DataSet类，可以逐个数据进行处理。</p>
<p>LCQMCDataset的作用首先就是把文本根据词表转换成ID的形式，对于一个不在词汇表里面的字，默认会将该词用[UNK]代替，由于输入的是两句话，所以需要加入分隔符号[SEP]，并且在起始位置加入[CLS]占位符来表示语义级别的特征表示。</p>
<p>比如对于前面的例子，我们转换成如下的形式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS]什么花一年四季都开[SEP]什么花一年四季都是开的[SEP]</span><br></pre></td></tr></table></figure>
<p>然后根据词表将每个词转换成相应的ID表示input_ids。除了用分隔符号[SEP]外，对每个词分别加一维特征segment_ids=0,1来区分该词是来自于哪个句子。0表示该词是来自第一个句子；1表示该词是来自第二个句子。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[101, 784, 720, 5709, 671, 2399, 1724, 2108, 6963, 2458, 102, 784, 720, 5709, 671, 2399, 1724, 2108, 6963, 3221, 2458, 4638, 102]</span><br><span class="line">[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LCQMCDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, word2id_dict</span>):</span><br><span class="line">        <span class="comment"># 词表</span></span><br><span class="line">        self.word2id_dict = word2id_dict</span><br><span class="line">        <span class="comment"># 数据</span></span><br><span class="line">        self.examples = data</span><br><span class="line">        <span class="comment"># [&#x27;CLS&#x27;]的id，占位符</span></span><br><span class="line">        self.cls_id = self.word2id_dict[<span class="string">&#x27;[CLS]&#x27;</span>]</span><br><span class="line">        <span class="comment"># [&#x27;SEP&#x27;]的id，句子的分隔</span></span><br><span class="line">        self.sep_id = self.word2id_dict[<span class="string">&#x27;[SEP]&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 返回单条样本</span></span><br><span class="line">        example = self.examples[idx]</span><br><span class="line">        text, segment, label = self.words_to_id(example)</span><br><span class="line">        <span class="keyword">return</span> text, segment, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回样本的个数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.examples)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">words_to_id</span>(<span class="params">self, example</span>):</span><br><span class="line">        text_a, text_b, label = example</span><br><span class="line">        <span class="comment"># text_a 转换成id的形式</span></span><br><span class="line">        input_ids_a = [self.word2id_dict[item] <span class="keyword">if</span> item <span class="keyword">in</span> self.word2id_dict <span class="keyword">else</span> self.word2id_dict[<span class="string">&#x27;[UNK]&#x27;</span>] <span class="keyword">for</span> item <span class="keyword">in</span> text_a]</span><br><span class="line">        <span class="comment"># text_b 转换成id的形式</span></span><br><span class="line">        input_ids_b = [self.word2id_dict[item] <span class="keyword">if</span> item <span class="keyword">in</span> self.word2id_dict <span class="keyword">else</span> self.word2id_dict[<span class="string">&#x27;[UNK]&#x27;</span>] <span class="keyword">for</span> item <span class="keyword">in</span> text_b]</span><br><span class="line">        <span class="comment"># 加入[CLS],[SEP]</span></span><br><span class="line">        input_ids = [self.cls_id]+ input_ids_a + [self.sep_id] + input_ids_b + [self.sep_id]</span><br><span class="line">        <span class="comment"># 对句子text_a,text_b做id的区分，进行的分隔</span></span><br><span class="line">        segment_ids = [<span class="number">0</span>]*(<span class="built_in">len</span>(input_ids_a)+<span class="number">2</span>)+[<span class="number">1</span>]*(<span class="built_in">len</span>(input_ids_b)+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> input_ids, segment_ids, <span class="built_in">int</span>(label)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">label_list</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 0表示不相似，1表示相似</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练集</span></span><br><span class="line">train_dataset = LCQMCDataset(train_data,word2id_dict)</span><br><span class="line"><span class="comment"># 加载验证集</span></span><br><span class="line">dev_dataset = LCQMCDataset(dev_data,word2id_dict)</span><br><span class="line"><span class="comment"># 加载测试集</span></span><br><span class="line">test_dataset = LCQMCDataset(test_data,word2id_dict)</span><br></pre></td></tr></table></figure>
<h3 id="8-3-1-3-构建Dataloader"><a href="#8-3-1-3-构建Dataloader" class="headerlink" title="8.3.1.3 构建Dataloader"></a>8.3.1.3 构建Dataloader</h3><p>构建DataLoader的目的是组装成小批量的数据，在组装小批量的数据之前，首先对文本数据转为ID表示，然后把数据用[PAD]进行对齐。这里我们将[PAD]的ID设为0，因此补齐操作就是把ID序列用0对齐到最大长度。</p>
<p>对数据进行统一格式化后，使用DataLoader组装成小批次的数据迭代器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch_data, pad_val=<span class="number">0</span>, max_seq_len=<span class="number">512</span></span>):</span><br><span class="line">    input_ids, segment_ids, labels = [], [], []</span><br><span class="line">    max_len = <span class="number">0</span></span><br><span class="line">    <span class="comment"># print(batch_data)</span></span><br><span class="line">    <span class="keyword">for</span> example <span class="keyword">in</span> batch_data:</span><br><span class="line">        input_id, segment_id, label = example</span><br><span class="line">        <span class="comment"># 对数据序列进行截断</span></span><br><span class="line">        input_ids.append(input_id[:max_seq_len])</span><br><span class="line">        segment_ids.append(segment_id[:max_seq_len])</span><br><span class="line">        labels.append(label)</span><br><span class="line">        <span class="comment"># 保存序列最大长度</span></span><br><span class="line">        max_len = <span class="built_in">max</span>(max_len, <span class="built_in">len</span>(input_id))</span><br><span class="line">    <span class="comment"># 对数据序列进行填充至最大长度</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">        input_ids[i] = input_ids[i]+[pad_val] * (max_len - <span class="built_in">len</span>(input_ids[i]))</span><br><span class="line">        segment_ids[i] = segment_ids[i]+[pad_val] * (max_len - <span class="built_in">len</span>(segment_ids[i]))</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        paddle.to_tensor(input_ids),</span><br><span class="line">        paddle.to_tensor(segment_ids),</span><br><span class="line">    ), paddle.to_tensor(labels)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"><span class="comment"># 构建训练集,验证集，测试集的dataloader</span></span><br><span class="line">train_loader = DataLoader(</span><br><span class="line">    train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line">dev_loader = DataLoader(</span><br><span class="line">    dev_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line">test_loader = DataLoader(</span><br><span class="line">    test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印输出一条mini-batch的数据</span></span><br><span class="line"><span class="keyword">for</span> idx, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="keyword">if</span> idx == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(item)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>[[Tensor(shape=[32, 52], dtype=int64, place=CUDAPlace(0), stop_gradient=True,
       [[101 , 1599, 3614, ..., 0   , 0   , 0   ],
        [101 , 2769, 2797, ..., 0   , 0   , 0   ],
        [101 , 1920, 2157, ..., 0   , 0   , 0   ],
        ...,
        [101 , 1453, 3345, ..., 0   , 0   , 0   ],
        [101 , 3299, 1159, ..., 5023, 2521, 102 ],
        [101 , 3118, 802 , ..., 0   , 0   , 0   ]]), Tensor(shape=[32, 52], dtype=int64, place=CUDAPlace(0), stop_gradient=True,
       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 1, 1, 1],
        [0, 0, 0, ..., 0, 0, 0]])], Tensor(shape=[32], dtype=int64, place=CUDAPlace(0), stop_gradient=True,
       [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1])]
</code></pre><p>从上面的输出可以看到，第一个张量的形状是[32,52],其中分别是样本数量和句子长度。如果句子本身的长度不够52个字符，则会补0处理；第二个张量的输出的维度是[32,52]，表示的是句子的编码，可以看到两个句子被编码成了只包含0，1的向量；最后一个张量是标签的编码，维度是32，表示有32个标签，1表示的是该句子是相似的，0表示的是该句子是不相似的。</p>
<h2 id="8-3-2-模型构建"><a href="#8-3-2-模型构建" class="headerlink" title="8.3.2  模型构建"></a>8.3.2  模型构建</h2><center><img src="https://ai-studio-static-online.cdn.bcebos.com/9256f30d032045f2a1fec3c44bd4e7747e4d331a04774d05938db566896aee96" width="700px"></center>

<p><br><center>图8.16 文本语义匹配模型结构</center></br></p>
<p>如图8.15所示，基于Transformer编码器的语义匹配模型由以下几个部分组成：</p>
<p> 1） 嵌入层：用于输入的句子中的词语的向量化表示，由于语义匹配任务的输入是两句文本，所以需要分段编码对每个句子进行向量化表示，最后还需要对句子中词语的位置进行向量化表示，即位置编码。</p>
<p> 2） Transformer组块：使用Transformer的编码组块来计算深层次的特征表示。</p>
<p> 3） 线性层：输出层，得到该句子的分类。线性层的输入是第一个位置[CLS]的输出向量。</p>
<h3 id="8-3-2-1-嵌入层"><a href="#8-3-2-1-嵌入层" class="headerlink" title="8.3.2.1 嵌入层"></a>8.3.2.1 嵌入层</h3><p>嵌入层是将输入的文字序列转换为向量序列。这里除了之前的词向量外，我们还需要引入两种编码。</p>
<p>1） <strong>位置编码</strong>{Position Embeddings}：自注意力模块本身无法感知序列的输入顺序信息，即一个词对其它词的影响和它们之间的距离没有关系。因此，自注意力模块通常需要和卷积网络、循环网络一起组合使用。如果单独使用自注意力模块，就需要对其输入表示补充位置信息。位置编码主要是把字的位置信息向量化。</p>
<p>2） <strong>分段编码</strong> {Segment Embeddings}：由于本实践中处理的输入序列是由两个句子拼接而成。为了区分每个词来自于哪个句子，对每个词增加一个0,1分段标记，表示该词是第0或1个句子。分段编码是将分段标记也向量化。</p>
<p>下面我们分别介绍这几种编码。</p>
<p><strong>（1）输入编码</strong></p>
<p><strong>输入编码</strong> {Input Embeddings}把输入的词转化成向量的形式。输入编码看作是一个查表的操作，对于每个单词，要将这些符号转换为向量形式．一种简单的转换方法是通过一个<strong>嵌入表</strong> {Embedding Lookup Table}来将每个符号直接映射成向量表示．这里使用飞桨的paddle.nn.Embedding来根据输入序列中的id信息从嵌入矩阵中查询对应嵌入向量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class paddle.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, sparse=False, weight_attr=None, name=None)</span><br></pre></td></tr></table></figure>
<p><code>paddle.nn.Embedding</code>会根据<code>[num_embeddings, embedding_dim]</code>自动构造一个二维嵌入矩阵。如果配置了<code>padding_idx</code>，那么在训练过程中遇到此id时，其参数及对应的梯度将会以0进行填充。</p>
<p>在Transformer的输入编码的实现中，初始化使用的是随机正态分布，均值为0，标准差为$\frac{1}{\sqrt{\textrm{emb_size}}}$，$\textrm{emb_size}$表示的是词向量的维度。所有的向量都乘以了$\sqrt{\textrm{emb_size}}$。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WordEmbedding</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, emb_size, padding_idx=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(WordEmbedding, self).__init__()</span><br><span class="line">        <span class="comment"># Embedding的维度</span></span><br><span class="line">        self.emb_size = emb_size</span><br><span class="line">        <span class="comment"># 使用随机正态（高斯）分布初始化 embedding</span></span><br><span class="line">        self.word_embedding = nn.Embedding(vocab_size, emb_size,</span><br><span class="line">            padding_idx=padding_idx, weight_attr=paddle.ParamAttr(</span><br><span class="line">                initializer=nn.initializer.Normal(<span class="number">0.0</span>, emb_size ** -<span class="number">0.5</span>) ), )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, word</span>):</span><br><span class="line">        word_emb = self.emb_size ** <span class="number">0.5</span> * self.word_embedding(word)</span><br><span class="line">        <span class="keyword">return</span> word_emb</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">2021</span>)</span><br><span class="line"><span class="comment"># 构造一个输入</span></span><br><span class="line">X = paddle.to_tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 表示构造的输入编码的词汇表的大小是10，每个词的维度是4</span></span><br><span class="line">word_embed = WordEmbedding(<span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入编码为： &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X.numpy()))</span><br><span class="line">word_out = word_embed(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出为： &#123;&#125;&quot;</span>.<span class="built_in">format</span>(word_out.numpy()))</span><br></pre></td></tr></table></figure>
<pre><code>W0529 16:42:54.388808   140 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1
W0529 16:42:54.394099   140 device_context.cc:465] device: 0, cuDNN Version: 7.6.


输入编码为： [1 0 2]
输出为： [[-0.7112208  -0.35037443  0.7261958  -0.31876457]
 [ 0.          0.          0.          0.        ]
 [-0.43065292  0.35489145  1.9781216   0.12072387]]
</code></pre><p><strong>（2）分段编码</strong></p>
<p>分段编码的作用是使得模型能够接受句子对进行训练，用编码的方法使得模型能够区分两个句子。这里指定0来标记句子0，用1标记句子1。对于句子0，创建标记为0的向量，对于句子1，创建标记为1的向量。分段编码的实现跟输入编码类似，不同在于词表大小为2。</p>
<p>以下面的句子为例，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">什么花一年四季都开      什么花一年四季都是开的</span><br></pre></td></tr></table></figure>
<p>我们的分段标记为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0 0 0 0 0 0 0 0 0    1 1 1 1 1 1 1 1 1 1 1</span><br></pre></td></tr></table></figure>
<p>下面我们实现分段编码，将分段标记映射为向量表示。分段编码的维度和输入编码一样。<br>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SegmentEmbedding</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, emb_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(SegmentEmbedding, self).__init__()</span><br><span class="line">        <span class="comment"># Embedding的维度</span></span><br><span class="line">        self.emb_size = emb_size</span><br><span class="line">        <span class="comment"># 分段编码</span></span><br><span class="line">        self.seg_embedding = nn.Embedding(</span><br><span class="line">            num_embeddings=vocab_size, embedding_dim=emb_size</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, word</span>):</span><br><span class="line">        seg_embedding = self.seg_embedding(word)</span><br><span class="line">        <span class="keyword">return</span> seg_embedding</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">2021</span>)</span><br><span class="line"><span class="comment"># 构造一个输入,0表示第0句的token，1表示第1句的token</span></span><br><span class="line">X = paddle.to_tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">word_embed = SegmentEmbedding(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入编码为： &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X.numpy()))</span><br><span class="line">word_out = word_embed(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出为： &#123;&#125;&quot;</span>.<span class="built_in">format</span>(word_out.numpy()))</span><br></pre></td></tr></table></figure>
<pre><code>输入编码为： [0 0 1 1]
输出为： [[-0.90914416  0.7011595  -0.33276933  0.892782  ]
 [-0.90914416  0.7011595  -0.33276933  0.892782  ]
 [-0.52305263 -0.2739423   0.5322813  -0.25009495]
 [-0.52305263 -0.2739423   0.5322813  -0.25009495]]
</code></pre><p><strong>（3）位置编码</strong></p>
<p>为了使得自注意力模块可以感知序列的顺序信息，Transformer给编码层输入添加了一个额外的位置编码。<br>位置编码的目的是在让自注意力模块在计算时引入词之间的距离信息。</p>
<p>下面我们用三角函数（正弦或者余弦）来编码位置信息。假设位置编码的维度为$D$，则其中每一维的值的计算如下：</p>
<script type="math/tex; mode=display">
PE(pos,2i) =\sin\left(\frac{t}{10000^NaN}\right),\\
PE(pos,2i+1) =\cos\left(\frac{t}{10000^NaN}\right),</script><p>其中$t$是指当前词在句子中的位置，$0 \leqslant i\leqslant \frac{D}{2}$为编码向量的维数。在偶数维，使用正弦编码；在奇数维，使用余弦编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"><span class="comment"># position_size 为句子划分成字符或者词的长度，hidden_size为词向量的维度。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_sinusoid_encoding</span>(<span class="params">position_size, hidden_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码 &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cal_angle</span>(<span class="params">pos, hidden_idx</span>):</span><br><span class="line">        <span class="comment"># 公式里的 i = hid_idx // 2</span></span><br><span class="line">        <span class="keyword">return</span> pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * (hidden_idx // <span class="number">2</span>) / hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_posi_angle_vec</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="keyword">return</span> [cal_angle(pos, hidden_j) <span class="keyword">for</span> hidden_j <span class="keyword">in</span> <span class="built_in">range</span>(hidden_size)]</span><br><span class="line"></span><br><span class="line">    sinusoid = np.array([get_posi_angle_vec(pos_i) <span class="keyword">for</span> pos_i <span class="keyword">in</span> <span class="built_in">range</span>(position_size)])</span><br><span class="line">    <span class="comment"># dim 2i  偶数正弦</span></span><br><span class="line">    <span class="comment"># 从0开始，每隔2间隔求正弦值</span></span><br><span class="line">    sinusoid[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(sinusoid[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">    <span class="comment"># dim 2i 1  奇数余弦</span></span><br><span class="line">    <span class="comment"># 从1开始，每隔2间隔取余弦</span></span><br><span class="line">    sinusoid[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(sinusoid[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line">    <span class="comment"># position_size × hidden_size  得到每一个词的位置向量</span></span><br><span class="line">    <span class="keyword">return</span> sinusoid.astype(<span class="string">&quot;float32&quot;</span>)</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">2021</span>)</span><br><span class="line">position_size = <span class="number">4</span></span><br><span class="line">hidden_size = <span class="number">3</span></span><br><span class="line">encoding_vec=get_sinusoid_encoding(position_size, hidden_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;位置编码的输出为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(encoding_vec))</span><br></pre></td></tr></table></figure>
<pre><code>位置编码的输出为：[[ 0.          1.          0.        ]
 [ 0.84147096  0.5403023   0.00215443]
 [ 0.9092974  -0.41614684  0.00430886]
 [ 0.14112    -0.9899925   0.00646326]]
</code></pre><p>利用上面的三角函数来实现位置编码，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEmbedding</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, max_length,emb_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEmbedding, self).__init__()</span><br><span class="line">        self.emb_size = emb_size</span><br><span class="line">        <span class="comment"># 使用三角函数初始化Embedding</span></span><br><span class="line">        self.pos_encoder = nn.Embedding(</span><br><span class="line">            num_embeddings=max_length,</span><br><span class="line">            embedding_dim=self.emb_size,</span><br><span class="line">            weight_attr=paddle.ParamAttr(</span><br><span class="line">                initializer=paddle.nn.initializer.Assign(</span><br><span class="line">                    get_sinusoid_encoding(max_length, self.emb_size))))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos</span>):</span><br><span class="line">        pos_emb = self.pos_encoder(pos)</span><br><span class="line">        <span class="comment"># 关闭位置编码的梯度更新</span></span><br><span class="line">        pos_emb.stop_gradient = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> pos_emb</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">2021</span>)</span><br><span class="line">out = paddle.randint(low=<span class="number">0</span>, high=<span class="number">5</span>, shape=[<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;输入向量为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(out.numpy()))</span><br><span class="line">pos_embed=PositionalEmbedding(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">pos_out=pos_embed(out)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;位置编码的输出为： &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(pos_out.numpy()))</span><br></pre></td></tr></table></figure>
<pre><code>输入向量为：[1 0 1]
位置编码的输出为： [[8.4147096e-01 5.4030228e-01 2.5116222e-02 9.9968451e-01 6.3095731e-04]
 [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00]
 [8.4147096e-01 5.4030228e-01 2.5116222e-02 9.9968451e-01 6.3095731e-04]]
</code></pre><p>为了对使用三角函数的位置编码有个直观了解，这里对三角函数初始化的值进行可视化，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_curve</span>(<span class="params">size,y</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">    plt.plot(np.arange(size), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">5</span>].numpy(),color=<span class="string">&#x27;#E20079&#x27;</span>,linestyle=<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">    plt.plot(np.arange(size), y[<span class="number">0</span>, :, <span class="number">5</span>:<span class="number">6</span>].numpy(),color=<span class="string">&#x27;#8E004D&#x27;</span>,linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    plt.plot(np.arange(size), y[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">7</span>].numpy(),color=<span class="string">&#x27;#3D3D3F&#x27;</span>,linestyle=<span class="string">&#x27;-.&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&quot;dim %d&quot;</span>%p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]], fontsize=<span class="string">&#x27;large&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;att-vis2.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = PositionalEmbedding(emb_size=<span class="number">20</span>, max_length=<span class="number">5000</span>)</span><br><span class="line"><span class="comment"># 生成0~99这100个数，表示0~99这100个位置</span></span><br><span class="line">size = <span class="number">100</span></span><br><span class="line">X= paddle.arange((size)).reshape([<span class="number">1</span>,size])</span><br><span class="line"><span class="comment"># 对这100个位置进行编码，得到每个位置的向量表示</span></span><br><span class="line"><span class="comment"># y: [1,100,20]</span></span><br><span class="line">y = model(X)</span><br><span class="line"><span class="comment"># 把这100个位置的第4，5，6列的数据可视化出来</span></span><br><span class="line">plot_curve(size,y)</span><br></pre></td></tr></table></figure>
<p>​<br><img src="/img-nndl/output_22_0.png" alt="png"><br>​    </p>
<p>可以看到，位置编码本质上是一个和位置相关的正弦曲线，每个维度的正弦波的频率和大小不一样，取值范围在$[-1,1]$之间。</p>
<hr>
<p>基于三角函数的位置编码，计算任意两个位置的点积，观察是否可以推断出两个位置的距离信息？</p>
<hr>
<hr>
<p>总结基于三角函数的位置编码的优缺点，并思考更好的位置编码方式</p>
<hr>
<p><strong>（4）嵌入层汇总</strong></p>
<p>最后，我们把输入编码，分段编码和位置编码进行相加，并对加和后的向量进行层规范化和暂退操作，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEmbeddings</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    包括输入编码，分段编码，位置编码</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        vocab_size,</span></span><br><span class="line"><span class="params">        hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">        hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        position_size=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">        segment_size=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEmbeddings, self).__init__()</span><br><span class="line">        <span class="comment"># 输入编码向量</span></span><br><span class="line">        self.word_embeddings = WordEmbedding(vocab_size, hidden_size)</span><br><span class="line">        <span class="comment"># 位置编码向量</span></span><br><span class="line">        self.position_embeddings = PositionalEmbedding(position_size, hidden_size)</span><br><span class="line">        <span class="comment"># 分段编码</span></span><br><span class="line">        self.segment_embeddings = SegmentEmbedding(segment_size, hidden_size)</span><br><span class="line">        <span class="comment"># 层规范化</span></span><br><span class="line">        self.layer_norm = nn.LayerNorm(hidden_size)</span><br><span class="line">        <span class="comment"># Dropout操作</span></span><br><span class="line">        self.dropout = nn.Dropout(hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, segment_ids = <span class="literal">None</span>, position_ids = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 初始化全1的向量，比如[1,1,1,1]</span></span><br><span class="line">            ones = paddle.ones_like(input_ids, dtype=<span class="string">&quot;int64&quot;</span>)</span><br><span class="line">            <span class="comment"># 累加输入,求出序列前K个的长度,比如[1,2,3,4]</span></span><br><span class="line">            seq_length = paddle.cumsum(ones, axis=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># position id的形式： 比如[0,1,2,3]</span></span><br><span class="line">            position_ids = seq_length - ones</span><br><span class="line">            position_ids.stop_gradient = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 输入编码</span></span><br><span class="line">        input_embedings = self.word_embeddings(input_ids)</span><br><span class="line">        <span class="comment"># 分段编码</span></span><br><span class="line">        segment_embeddings = self.segment_embeddings(segment_ids)</span><br><span class="line">        <span class="comment"># 位置编码</span></span><br><span class="line">        position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">        <span class="comment"># 输入张量, 分段张量，位置张量进行叠加</span></span><br><span class="line">        embeddings = input_embedings + segment_embeddings + position_embeddings</span><br><span class="line">        <span class="comment"># 层规范化</span></span><br><span class="line">        embeddings = self.layer_norm(embeddings)</span><br><span class="line">        <span class="comment"># Dropout</span></span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>
<h3 id="8-3-2-2-Transformer组块"><a href="#8-3-2-2-Transformer组块" class="headerlink" title="8.3.2.2 Transformer组块"></a>8.3.2.2 Transformer组块</h3><p>Transformer编码器由多个Transformer组块叠加而成。一个Transformer组块的结构如图8.17所示，共包含四个模块：多头注意力层、加与规范化层、前馈层、加与规范化层。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/0ec699f6bdb04e7db58c18be2bbfd41d7b8def8ac9a847ccb7ca75b6726ed538" width="500px"></center>

<p><br><center>图8.17 Transformer组块结构</center></br></p>
<p>下面我们分别实现这几个层。</p>
<p><strong>多头自注意力层</strong></p>
<p>多头自注意力直接使用在第8.2.1.3中定义的<code>MultiHeadSelfAttention</code>算子。</p>
<p><strong>加与规范层</strong></p>
<p><strong>加与规范</strong>{Add\&amp;Norm}层主要功能是加入<strong>残差连接</strong>{Residual Connection}与<strong>层规范化</strong>{Layer Normalization}两个组件，使得网络可以更好地训练。残差连接有助于避免深度网络中的梯度消失问题，而层规范化保证数据特征分布的稳定性，网络的收敛性更好。</p>
<p>在Transformer组块，有两个地方使用了加与规范层，这里是第一次使用。</p>
<p>假设多头自注意力层的输入和输出分别为$\mathbf X\in \mathbb{R}^{B \times L \times D}$和$\mathbf H\in \mathbb{R}^{B \times L \times D}$，加与规范层可以写为</p>
<script type="math/tex; mode=display">
\mathbf H=\mathrm{LN}(\mathbf H+\mathbf X),</script><p>$\mathrm{LN(\cdot)}$表示层规范化操作。</p>
<p>加与规范层的实现如下，这里还对$\mathbf H$进行了Dropout操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddNorm</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加与规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout_rate</span>):</span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__()</span><br><span class="line">        self.layer_norm = nn.LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, H</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            X：表示被包裹的非线性层的输入</span></span><br><span class="line"><span class="string">            H：表示被包裹的非线性层的输出</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        H = X + self.dropout(H)</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(H)</span><br></pre></td></tr></table></figure>
<p>  在Transformer的具体实现中，层规范化操作的位置有两种，分别称为PreNorm和PostNorm。假设要给非线性$\mathbf H=f(\mathbf X)$加上加与规范层，则PreNorm和PostNorm分别定义为：</p>
<script type="math/tex; mode=display">
  \mathrm{PreNorm} : \mathbf H=\mathbf X + f(\mathrm{LN}(\mathbf X) ) \\
  \mathrm{PostNorm} : \mathbf H=\mathrm{LN} (f(\mathbf X)+\mathbf X))</script><p>很多研究表明， PreNorm更容易训练，但PostNorm上限更佳。请分析其背后的原因。</p>
<p><strong>逐位前馈层</strong></p>
<p>逐位前馈层（Position-wised Feed Forward Networks，FFN）是两层全连接神经网络，使用ReLU激活函数。将每个位置的特征表示进行融合变换，类似于更复杂的核大小为1的“卷积”。</p>
<p>假设逐位前馈层的输入为张量$\mathbf H \in \mathbb{R}^{B \times L \times D}$，其中$B,L,D$分别表示输入张量的批量大小、序列长度和特征维度，则前馈层的计算公式为：</p>
<script type="math/tex; mode=display">
\mathrm{FNN}(\mathbf H)=\max(0,\mathbf H\mathbf W_1+\mathbf b_1)\mathbf W_2+\mathbf b_2,</script><p>其中$\mathbf W_{1} \in \mathbb{R}^{ D \times D’}$，$\mathbf W_{2} \in \mathbb{R}^{D’\times D}$，$\mathbf b_{1} \in \mathbb{R}^{D’}$，$\mathbf b_{2} \in \mathbb{R}^{D}$是可学习的参数，</p>
<p>逐位前馈层的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFFN</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;逐位前馈层&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, mid_size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFFN, self).__init__()</span><br><span class="line">        self.W_1 = nn.Linear(input_size, mid_size)</span><br><span class="line">        self.W_2 = nn.Linear(mid_size, input_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.W_2(self.dropout(F.relu(self.W_1(X))))</span><br></pre></td></tr></table></figure>
<p><strong>加与规范层</strong></p>
<p>逐位前馈层之后是第二个加与规范层，实现和第8.3.2.2<br>节中一样，这里就不再重复。</p>
<p><strong>Transformer组块汇总</strong></p>
<p>汇总上面的的4个模块，构建Transformer组块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        input_size,</span></span><br><span class="line"><span class="params">        head_num,</span></span><br><span class="line"><span class="params">        ffn_size,</span></span><br><span class="line"><span class="params">        dropout=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        attn_dropout=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        act_dropout=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerBlock, self).__init__()</span><br><span class="line">        <span class="comment"># 输入数据的维度</span></span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        <span class="comment"># 多头自注意力多头的个数</span></span><br><span class="line">        self.head_num = head_num</span><br><span class="line">        <span class="comment"># 逐位前馈层的大小</span></span><br><span class="line">        self.ffn_size = ffn_size</span><br><span class="line">        <span class="comment"># 加与规范化里面 Dropout的参数</span></span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        <span class="comment"># 多头注意力里面的 Dropout参数</span></span><br><span class="line">        self.attn_dropout = dropout <span class="keyword">if</span> attn_dropout <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> attn_dropout</span><br><span class="line">        <span class="comment"># 逐位前馈层里面的 Dropout参数</span></span><br><span class="line">        self.act_dropout = dropout <span class="keyword">if</span> act_dropout <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> act_dropout</span><br><span class="line">        <span class="comment"># 多头自注意力机制</span></span><br><span class="line">        self.multi_head_attention = nn.MultiHeadAttention(</span><br><span class="line">            self.input_size,</span><br><span class="line">            self.head_num,</span><br><span class="line">            dropout=self.attn_dropout,</span><br><span class="line">            need_weights=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 逐位前馈层</span></span><br><span class="line">        self.ffn = PositionwiseFFN(self.input_size, self.ffn_size, self.act_dropout)</span><br><span class="line">        <span class="comment"># 加与规范化</span></span><br><span class="line">        self.addnorm = AddNorm(self.input_size, self.dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, src_mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 多头注意力</span></span><br><span class="line">        X_atten, atten_weights = self.multi_head_attention(X, attn_mask=src_mask)</span><br><span class="line">        <span class="comment"># 加与规范化</span></span><br><span class="line">        X = self.addnorm(X, X_atten)</span><br><span class="line">        <span class="comment"># 前馈层</span></span><br><span class="line">        X_ffn = self.ffn(X)</span><br><span class="line">        <span class="comment"># 加与规范化</span></span><br><span class="line">        X = self.addnorm(X, X_ffn)</span><br><span class="line">        <span class="keyword">return</span> X, atten_weights</span><br></pre></td></tr></table></figure>
<h3 id="8-3-2-3-模型汇总"><a href="#8-3-2-3-模型汇总" class="headerlink" title="8.3.2.3 模型汇总"></a>8.3.2.3 模型汇总</h3><p>接下来，我们将嵌入层、Transformer组件、线性输出层进行组合，构建Transformer模型。</p>
<p>Transformer模型主要是输入编码，分段编码，位置编码，Transformer编码器和最后的全连接分类器，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model_Transformer</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        vocab_size,</span></span><br><span class="line"><span class="params">        n_block=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">        hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">        heads_num=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">        intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">        hidden_dropout=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        attention_dropout=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        act_dropout=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">        position_size=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">        num_classes=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">        padding_idx=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(Model_Transformer, self).__init__()</span><br><span class="line">        <span class="comment"># 词表大小</span></span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        <span class="comment"># Transformer的编码器的数目</span></span><br><span class="line">        self.n_block = n_block</span><br><span class="line">        <span class="comment"># 每个词映射成稠密向量的维度</span></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        <span class="comment"># 多头注意力的个数</span></span><br><span class="line">        self.heads_num = heads_num</span><br><span class="line">        <span class="comment"># 逐位前馈层的的维度</span></span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        <span class="comment"># Embedding层的 Dropout</span></span><br><span class="line">        self.hidden_dropout = hidden_dropout</span><br><span class="line">        <span class="comment"># 多头注意力的dropout的 dropout参数</span></span><br><span class="line">        self.attention_dropout = attention_dropout</span><br><span class="line">        <span class="comment"># 位置编码的大小 position_size</span></span><br><span class="line">        self.position_size = position_size</span><br><span class="line">        <span class="comment"># 类别数</span></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        <span class="comment"># 逐位前馈层的dropout</span></span><br><span class="line">        self.act_dropout = act_dropout</span><br><span class="line">        <span class="comment"># [PAD]字符的ID</span></span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        <span class="comment"># 实例化输入编码，分段编码和位置编码</span></span><br><span class="line">        self.embeddings = TransformerEmbeddings(</span><br><span class="line">            self.vocab_size, self.hidden_size, self.hidden_dropout, self.position_size )</span><br><span class="line">        <span class="comment"># 实例化Transformer的编码器</span></span><br><span class="line">        self.layers = nn.LayerList([])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_block):</span><br><span class="line">            encoder_layer = TransformerBlock(</span><br><span class="line">                hidden_size,</span><br><span class="line">                heads_num,</span><br><span class="line">                intermediate_size,</span><br><span class="line">                dropout=hidden_dropout,</span><br><span class="line">                attn_dropout=attention_dropout,</span><br><span class="line">                act_dropout=act_dropout,</span><br><span class="line">            )</span><br><span class="line">            self.layers.append(encoder_layer)</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        self.dense = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        <span class="comment"># 双曲正切激活函数</span></span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line">        <span class="comment"># 最后一层分类器</span></span><br><span class="line">        self.classifier = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, position_ids=<span class="literal">None</span>, attention_mask=<span class="literal">None</span></span>):</span><br><span class="line">        input_ids, segment_ids = inputs</span><br><span class="line">        <span class="comment"># 构建Mask矩阵，把Pad的位置即input_ids中为0的位置设置为True,非0的位置设置为False</span></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attention_mask = paddle.unsqueeze(</span><br><span class="line">                (input_ids == self.padding_idx).astype(<span class="string">&quot;float32&quot;</span>) * -<span class="number">1e9</span>, axis=[<span class="number">1</span>, <span class="number">2</span>] )</span><br><span class="line">        <span class="comment"># 抽取特征向量</span></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids=input_ids, position_ids=position_ids, segment_ids=segment_ids )</span><br><span class="line">        sequence_output = embedding_output</span><br><span class="line">        self._attention_weights = []</span><br><span class="line">        <span class="comment"># Transformer的输出和注意力权重的输出</span></span><br><span class="line">        <span class="keyword">for</span> i, encoder_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layers):</span><br><span class="line">            sequence_output, atten_weights = encoder_layer(</span><br><span class="line">                sequence_output, src_mask=attention_mask )</span><br><span class="line">            self._attention_weights.append(atten_weights)</span><br><span class="line">        <span class="comment"># 选择第0个位置的向量作为句向量</span></span><br><span class="line">        first_token_tensor = sequence_output[:, <span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="comment"># 句子级别的输出经过分类器</span></span><br><span class="line">        logits = self.classifier(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure>
<p>在模型构建完成之后，我们使用RunnerV3类来进行模型的训练、评价、预测等过程。</p>
<h2 id="8-3-3-模型训练"><a href="#8-3-3-模型训练" class="headerlink" title="8.3.3 模型训练"></a>8.3.3 模型训练</h2><p>模型的训练配置的设置，包括一些超参数、优化器、损失函数等等，训练过程：模型的训练从Dataloader中取数据进行前向反向训练，每隔100个step输出一次日志，每隔500个step在验证集合上计算一次准确率。训练3个回合，并保存在验证集上最佳准确率的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> Accuracy, RunnerV3</span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">paddle.seed(<span class="number">2021</span>)</span><br><span class="line">heads_num = <span class="number">4</span></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">vocab_size=<span class="number">21128</span></span><br><span class="line">num_classes= <span class="number">2</span></span><br><span class="line">padding_idx=word2id_dict[<span class="string">&#x27;[PAD]&#x27;</span>]</span><br><span class="line"><span class="comment"># 注意力多头的数目</span></span><br><span class="line"><span class="comment"># 交叉熵损失</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 评估的时候采用准确率指标</span></span><br><span class="line">metric = Accuracy()</span><br><span class="line"><span class="comment"># Transformer的分类模型</span></span><br><span class="line">model = Model_Transformer(</span><br><span class="line">    vocab_size=vocab_size,</span><br><span class="line">    n_block=<span class="number">1</span>,</span><br><span class="line">    num_classes=num_classes,</span><br><span class="line">    heads_num=heads_num,</span><br><span class="line">    padding_idx=padding_idx,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 排除所有的偏置和LayerNorm的参数</span></span><br><span class="line">decay_params = [</span><br><span class="line">    p.name <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> [<span class="string">&quot;bias&quot;</span>, <span class="string">&quot;norm&quot;</span>])</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 Optimizer</span></span><br><span class="line">optimizer = paddle.optimizer.AdamW(</span><br><span class="line">    learning_rate=<span class="number">5E-5</span>,</span><br><span class="line">    parameters=model.parameters(),</span><br><span class="line">    weight_decay=<span class="number">0.0</span>,</span><br><span class="line">    apply_decay_param_fun=<span class="keyword">lambda</span> x: x <span class="keyword">in</span> decay_params)</span><br><span class="line"></span><br><span class="line">runner = RunnerV3(model, optimizer, criterion, metric)</span><br><span class="line">save_path = <span class="string">&quot;./checkpoint/model_best.pdparams&quot;</span></span><br><span class="line">runner.train(</span><br><span class="line">    train_loader,</span><br><span class="line">    dev_loader,</span><br><span class="line">    num_epochs=epochs,</span><br><span class="line">    log_steps=<span class="number">100</span>,</span><br><span class="line">    eval_steps=<span class="number">500</span>,</span><br><span class="line">    save_path=save_path,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<pre><code>[Train] epoch: 0/3, step: 0/22386, loss: 1.34774
[Train] epoch: 0/3, step: 100/22386, loss: 0.66738
[Train] epoch: 0/3, step: 200/22386, loss: 0.65944
[Train] epoch: 0/3, step: 300/22386, loss: 0.55590
[Train] epoch: 0/3, step: 400/22386, loss: 0.64057
[Train] epoch: 0/3, step: 500/22386, loss: 0.65418
[Evaluate]  dev score: 0.53670, dev loss: 0.72705
[Evaluate] best accuracy performence has been updated: 0.00000 --&gt; 0.53670
[Train] epoch: 0/3, step: 600/22386, loss: 0.58114
[Train] epoch: 0/3, step: 700/22386, loss: 0.68947
[Train] epoch: 0/3, step: 800/22386, loss: 0.65211
[Train] epoch: 0/3, step: 900/22386, loss: 0.71017
[Train] epoch: 0/3, step: 1000/22386, loss: 0.53267
[Evaluate]  dev score: 0.53283, dev loss: 0.72763
[Train] epoch: 0/3, step: 1100/22386, loss: 0.58642
[Train] epoch: 0/3, step: 1200/22386, loss: 0.96723
[Train] epoch: 0/3, step: 1300/22386, loss: 0.55056
[Train] epoch: 0/3, step: 1400/22386, loss: 0.49072
[Train] epoch: 0/3, step: 1500/22386, loss: 0.71014
[Evaluate]  dev score: 0.55124, dev loss: 0.73825
[Evaluate] best accuracy performence has been updated: 0.53670 --&gt; 0.55124
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> plot</span><br><span class="line">plot(runner, <span class="string">&#x27;att-loss-acc3.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="8-3-4-模型评价"><a href="#8-3-4-模型评价" class="headerlink" title="8.3.4 模型评价"></a>8.3.4 模型评价</h2><p>模型评价使用test_loader进行评价，并输出准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;checkpoint/model_best.pdparams&quot;</span></span><br><span class="line">runner.load_model(model_path)</span><br><span class="line">accuracy, _ =  runner.evaluate(test_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Evaluate on test set, Accuracy: <span class="subst">&#123;accuracy:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p>叠加多层的Transformer组块，观察对比实验效果。</p>
<hr>
<h2 id="8-3-5-模型预测"><a href="#8-3-5-模型预测" class="headerlink" title="8.3.5 模型预测"></a>8.3.5 模型预测</h2><p>从测试的数据集中取出一条数据，然后用word2id_dict进行编码变成ID的形式，然后放到模型里面进行预测输出，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;checkpoint_base/model_best.pdparams&quot;</span></span><br><span class="line">runner.load_model(model_path)</span><br><span class="line">text_a = <span class="string">&quot;电脑怎么录像？&quot;</span></span><br><span class="line">text_b = <span class="string">&quot;如何在计算机上录视频&quot;</span></span><br><span class="line"><span class="comment"># [CLS]转换成id</span></span><br><span class="line">cls_id = word2id_dict[<span class="string">&quot;[CLS]&quot;</span>]</span><br><span class="line"><span class="comment"># [SEP]转换成id</span></span><br><span class="line">sep_id = word2id_dict[<span class="string">&quot;[SEP]&quot;</span>]</span><br><span class="line"><span class="comment"># text_a转换成id的形式</span></span><br><span class="line">input_ids_a = [</span><br><span class="line">    word2id_dict[item] <span class="keyword">if</span> item <span class="keyword">in</span> word2id_dict <span class="keyword">else</span> word2id_dict[<span class="string">&quot;[UNK]&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> text_a</span><br><span class="line">]</span><br><span class="line"><span class="comment"># text_b转换成id的形式</span></span><br><span class="line">input_ids_b = [</span><br><span class="line">    word2id_dict[item] <span class="keyword">if</span> item <span class="keyword">in</span> word2id_dict <span class="keyword">else</span> word2id_dict[<span class="string">&quot;[UNK]&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> text_b</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 两个句子拼接成id的形式</span></span><br><span class="line">input_ids = [cls_id]+ input_ids_a + [sep_id] + input_ids_b + [sep_id]</span><br><span class="line"><span class="comment"># 分段id的形式</span></span><br><span class="line">segment_ids = [<span class="number">0</span>]*(<span class="built_in">len</span>(input_ids_a)+<span class="number">2</span>)+[<span class="number">1</span>]*(<span class="built_in">len</span>(input_ids_b)+<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 转换成Tensor张量</span></span><br><span class="line">input_ids = paddle.to_tensor([input_ids])</span><br><span class="line">segment_ids = paddle.to_tensor([segment_ids])</span><br><span class="line">inputs = [input_ids, segment_ids]</span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">logits = runner.predict(inputs)</span><br><span class="line"><span class="comment"># 取概率值最大的索引</span></span><br><span class="line">label_id = paddle.argmax(logits, axis=<span class="number">1</span>).numpy()[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测的label标签 &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(label_id))</span><br></pre></td></tr></table></figure>
<p>可以看到预测的标签是1，表明这两句话是相似的，这说明预测的结果是正确的。</p>
<h2 id="8-3-6-注意力可视化"><a href="#8-3-6-注意力可视化" class="headerlink" title="8.3.6  注意力可视化"></a>8.3.6  注意力可视化</h2><p>为了验证注意力机制学到了什么，本节把注意力机制的权重提取出来，然后进行可视化分析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先加载模型</span></span><br><span class="line">model_path=<span class="string">&#x27;checkpoint_base/model_best.pdparams&#x27;</span></span><br><span class="line">loaded_dict = paddle.load(model_path)</span><br><span class="line">model.load_dict(loaded_dict)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 输入一条样本</span></span><br><span class="line">text_a = <span class="string">&#x27;电脑怎么录像？&#x27;</span></span><br><span class="line">text_b = <span class="string">&#x27;如何在计算机上录视频&#x27;</span></span><br><span class="line">texts = [<span class="string">&#x27;CLS&#x27;</span>]+<span class="built_in">list</span>(text_a)+[<span class="string">&#x27;SEP&#x27;</span>]+<span class="built_in">list</span>(text_b)+[<span class="string">&#x27;SEP&#x27;</span>]</span><br><span class="line"><span class="comment"># text_a和text_b分别转换成id的形式</span></span><br><span class="line">input_ids_a = [</span><br><span class="line">    word2id_dict[item] <span class="keyword">if</span> item <span class="keyword">in</span> word2id_dict <span class="keyword">else</span> word2id_dict[<span class="string">&quot;[UNK]&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> text_a</span><br><span class="line">]</span><br><span class="line">input_ids_b = [</span><br><span class="line">    word2id_dict[item] <span class="keyword">if</span> item <span class="keyword">in</span> word2id_dict <span class="keyword">else</span> word2id_dict[<span class="string">&quot;[UNK]&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> text_b</span><br><span class="line">]</span><br><span class="line"><span class="comment"># text_a和text_b拼接</span></span><br><span class="line">input_ids = [cls_id]+ input_ids_a + [sep_id] + input_ids_b + [sep_id]</span><br><span class="line"><span class="comment"># 分段编码的id的形式</span></span><br><span class="line">segment_ids = [<span class="number">0</span>]*(<span class="built_in">len</span>(input_ids_a)+<span class="number">2</span>)+[<span class="number">1</span>]*(<span class="built_in">len</span>(input_ids_b)+<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入的文本：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(texts))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入的id形式：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(input_ids))</span><br><span class="line"><span class="comment"># 转换成Tensor</span></span><br><span class="line">input_ids = paddle.to_tensor([input_ids])</span><br><span class="line">segment_ids = paddle.to_tensor([segment_ids])</span><br><span class="line">inputs = [input_ids, segment_ids]</span><br><span class="line"><span class="comment"># 评估模式</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line"><span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">    pooled_output = model(inputs)</span><br><span class="line"><span class="comment"># 获取多头注意力权重</span></span><br><span class="line">atten_weights = model.attention_weights[<span class="number">0</span>].numpy()</span><br></pre></td></tr></table></figure>
<p>将注意力权重atten_weights进行可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data_attention = atten_weights[<span class="number">0</span>]</span><br><span class="line">plt.clf()</span><br><span class="line">font_size = <span class="number">25</span></span><br><span class="line">font = FontProperties(fname=<span class="string">&quot;simhei.ttf&quot;</span>, size=font_size)</span><br><span class="line"><span class="comment"># 可视化其中的head，总共heads_num 个head</span></span><br><span class="line"><span class="keyword">for</span> head <span class="keyword">in</span> <span class="built_in">range</span>(heads_num):</span><br><span class="line">    data = pd.DataFrame(data=data_attention[head], index=texts, columns=texts)</span><br><span class="line">    f, ax = plt.subplots(figsize=(<span class="number">13</span>, <span class="number">13</span>))</span><br><span class="line">    <span class="comment"># 使用heatmap可视化</span></span><br><span class="line">    sns.heatmap(data, ax=ax, cmap=<span class="string">&quot;OrRd&quot;</span>, cbar=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># y轴旋转270度</span></span><br><span class="line">    label_y = ax.get_yticklabels()</span><br><span class="line">    plt.setp(label_y, rotation=<span class="number">270</span>, horizontalalignment=<span class="string">&quot;right&quot;</span>, fontproperties=font)</span><br><span class="line">    <span class="comment"># x轴旋转0度</span></span><br><span class="line">    label_x = ax.get_xticklabels()</span><br><span class="line">    plt.setp(label_x, rotation=<span class="number">0</span>, horizontalalignment=<span class="string">&quot;right&quot;</span>, fontproperties=font)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;att-vis3_&#123;&#125;.pdf&#x27;</span>.<span class="built_in">format</span>(head))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上图所示，颜色越深，表示的权重越高，可以看到第一个位置的[CLS]跟“计”，“视”，“像”，“何”的关系比较大。另外第二句话里面的“计”与“视”跟第一句话里面的“录像”，“电脑”等词关系比较大。对于同一个句子内，“视”和“频”的关系很大等等。</p>
<hr>
<p>请可视化自注意力头，挖掘出更多更有意思的信息。</p>
<hr>
<h1 id="8-4-基于框架API实现文本语义匹配"><a href="#8-4-基于框架API实现文本语义匹配" class="headerlink" title="8.4 基于框架API实现文本语义匹配"></a>8.4 基于框架API实现文本语义匹配</h1><h2 id="8-4-1【框架API】文本语义匹配"><a href="#8-4-1【框架API】文本语义匹配" class="headerlink" title="8.4.1【框架API】文本语义匹配"></a>8.4.1【框架API】文本语义匹配</h2><h3 id="8-4-1-1-模型构建"><a href="#8-4-1-1-模型构建" class="headerlink" title="8.4.1.1 模型构建"></a>8.4.1.1 模型构建</h3><p>前面的章节使用了自己实现的TransformerBlock来构建文本语义匹配模型，在实际使用过程中不需要自己实现，直接使用框架的API即可，下面我们来实现一下，对比一下结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model_Transformer_v1</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        vocab_size,</span></span><br><span class="line"><span class="params">        n_block=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">        hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">        heads_num=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">        intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">        hidden_dropout=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        attention_dropout=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        act_dropout=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">        position_size=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">        num_classes=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">        padding_idx=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(Model_Transformer_v1, self).__init__()</span><br><span class="line">        <span class="comment"># 词表大小</span></span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        <span class="comment"># Transformer的编码器的数目</span></span><br><span class="line">        self.n_block = n_block</span><br><span class="line">        <span class="comment"># 每个词映射成稠密向量的维度</span></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        <span class="comment"># 多头注意力的个数</span></span><br><span class="line">        self.heads_num = heads_num</span><br><span class="line">        <span class="comment"># 逐位前馈层的的维度</span></span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        <span class="comment"># Embedding层的 Dropout</span></span><br><span class="line">        self.hidden_dropout = hidden_dropout</span><br><span class="line">        <span class="comment"># 多头注意力的dropout的 dropout参数</span></span><br><span class="line">        self.attention_dropout = attention_dropout</span><br><span class="line">        <span class="comment"># 位置编码的大小 position_size</span></span><br><span class="line">        self.position_size = position_size</span><br><span class="line">        <span class="comment"># 类别数</span></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        <span class="comment"># 逐位前馈层的dropout</span></span><br><span class="line">        self.act_dropout = act_dropout</span><br><span class="line">        <span class="comment"># [PAD]字符的ID</span></span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        <span class="comment"># 实例化输入编码，分段编码和位置编码</span></span><br><span class="line">        self.embeddings = TransformerEmbeddings(</span><br><span class="line">            self.vocab_size, self.hidden_size, self.hidden_dropout, self.position_size )</span><br><span class="line">        <span class="comment"># 实例化Transformer的编码器</span></span><br><span class="line">        self.layers = nn.LayerList([])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_block):</span><br><span class="line">            <span class="comment"># 使用框架API</span></span><br><span class="line">            encoder_layer = nn.TransformerEncoderLayer(hidden_size, </span><br><span class="line">                                                    heads_num, </span><br><span class="line">                                                    intermediate_size,</span><br><span class="line">                                                    dropout=hidden_dropout,</span><br><span class="line">                                                    attn_dropout=attention_dropout,</span><br><span class="line">                                                    act_dropout=act_dropout)</span><br><span class="line">            self.layers.append(encoder_layer)</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        self.dense = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        <span class="comment"># 双曲正切激活函数</span></span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line">        <span class="comment"># 最后一层分类器</span></span><br><span class="line">        self.classifier = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, position_ids=<span class="literal">None</span>, attention_mask=<span class="literal">None</span></span>):</span><br><span class="line">        input_ids, segment_ids = inputs</span><br><span class="line">        <span class="comment"># 构建Mask矩阵，把Pad的位置即input_ids中为0的位置设置为True,非0的位置设置为False</span></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attention_mask = paddle.unsqueeze(</span><br><span class="line">                (input_ids == self.padding_idx).astype(<span class="string">&quot;float32&quot;</span>) * -<span class="number">1e9</span>, axis=[<span class="number">1</span>, <span class="number">2</span>] )</span><br><span class="line">        <span class="comment"># 抽取特征向量</span></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids=input_ids, position_ids=position_ids, segment_ids=segment_ids )</span><br><span class="line">        sequence_output = embedding_output</span><br><span class="line">        self._attention_weights = []</span><br><span class="line">        <span class="comment"># Transformer的输出和注意力权重的输出</span></span><br><span class="line">        <span class="keyword">for</span> i, encoder_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layers):</span><br><span class="line">            sequence_output = encoder_layer(</span><br><span class="line">                sequence_output, src_mask=attention_mask )</span><br><span class="line">        <span class="comment"># 选择第0个位置的向量作为句向量</span></span><br><span class="line">        first_token_tensor = sequence_output[:, <span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="comment"># 句子级别的输出经过分类器</span></span><br><span class="line">        logits = self.classifier(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<h3 id="8-4-1-2-模型训练"><a href="#8-4-1-2-模型训练" class="headerlink" title="8.4.1.2 模型训练"></a>8.4.1.2 模型训练</h3><p>模型的训练配置的设置，包括一些超参数、优化器、损失函数等等，训练过程：模型的训练从Dataloader中取数据进行前向反向训练，每隔100个step输出一次日志，每隔500个step在验证集合上计算一次准确率。训练3个回合，并保存在验证集上最佳准确率的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">2021</span>)</span><br><span class="line">heads_num = <span class="number">4</span></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">vocab_size=<span class="number">21128</span></span><br><span class="line">num_classes= <span class="number">2</span></span><br><span class="line"><span class="comment"># 注意力多头的数目</span></span><br><span class="line"><span class="comment"># 交叉熵损失</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 评估的时候采用准确率指标</span></span><br><span class="line">metric = Accuracy()</span><br><span class="line"><span class="comment"># Transformer的分类模型</span></span><br><span class="line">model = Model_Transformer_v1(</span><br><span class="line">    vocab_size=vocab_size,</span><br><span class="line">    n_block=<span class="number">1</span>,</span><br><span class="line">    num_classes=num_classes,</span><br><span class="line">    heads_num=heads_num,</span><br><span class="line">    padding_idx=padding_idx,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 排除所有的偏置和LayerNorm的参数</span></span><br><span class="line">decay_params = [</span><br><span class="line">    p.name <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> [<span class="string">&quot;bias&quot;</span>, <span class="string">&quot;norm&quot;</span>])</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 定义 Optimizer</span></span><br><span class="line">optimizer = paddle.optimizer.AdamW(</span><br><span class="line">    learning_rate=<span class="number">5E-5</span>,</span><br><span class="line">    parameters=model.parameters(),</span><br><span class="line">    weight_decay=<span class="number">0.0</span>,</span><br><span class="line">    apply_decay_param_fun=<span class="keyword">lambda</span> x: x <span class="keyword">in</span> decay_params)</span><br><span class="line"></span><br><span class="line">runner = RunnerV3(model, optimizer, criterion, metric)</span><br><span class="line">save_path=<span class="string">&quot;./checkpoint/model_best.pdparams&quot;</span></span><br><span class="line">runner.train(train_loader, dev_loader, num_epochs=epochs, log_steps=<span class="number">100</span>, eval_steps=<span class="number">500</span>, save_path=save_path)</span><br></pre></td></tr></table></figure>
<h3 id="8-4-1-2-模型评价"><a href="#8-4-1-2-模型评价" class="headerlink" title="8.4.1.2 模型评价"></a>8.4.1.2 模型评价</h3><p>模型评价使用test_loader进行评价，并输出准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;checkpoint/model_best.pdparams&quot;</span></span><br><span class="line">runner.load_model(model_path)</span><br><span class="line">accuracy, _ =  runner.evaluate(test_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Evaluate on test set, Accuracy: <span class="subst">&#123;accuracy:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="8-4-2-增加Transformer层数的实验"><a href="#8-4-2-增加Transformer层数的实验" class="headerlink" title="8.4.2 增加Transformer层数的实验"></a>8.4.2 增加Transformer层数的实验</h2><h3 id="8-4-2-1-模型训练"><a href="#8-4-2-1-模型训练" class="headerlink" title="8.4.2.1 模型训练"></a>8.4.2.1 模型训练</h3><p>模型的训练配置的设置，包括一些超参数、优化器、损失函数等等，训练过程：模型的训练从Dataloader中取数据进行前向反向训练，每隔100个step输出一次日志，每隔500个step在验证集合上计算一次准确率。训练3个回合，并保存在验证集上最佳准确率的模型。</p>
<p>为了验证增加Transformer的层数的效果，我们把层数设置为2层来实验。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">2021</span>)</span><br><span class="line">heads_num = <span class="number">4</span></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">vocab_size=<span class="number">21128</span></span><br><span class="line">num_classes= <span class="number">2</span></span><br><span class="line"><span class="comment"># 注意力多头的数目</span></span><br><span class="line"><span class="comment"># 交叉熵损失</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 评估的时候采用准确率指标</span></span><br><span class="line">metric = Accuracy()</span><br><span class="line"><span class="comment"># Transformer的分类模型</span></span><br><span class="line">model = Model_Transformer_v1(</span><br><span class="line">    vocab_size=vocab_size,</span><br><span class="line">    n_block=<span class="number">2</span>,</span><br><span class="line">    num_classes=num_classes,</span><br><span class="line">    heads_num=heads_num,</span><br><span class="line">    padding_idx=padding_idx,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 排除所有的偏置和LayerNorm的参数</span></span><br><span class="line">decay_params = [</span><br><span class="line">    p.name <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> [<span class="string">&quot;bias&quot;</span>, <span class="string">&quot;norm&quot;</span>])</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 定义 Optimizer</span></span><br><span class="line">optimizer = paddle.optimizer.AdamW(</span><br><span class="line">    learning_rate=<span class="number">5E-5</span>,</span><br><span class="line">    parameters=model.parameters(),</span><br><span class="line">    weight_decay=<span class="number">0.0</span>,</span><br><span class="line">    apply_decay_param_fun=<span class="keyword">lambda</span> x: x <span class="keyword">in</span> decay_params)</span><br><span class="line"></span><br><span class="line">runner = RunnerV3(model, optimizer, criterion, metric)</span><br><span class="line">save_path=<span class="string">&quot;./checkpoint/model_best.pdparams&quot;</span></span><br><span class="line">runner.train(train_loader, dev_loader, num_epochs=epochs, log_steps=<span class="number">100</span>, eval_steps=<span class="number">500</span>, save_path=save_path)</span><br></pre></td></tr></table></figure>
<h3 id="8-4-2-2-模型评价"><a href="#8-4-2-2-模型评价" class="headerlink" title="8.4.2.2 模型评价"></a>8.4.2.2 模型评价</h3><p>模型评价使用test_loader进行评价，并输出准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;checkpoint/model_best.pdparams&quot;</span></span><br><span class="line">runner.load_model(model_path)</span><br><span class="line">accuracy, _ =  runner.evaluate(test_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Evaluate on test set, Accuracy: <span class="subst">&#123;accuracy:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到，增加了一层transformer的编码器了之后，精度变成了0.72733，说明增加Transformer的编码器的层数，能够提升模型的精度。</p>
<h1 id="8-5-小结"><a href="#8-5-小结" class="headerlink" title="8.5 小结"></a>8.5 小结</h1><p>本章介绍注意力机制的基本概念和代码实现。首先在上一章实践的基础上引入注意力机制来改进文本分类的效果，并进一步实现了多头自注意力模型来提高网络能力。</p>
<p>在实践部分，我们利用进一步在多头自注意力的基础上，复现了Transformer编码器模型。由于自注意力模型本书无法建模序列中的位置信息，因此Transformer模型引入了位置编码、分段编码等信息。最后，我们用Transformer编码器模型完成一个文本语义匹配任务。</p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/08/12/notes/tips-for-hexo/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          Tips for Hexo
        
      </div>
    </a>
  
  
    <a href="/2022/08/12/nndl/chapter8A/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">第8章（上）：注意力机制的理论解读</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2022 宝可梦训练师
      </div>
        <div class="footer-right">
          <a href="https://space.bilibili.com/782159" target="_blank">访问我的哔哩哔哩</a> 
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: ,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>