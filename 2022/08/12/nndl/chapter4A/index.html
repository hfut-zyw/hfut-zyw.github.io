<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第4章（上）：前馈神经网络理论解读 | Homepage | 张有为</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="第4章 前馈神经网络神经网络是由神经元按照一定的连接结构组合而成的网络。神经网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射 。前馈神经网络是最早发明的简单人工神经网络。整个网络中的信息单向传播，可以用一个有向无环路图表示，这种网络结构简单，易于实现。 在学习本章内容前，建议先阅读《神经网络与深度学习》第2章：机器学习概述的相关内容，关键知识点如 图4.1 所示">
<meta property="og:type" content="article">
<meta property="og:title" content="第4章（上）：前馈神经网络理论解读">
<meta property="og:url" content="http://example.com/2022/08/12/nndl/chapter4A/index.html">
<meta property="og:site_name" content="Homepage | 张有为">
<meta property="og:description" content="第4章 前馈神经网络神经网络是由神经元按照一定的连接结构组合而成的网络。神经网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射 。前馈神经网络是最早发明的简单人工神经网络。整个网络中的信息单向传播，可以用一个有向无环路图表示，这种网络结构简单，易于实现。 在学习本章内容前，建议先阅读《神经网络与深度学习》第2章：机器学习概述的相关内容，关键知识点如 图4.1 所示">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/0ae8775e92a04173928b4e3fed98b5934f0aad6582dc4143b2205669bfae6b55">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/d084b5e9e8654367ab4af046184f2f294f85a48417e64e36a79df942d5d6ff68">
<meta property="og:image" content="http://example.com/img-nndl/output_5_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_8_0.png">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/dbcf147a4e00446792eb2b93834e0f3154936e08ea124242af8631fde204381c">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/8562dfb10d464396948d05ee3620cec1d057025dddee43ff92dae3fbb72e8f65">
<meta property="og:image" content="http://example.com/img-nndl/output_40_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_45_1.png">
<meta property="og:image" content="http://example.com/img-nndl/output_53_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_64_0.png">
<meta property="og:image" content="http://example.com/img-nndl/output_67_0.png">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/d1fe3501bdbe47c2be309151c4aa2cf8419b561a64184e468e89c7d3de4f480d">
<meta property="article:published_time" content="2022-08-12T01:27:16.000Z">
<meta property="article:modified_time" content="2022-08-13T01:09:07.408Z">
<meta property="article:author" content="宝可梦训练师">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ai-studio-static-online.cdn.bcebos.com/0ae8775e92a04173928b4e3fed98b5934f0aad6582dc4143b2205669bfae6b55">
  
    <link rel="alternative" href="/atom.xml" title="Homepage | 张有为" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>


  
<meta name="generator" content="Hexo 6.2.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
				<img lazy-src="/img/face.png" class="js-avatar">
			
		</a>

		<hgroup>
			<h1 class="header-author"><a href="/">宝可梦训练师</a></h1>
		</hgroup>

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/2022/08/09/me/aboutme">Homepage</a></li>
				        
							<li><a href="/categories/notes">随写</a></li>
				        
							<li><a href="/categories/pytorch">Pytorch</a></li>
				        
							<li><a href="/categories/opt">优化笔记</a></li>
				        
							<li><a href="/categories/nndl">nndl案例与实践</a></li>
				        
						</ul>
					</nav>
					<nav class="half-header-menu">
						<a class="hide">Home</a>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
					        
						</div>
						<!-- music -->
						
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/face.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/2022/08/09/me/aboutme">Homepage</a></li>
		        
					<li><a href="/categories/notes">随写</a></li>
		        
					<li><a href="/categories/pytorch">Pytorch</a></li>
		        
					<li><a href="/categories/opt">优化笔记</a></li>
		        
					<li><a href="/categories/nndl">nndl案例与实践</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/hfut-zyw" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-nndl/chapter4A" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/08/12/nndl/chapter4A/" class="article-date">
  	<time datetime="2022-08-12T01:27:16.000Z" itemprop="datePublished">2022-08-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第4章（上）：前馈神经网络理论解读
      
          <span class="title-pop-out"></a>
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/nndl/">nndl</a>
	</div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第4章-前馈神经网络"><a href="#第4章-前馈神经网络" class="headerlink" title="第4章 前馈神经网络"></a>第4章 前馈神经网络</h1><p>神经网络是由神经元按照一定的连接结构组合而成的网络。神经网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射 。<br>前馈神经网络是最早发明的简单人工神经网络。整个网络中的信息单向传播，可以用一个有向无环路图表示，这种网络结构简单，易于实现。</p>
<p>在学习本章内容前，建议先阅读《神经网络与深度学习》第2章：机器学习概述的相关内容，关键知识点如 <strong>图4.1</strong> 所示，以便更好的理解和掌握相应的理论知识，及其在实践中的应用方法。</p>
<span id="more"></span>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/0ae8775e92a04173928b4e3fed98b5934f0aad6582dc4143b2205669bfae6b55" width=500></center>



<p><br><center>图4.1 《神经网络与深度学习》关键知识点回顾</center></br></p>
<p>本实践基于 <strong>《神经网络与深度学习》第4章：前馈神经网络</strong> 相关内容进行设计，主要包含两部分：</p>
<ul>
<li><strong>模型解读</strong>：介绍前馈神经网络的基本概念、网络结构及代码实现，利用前馈神经网络完成一个分类任务，并通过两个简单的实验，观察前馈神经网络的梯度消失问题和死亡ReLU问题，以及对应的优化策略；</li>
<li><strong>案例与实践</strong>：基于前馈神经网络完成鸢尾花分类任务。</li>
</ul>
<h2 id="4-1-神经元"><a href="#4-1-神经元" class="headerlink" title="4.1 神经元"></a>4.1 神经元</h2><p>神经网络的基本组成单元为带有非线性激活函数的神经元，其结构如如<strong>图4.2</strong>所示。神经元是对生物神经元的结构和特性的一种简化建模，接收一组输入信号并产生输出。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/d084b5e9e8654367ab4af046184f2f294f85a48417e64e36a79df942d5d6ff68" width="400" hegiht="" ></center>
<center><br>图4.2: 典型的神经元结构</br></center>

<h3 id="4-1-1-净活性值"><a href="#4-1-1-净活性值" class="headerlink" title="4.1.1 净活性值"></a>4.1.1 净活性值</h3><p>假设一个神经元接收的输入为$\mathbf{x}\in \mathbb{R}^D$，其权重向量为$\mathbf{w}\in \mathbb{R}^D$，神经元所获得的输入信号，即净活性值$z$的计算方法为</p>
<script type="math/tex; mode=display">
z =\mathbf{w}^T\mathbf{x}+b，（4.1）</script><p>其中$b$为偏置。</p>
<p>为了提高预测样本的效率，我们通常会将$N$个样本归为一组进行成批地预测。</p>
<script type="math/tex; mode=display">
\boldsymbol{z} =\boldsymbol{X} \boldsymbol{w} + b, (4.2)</script><p>其中$\boldsymbol{X}\in \mathbb{R}^{N\times D}$为$N$个样本的特征矩阵，$\boldsymbol{z}\in \mathbb{R}^N$为$N$个预测值组成的列向量。</p>
<p>使用Paddle计算一组输入的净活性值。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2个特征数为5的样本</span></span><br><span class="line">X = paddle.rand(shape=[<span class="number">2</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 含有5个参数的权重向量</span></span><br><span class="line">w = paddle.rand(shape=[<span class="number">5</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 偏置项</span></span><br><span class="line">b = paddle.rand(shape=[<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用&#x27;paddle.matmul&#x27;实现矩阵相乘</span></span><br><span class="line">z = paddle.matmul(X, w) + b</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input X:&quot;</span>, X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weight w:&quot;</span>, w, <span class="string">&quot;\nbias b:&quot;</span>, b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;output z:&quot;</span>, z)</span><br></pre></td></tr></table></figure>
<pre><code>input X: Tensor(shape=[2, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
       [[0.05338356, 0.87802440, 0.11555015, 0.72130114, 0.92724550],
        [0.06712580, 0.22947401, 0.93955714, 0.36251226, 0.82916337]])
weight w: Tensor(shape=[5, 1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
       [[0.37444150],
        [0.66573620],
        [0.75348777],
        [0.60667807],
        [0.95841354]]) 
bias b: Tensor(shape=[1, 1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
       [[0.46258664]])
output z: Tensor(shape=[2, 1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
       [[2.48003697],
        [2.36282158]])
</code></pre><hr>
<p><strong>说明</strong></p>
<p>在飞桨中，可以使用<strong>nn.Linear</strong>完成输入张量的上述变换。</p>
<hr>
<h3 id="4-1-2-激活函数"><a href="#4-1-2-激活函数" class="headerlink" title="4.1.2 激活函数"></a>4.1.2 激活函数</h3><p>净活性值$z$再经过一个非线性函数$f(·)$后，得到神经元的活性值$a$。</p>
<script type="math/tex; mode=display">
a = f(z)，（4.3）</script><p>激活函数通常为非线性函数，可以增强神经网络的表示能力和学习能力。常用的激活函数有S型函数和ReLU函数。</p>
<h4 id="4-1-2-1-Sigmoid-型函数"><a href="#4-1-2-1-Sigmoid-型函数" class="headerlink" title="4.1.2.1 Sigmoid 型函数"></a>4.1.2.1 Sigmoid 型函数</h4><p>Sigmoid 型函数是指一类S型曲线函数，为两端饱和函数。常用的 Sigmoid 型函数有 Logistic 函数和 Tanh 函数，其数学表达式为</p>
<p>Logistic 函数：</p>
<script type="math/tex; mode=display">
\sigma(z) = \frac{1}{1+\exp(-z)}。（4.4）</script><p>Tanh 函数：</p>
<script type="math/tex; mode=display">
\mathrm{tanh}(z) = \frac{\exp(z)-\exp(-z)}{\exp(z)+\exp(-z)}。（4.5）</script><p>Logistic函数和Tanh函数的代码实现和可视化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistic函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + paddle.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tanh函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> (paddle.exp(z) - paddle.exp(-z)) / (paddle.exp(z) + paddle.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在[-10,10]的范围内生成10000个输入值，用于绘制函数曲线</span></span><br><span class="line">z = paddle.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(z.tolist(), logistic(z).tolist(), color=<span class="string">&#x27;#8E004D&#x27;</span>, label=<span class="string">&quot;Logistic Function&quot;</span>)</span><br><span class="line">plt.plot(z.tolist(), tanh(z).tolist(), color=<span class="string">&#x27;#E20079&#x27;</span>, linestyle =<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&quot;Tanh Function&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax = plt.gca() <span class="comment"># 获取轴，默认有4个</span></span><br><span class="line"><span class="comment"># 隐藏两个轴，通过把颜色设置成none</span></span><br><span class="line">ax.spines[<span class="string">&#x27;top&#x27;</span>].set_color(<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">ax.spines[<span class="string">&#x27;right&#x27;</span>].set_color(<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># 调整坐标轴位置   </span></span><br><span class="line">ax.spines[<span class="string">&#x27;left&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>,<span class="number">0</span>))</span><br><span class="line">ax.spines[<span class="string">&#x27;bottom&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>,<span class="number">0</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>, fontsize=<span class="string">&#x27;large&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">&#x27;fw-logistic-tanh.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>​<br><img src="/img-nndl/output_5_0.png" alt="png"><br>​    </p>
<hr>
<p><strong>说明</strong></p>
<p>在飞桨中，可以通过调用<code>paddle.nn.functional.sigmoid</code>和<code>paddle.nn.functional.tanh</code>实现对张量的Logistic和Tanh计算。</p>
<hr>
<h4 id="4-1-2-2-ReLU型函数"><a href="#4-1-2-2-ReLU型函数" class="headerlink" title="4.1.2.2 ReLU型函数"></a>4.1.2.2 ReLU型函数</h4><p>常见的ReLU函数有ReLU和带泄露的ReLU（Leaky ReLU），数学表达式分别为：</p>
<script type="math/tex; mode=display">
\mathrm{ReLU}(z) = \max(0,z),（4.6）</script><script type="math/tex; mode=display">
\mathrm{LeakyReLU}(z) = \max(0,z)+\lambda \min(0,z),（4.7）</script><p>其中$\lambda$为超参数。</p>
<p>可视化ReLU和带泄露的ReLU的函数的代码实现和可视化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ReLU</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> paddle.maximum(z, paddle.to_tensor(<span class="number">0.</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 带泄露的ReLU</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">leaky_relu</span>(<span class="params">z, negative_slope=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="comment"># 当前版本paddle暂不支持直接将bool类型转成int类型，因此调用了paddle的cast函数来进行显式转换</span></span><br><span class="line">    a1 = (paddle.cast((z &gt; <span class="number">0</span>), dtype=<span class="string">&#x27;float32&#x27;</span>) * z) </span><br><span class="line">    a2 = (paddle.cast((z &lt;= <span class="number">0</span>), dtype=<span class="string">&#x27;float32&#x27;</span>) * (negative_slope * z))</span><br><span class="line">    <span class="keyword">return</span> a1 + a2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在[-10,10]的范围内生成一系列的输入值，用于绘制relu、leaky_relu的函数曲线</span></span><br><span class="line">z = paddle.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(z.tolist(), relu(z).tolist(), color=<span class="string">&quot;#8E004D&quot;</span>, label=<span class="string">&quot;ReLU Function&quot;</span>)</span><br><span class="line">plt.plot(z.tolist(), leaky_relu(z).tolist(), color=<span class="string">&quot;#E20079&quot;</span>, linestyle=<span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;LeakyReLU Function&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[<span class="string">&#x27;top&#x27;</span>].set_color(<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">ax.spines[<span class="string">&#x27;right&#x27;</span>].set_color(<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">ax.spines[<span class="string">&#x27;left&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>,<span class="number">0</span>))</span><br><span class="line">ax.spines[<span class="string">&#x27;bottom&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>,<span class="number">0</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>, fontsize=<span class="string">&#x27;large&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;fw-relu-leakyrelu.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>​<br><img src="/img-nndl/output_8_0.png" alt="png"><br>​    </p>
<hr>
<p><strong>说明</strong></p>
<p>在飞桨中，可以通过调用<code>paddle.nn.functional.relu</code>和<code>paddle.nn.functional.leaky_relu</code>完成ReLU与带泄露的ReLU的计算。</p>
<hr>
<p><strong>动手练习</strong><br>本节重点介绍和实现了几个经典的Sigmoid函数和ReLU函数。<br>请动手实现《神经网络与深度学习》4.1节中提到的其他激活函数，如：Hard-Logistic、Hard-Tanh、ELU、Softplus、Swish等。</p>
<h2 id="4-2-基于前馈神经网络的二分类任务"><a href="#4-2-基于前馈神经网络的二分类任务" class="headerlink" title="4.2 基于前馈神经网络的二分类任务"></a>4.2 基于前馈神经网络的二分类任务</h2><p>前馈神经网络的网络结构如<strong>图4.3</strong>所示。每一层获取前一层神经元的活性值，并重复上述计算得到该层的活性值，传入到下一层。整个网络中无反馈，信号从输入层向输出层逐层的单向传播，得到网络最后的输出 $\boldsymbol{a}^{(L)}$。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/dbcf147a4e00446792eb2b93834e0f3154936e08ea124242af8631fde204381c" width="500" hegiht="" ></center>
<center><br>图4.3: 前馈神经网络结构</br></center>

<h3 id="4-2-1-数据集构建"><a href="#4-2-1-数据集构建" class="headerlink" title="4.2.1 数据集构建"></a>4.2.1 数据集构建</h3><p>这里，我们使用第3.1.1节中构建的二分类数据集：Moon1000数据集，其中训练集640条、验证集160条、测试集200条。<br>该数据集的数据是从两个带噪音的弯月形状数据分布中采样得到，每个样本包含2个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> make_moons</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采样1000个样本</span></span><br><span class="line">n_samples = <span class="number">1000</span></span><br><span class="line">X, y = make_moons(n_samples=n_samples, shuffle=<span class="literal">True</span>, noise=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">num_train = <span class="number">640</span></span><br><span class="line">num_dev = <span class="number">160</span></span><br><span class="line">num_test = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">X_train, y_train = X[:num_train], y[:num_train]</span><br><span class="line">X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]</span><br><span class="line">X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]</span><br><span class="line"></span><br><span class="line">y_train = y_train.reshape([-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">y_dev = y_dev.reshape([-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">y_test = y_test.reshape([-<span class="number">1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h3 id="4-2-2-模型构建"><a href="#4-2-2-模型构建" class="headerlink" title="4.2.2 模型构建"></a>4.2.2 模型构建</h3><p>为了更高效的构建前馈神经网络，我们先定义每一层的算子，然后再通过算子组合构建整个前馈神经网络。</p>
<p>假设网络的第$l$层的输入为第$l-1$层的神经元活性值$\boldsymbol{a}^{(l-1)}$，经过一个仿射变换，得到该层神经元的净活性值$\boldsymbol{z}$，再输入到激活函数得到该层神经元的活性值$\boldsymbol{a}$。</p>
<p>在实践中，为了提高模型的处理效率，通常将$N$个样本归为一组进行成批地计算。假设网络第$l$层的输入为$\boldsymbol{A}^{(l-1)}\in \mathbb{R}^{N\times M_{l-1}}$，其中每一行为一个样本，则前馈网络中第$l$层的计算公式为</p>
<script type="math/tex; mode=display">
\mathbf Z^{(l)}=\mathbf A^{(l-1)} \mathbf W^{(l)} +\mathbf b^{(l)}  \in \mathbb{R}^{N\times M_{l}}, (4.8)</script><script type="math/tex; mode=display">
\mathbf A^{(l)}=f_l(\mathbf Z^{(l)}) \in \mathbb{R}^{N\times M_{l}}, (4.9)</script><p>其中$\mathbf Z^{(l)}$为$N$个样本第$l$层神经元的净活性值，$\mathbf A^{(l)}$为$N$个样本第$l$层神经元的活性值，$\boldsymbol{W}^{(l)}\in \mathbb{R}^{M_{l-1}\times M_{l}}$为第$l$层的权重矩阵，$\boldsymbol{b}^{(l)}\in \mathbb{R}^{1\times M_{l}}$为第$l$层的偏置。</p>
<hr>
<p>为了和代码的实现保存一致性，这里使用形状为$(样本数量\times 特征维度)$的张量来表示一组样本。样本的矩阵$\boldsymbol{X}$是由$N$个$\boldsymbol{x}$的<strong>行向量</strong>组成。而《神经网络与深度学习》中$\boldsymbol{x}$为列向量，因此这里的权重矩阵$\boldsymbol{W}$和偏置$\boldsymbol{b}$和《神经网络与深度学习》中的表示刚好为转置关系。</p>
<hr>
<p>为了使后续的模型搭建更加便捷，我们将神经层的计算，即公式(4.8)和(4.9)，都封装成算子，这些算子都继承<code>Op</code>基类。</p>
<h4 id="4-2-2-1-线性层算子"><a href="#4-2-2-1-线性层算子" class="headerlink" title="4.2.2.1 线性层算子"></a>4.2.2.1 线性层算子</h4><p>公式（4.8）对应一个线性层算子，权重参数采用默认的随机初始化，偏置采用默认的零初始化。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> Op</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现线性层算子</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, output_size, name, weight_init=paddle.standard_normal, bias_init=paddle.zeros</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - input_size：输入数据维度</span></span><br><span class="line"><span class="string">            - output_size：输出数据维度</span></span><br><span class="line"><span class="string">            - name：算子名称</span></span><br><span class="line"><span class="string">            - weight_init：权重初始化方式，默认使用&#x27;paddle.standard_normal&#x27;进行标准正态分布初始化</span></span><br><span class="line"><span class="string">            - bias_init：偏置初始化方式，默认使用全0初始化</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        self.params[<span class="string">&#x27;W&#x27;</span>] = weight_init(shape=[input_size,output_size])</span><br><span class="line">        <span class="comment"># 初始化偏置</span></span><br><span class="line">        self.params[<span class="string">&#x27;b&#x27;</span>] = bias_init(shape=[<span class="number">1</span>,output_size])</span><br><span class="line">        self.inputs = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - inputs：shape=[N,input_size], N是样本数量</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - outputs：预测值，shape=[N,output_size]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.inputs = inputs</span><br><span class="line"></span><br><span class="line">        outputs = paddle.matmul(self.inputs, self.params[<span class="string">&#x27;W&#x27;</span>]) + self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h4 id="4-2-2-2-Logistic算子"><a href="#4-2-2-2-Logistic算子" class="headerlink" title="4.2.2.2 Logistic算子"></a>4.2.2.2 Logistic算子</h4><p>本节我们采用Logistic函数来作为公式(4.9)中的激活函数。这里也将Logistic函数实现一个算子，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Logistic</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.inputs = <span class="literal">None</span></span><br><span class="line">        self.outputs = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - inputs: shape=[N,D]</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - outputs：shape=[N,D]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        outputs = <span class="number">1.0</span> / (<span class="number">1.0</span> + paddle.exp(-inputs))</span><br><span class="line">        self.outputs = outputs</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h4 id="4-2-2-3-层的串行组合"><a href="#4-2-2-3-层的串行组合" class="headerlink" title="4.2.2.3 层的串行组合"></a>4.2.2.3 层的串行组合</h4><p>在定义了神经层的线性层算子和激活函数算子之后，我们可以不断交叉重复使用它们来构建一个多层的神经网络。</p>
<p>下面我们实现一个两层的用于二分类任务的前馈神经网络，选用Logistic作为激活函数，可以利用上面实现的线性层和激活函数算子来组装。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现一个两层前馈神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model_MLP_L2</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - input_size：输入维度</span></span><br><span class="line"><span class="string">            - hidden_size：隐藏层神经元数量</span></span><br><span class="line"><span class="string">            - output_size：输出维度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.fc1 = Linear(input_size, hidden_size, name=<span class="string">&quot;fc1&quot;</span>)</span><br><span class="line">        self.act_fn1 = Logistic()</span><br><span class="line">        self.fc2 = Linear(hidden_size, output_size, name=<span class="string">&quot;fc2&quot;</span>)</span><br><span class="line">        self.act_fn2 = Logistic()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - X：shape=[N,input_size], N是样本数量</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - a2：预测值，shape=[N,output_size]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        z1 = self.fc1(X)</span><br><span class="line">        a1 = self.act_fn1(z1)</span><br><span class="line">        z2 = self.fc2(a1)</span><br><span class="line">        a2 = self.act_fn2(z2)</span><br><span class="line">        <span class="keyword">return</span> a2</span><br></pre></td></tr></table></figure>
<p><strong>测试一下</strong></p>
<p>现在，我们实例化一个两层的前馈网络，令其输入层维度为5，隐藏层维度为10，输出层维度为1。<br>并随机生成一条长度为5的数据输入两层神经网络，观察输出结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = Model_MLP_L2(input_size=<span class="number">5</span>, hidden_size=<span class="number">10</span>, output_size=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 随机生成1条长度为5的数据</span></span><br><span class="line">X = paddle.rand(shape=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">result = model(X)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;result: &quot;</span>, result)</span><br></pre></td></tr></table></figure>
<pre><code>result:  Tensor(shape=[1, 1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
       [[0.46669415]])
</code></pre><h3 id="4-2-3-损失函数"><a href="#4-2-3-损失函数" class="headerlink" title="4.2.3 损失函数"></a>4.2.3 损失函数</h3><p>二分类交叉熵损失函数见第三章，这里不再赘述。</p>
<h3 id="4-2-4-模型优化"><a href="#4-2-4-模型优化" class="headerlink" title="4.2.4 模型优化"></a>4.2.4 模型优化</h3><p>神经网络的参数主要是通过<strong>梯度下降法</strong>进行优化的，因此需要计算最终损失对每个参数的梯度。<br>由于神经网络的层数通常比较深，其梯度计算和上一章中的线性分类模型的不同的点在于：线性模型通常比较简单可以直接计算梯度，而神经网络相当于一个复合函数，需要利用链式法则进行反向传播来计算梯度。</p>
<h4 id="4-2-4-1-反向传播算法"><a href="#4-2-4-1-反向传播算法" class="headerlink" title="4.2.4.1 反向传播算法"></a>4.2.4.1 反向传播算法</h4><p>前馈神经网络的参数梯度通常使用<strong>误差反向传播</strong>算法来计算。使用误差反向传播算法的前馈神经网络训练过程可以分为以下三步：</p>
<ol>
<li>前馈计算每一层的净活性值$\boldsymbol{Z}^{(l)}$和激活值$\boldsymbol{A}^ {(l)}$，直到最后一层；</li>
<li>反向传播计算每一层的误差项$\delta^{(l)}=\frac{\partial R}{\partial \boldsymbol{Z}^{(l)}}$；</li>
<li>计算每一层参数的梯度，并更新参数。</li>
</ol>
<p>在上面实现算子的基础上，来实现误差反向传播算法。在上面的三个步骤中，</p>
<ol>
<li>第1步是前向计算，可以利用算子的<code>forward()</code>方法来实现；</li>
<li>第2步是反向计算梯度，可以利用算子的<code>backward()</code>方法来实现；</li>
<li>第3步中的计算参数梯度也放到<code>backward()</code>中实现，更新参数放到另外的<strong>优化器</strong>中专门进行。</li>
</ol>
<p>这样，在模型训练过程中，我们首先执行模型的<code>forward()</code>，再执行模型的<code>backward()</code>，就得到了所有参数的梯度，之后再利用优化器迭代更新参数。</p>
<p>以这我们这节中构建的两层全连接前馈神经网络<code>Model_MLP_L2</code>为例，下图给出了其前向和反向计算过程：</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/8562dfb10d464396948d05ee3620cec1d057025dddee43ff92dae3fbb72e8f65" alt=""></p>
<p>下面我们按照反向的梯度传播顺序，为每个算子添加<code>backward()</code>方法，并在其中实现每一层参数的梯度的计算。</p>
<h4 id="4-2-4-2-损失函数"><a href="#4-2-4-2-损失函数" class="headerlink" title="4.2.4.2 损失函数"></a>4.2.4.2 损失函数</h4><p>二分类交叉熵损失函数对神经网络的输出$\hat{\boldsymbol{y}}$的偏导数为:</p>
<script type="math/tex; mode=display">
\frac{\partial R}{\partial \hat{\boldsymbol{y}}} =  -\frac{1}{N}(\mathrm{dialog}(\frac{1}{\hat{\boldsymbol{y}}})\boldsymbol{y}-\mathrm{dialog}(\frac{1}{1-\hat{\boldsymbol{y}}})(1-\boldsymbol{y})) (4.10) \\ 
= -\frac{1}{N}(\frac{1}{\hat{\boldsymbol{y}}}\odot\boldsymbol{y}-\frac{1}{1-\hat{\boldsymbol{y}}}\odot(1-\boldsymbol{y})), (4.11)</script><p>其中$dialog(\boldsymbol{x})$表示以向量$\boldsymbol{x}$为对角元素的对角阵，$\frac{1}{\boldsymbol{x}}=\frac{1}{x_1},…,\frac{1}{x_N}$表示逐元素除，$\odot$表示逐元素积。</p>
<p>实现损失函数的<code>backward()</code>，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现交叉熵损失函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BinaryCrossEntropyLoss</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.predicts = <span class="literal">None</span></span><br><span class="line">        self.labels = <span class="literal">None</span></span><br><span class="line">        self.num = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, predicts, labels</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(predicts, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, predicts, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - predicts：预测值，shape=[N, 1]，N为样本数量</span></span><br><span class="line"><span class="string">            - labels：真实标签，shape=[N, 1]</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - 损失值：shape=[1]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.predicts = predicts</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.num = self.predicts.shape[<span class="number">0</span>]</span><br><span class="line">        loss = -<span class="number">1.</span> / self.num * (paddle.matmul(self.labels.t(), paddle.log(self.predicts)) </span><br><span class="line">                + paddle.matmul((<span class="number">1</span>-self.labels.t()), paddle.log(<span class="number">1</span>-self.predicts)))</span><br><span class="line"></span><br><span class="line">        loss = paddle.squeeze(loss, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 计算损失函数对模型预测的导数</span></span><br><span class="line">        loss_grad_predicts = -<span class="number">1.0</span> * (self.labels / self.predicts - </span><br><span class="line">                       (<span class="number">1</span> - self.labels) / (<span class="number">1</span> - self.predicts)) / self.num</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 梯度反向传播</span></span><br><span class="line">        self.model.backward(loss_grad_predicts)</span><br></pre></td></tr></table></figure>
<h4 id="4-2-4-3-Logistic算子"><a href="#4-2-4-3-Logistic算子" class="headerlink" title="4.2.4.3 Logistic算子"></a>4.2.4.3 Logistic算子</h4><p>在本节中，我们使用Logistic激活函数，所以这里为Logistic算子增加的反向函数。</p>
<p>Logistic算子的前向过程表示为$\boldsymbol{A}=\sigma(\boldsymbol{Z})$，其中$\sigma$为Logistic函数，$\boldsymbol{Z} \in R^{N \times D}$和$\boldsymbol{A} \in R^{N \times D}$的每一行表示一个样本。</p>
<p>为了简便起见，我们分别用向量$\boldsymbol{a} \in R^D$ 和 $\boldsymbol{z} \in R^D$表示同一个样本在激活函数前后的表示，则$\boldsymbol{a}$对$\boldsymbol{z}$的偏导数为：</p>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}=diag(\boldsymbol{a}\odot(1-\boldsymbol{a}))\in R^{D \times D}, (4.12)</script><p>按照反向传播算法，令$\delta_{\boldsymbol{a}}=\frac{\partial R}{\partial \boldsymbol{a}} \in R^D$表示最终损失$R$对Logistic算子的单个输出$\boldsymbol{a}$的梯度，则</p>
<script type="math/tex; mode=display">
\delta_{\boldsymbol{z}} \triangleq \frac{\partial R}{\partial \boldsymbol{z}} = \frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}\delta_{\boldsymbol{a}}  (4.13) \\
= diag(\boldsymbol{a}\odot(1-\boldsymbol{a}))\delta_{\boldsymbol(a)}, (4.14) \\
= \boldsymbol{a}\odot(1-\boldsymbol{a})\odot\delta_{\boldsymbol(a)}。 (4.15)</script><p>将上面公式利用批量数据表示的方式重写，令$\delta_{\boldsymbol{A}} =\frac{\partial R}{\partial \boldsymbol{A}} \in R^{N \times D}$表示最终损失$R$对Logistic算子输出$A$的梯度，损失函数对Logistic函数输入$\boldsymbol{Z}$的导数为</p>
<script type="math/tex; mode=display">
\delta_{\boldsymbol{Z}}=\boldsymbol{A} \odot (1-\boldsymbol{A})\odot \delta_{\boldsymbol{A}} \in R^{N \times D},(4.16)</script><p>$\delta_{\boldsymbol{Z}}$为Logistic算子反向传播的输出。</p>
<p>由于Logistic函数中没有参数，这里不需要在<code>backward()</code>方法中计算该算子参数的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Logistic</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.inputs = <span class="literal">None</span></span><br><span class="line">        self.outputs = <span class="literal">None</span></span><br><span class="line">        self.params = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        outputs = <span class="number">1.0</span> / (<span class="number">1.0</span> + paddle.exp(-inputs))</span><br><span class="line">        self.outputs = outputs</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grads</span>):</span><br><span class="line">        <span class="comment"># 计算Logistic激活函数对输入的导数</span></span><br><span class="line">        outputs_grad_inputs = paddle.multiply(self.outputs, (<span class="number">1.0</span> - self.outputs))</span><br><span class="line">        <span class="keyword">return</span> paddle.multiply(grads,outputs_grad_inputs)</span><br></pre></td></tr></table></figure>
<h4 id="4-2-4-4-线性层"><a href="#4-2-4-4-线性层" class="headerlink" title="4.2.4.4 线性层"></a>4.2.4.4 线性层</h4><p>线性层算子Linear的前向过程表示为$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{W}+\boldsymbol{b}$，其中输入为$\boldsymbol{X} \in R^{N \times M}$，输出为$\boldsymbol{Y} \in R^{N \times D}$，参数为权重矩阵$\boldsymbol{W} \in R^{M \times D}$和偏置$\boldsymbol{b} \in R^{1 \times D}$。$\boldsymbol{X}$和$\boldsymbol{Y}$中的每一行表示一个样本。</p>
<p>为了简便起见，我们用向量$\boldsymbol{x}\in R^M$和$\boldsymbol{y}\in R^D$表示同一个样本在线性层算子中的输入和输出，则有$\boldsymbol{y}=\boldsymbol{W}^T\boldsymbol{x}+\boldsymbol{b}^T$。$\boldsymbol{y}$对输入$\boldsymbol{x}$的偏导数为</p>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} = \boldsymbol{W}\in R^{D \times M}。(4.17)</script><p><strong>线性层输入的梯度</strong> 按照反向传播算法，令$\delta_{\boldsymbol{y}}=\frac{\partial R}{\partial \boldsymbol{y}}\in R^D$表示最终损失$R$对线性层算子的单个输出$\boldsymbol{y}$的梯度，则</p>
<script type="math/tex; mode=display">
\delta_{\boldsymbol{x}} \triangleq \frac{\partial R}{\partial \boldsymbol{x}}= \boldsymbol{W} \delta_{\boldsymbol{y}}。(4.18)</script><p>将上面公式利用批量数据表示的方式重写，令$\delta_{\boldsymbol{Y}}=\frac{\partial R}{\partial \boldsymbol{Y}}\in \mathbb{R}^{N\times D}$表示最终损失$R$对线性层算子输出$\boldsymbol{Y}$的梯度，公式可以重写为</p>
<script type="math/tex; mode=display">
\delta_{\boldsymbol{X}} =\delta_{\boldsymbol{Y}} \boldsymbol{W}^T,(4.19)</script><p>其中$\delta_{\boldsymbol{X}}$为线性层算子反向函数的输出。</p>
<p><strong>计算线性层参数的梯度</strong> 由于线性层算子中包含有可学习的参数$\boldsymbol{W}$和$\boldsymbol{b}$，因此<code>backward()</code>除了实现梯度反传外，还需要计算算子内部的参数的梯度。</p>
<p>令$\delta_{\boldsymbol{y}}=\frac{\partial R}{\partial \boldsymbol{y}}\in \mathbb{R}^D$表示最终损失$R$对线性层算子的单个输出$\boldsymbol{y}$的梯度，则</p>
<script type="math/tex; mode=display">
\delta_{\boldsymbol{W}} \triangleq \frac{\partial R}{\partial \boldsymbol{W}} = \boldsymbol{x}\delta_{\boldsymbol{y}}^T,(4.20) \\
\delta_{\boldsymbol{b}} \triangleq \frac{\partial R}{\partial \boldsymbol{b}} = \delta_{\boldsymbol{y}}^T。(4.21)</script><p>将上面公式利用批量数据表示的方式重写，令$\delta_{\boldsymbol{Y}}=\frac{\partial R}{\partial \boldsymbol{Y}}\in \mathbb{R}^{N\times D}$表示最终损失$R$对线性层算子输出$\boldsymbol{Y}$的梯度，则公式可以重写为</p>
<script type="math/tex; mode=display">
\delta_{\boldsymbol{W}} = \boldsymbol{X}^T \delta_{\boldsymbol{Y}},(4.22) \\
\delta_{\boldsymbol{b}} = \mathbf{1}^T \delta_{\boldsymbol{Y}}。(4.23)</script><p>具体实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, output_size, name, weight_init=paddle.standard_normal, bias_init=paddle.zeros</span>):</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">&#x27;W&#x27;</span>] = weight_init(shape=[input_size, output_size])</span><br><span class="line">        self.params[<span class="string">&#x27;b&#x27;</span>] = bias_init(shape=[<span class="number">1</span>, output_size])</span><br><span class="line"></span><br><span class="line">        self.inputs = <span class="literal">None</span></span><br><span class="line">        self.grads = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        self.inputs = inputs</span><br><span class="line">        outputs = paddle.matmul(self.inputs, self.params[<span class="string">&#x27;W&#x27;</span>]) + self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grads</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">            - grads：损失函数对当前层输出的导数</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">            - 损失函数对当前层输入的导数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.grads[<span class="string">&#x27;W&#x27;</span>] = paddle.matmul(self.inputs.T, grads)</span><br><span class="line">        self.grads[<span class="string">&#x27;b&#x27;</span>] = paddle.<span class="built_in">sum</span>(grads, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 线性层输入的梯度</span></span><br><span class="line">        <span class="keyword">return</span> paddle.matmul(grads, self.params[<span class="string">&#x27;W&#x27;</span>].T)</span><br></pre></td></tr></table></figure>
<h4 id="4-2-4-5-整个网络"><a href="#4-2-4-5-整个网络" class="headerlink" title="4.2.4.5 整个网络"></a>4.2.4.5 整个网络</h4><p>实现完整的两层神经网络的前向和反向计算。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model_MLP_L2</span>(<span class="title class_ inherited__">Op</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="comment"># 线性层</span></span><br><span class="line">        self.fc1 = Linear(input_size, hidden_size, name=<span class="string">&quot;fc1&quot;</span>)</span><br><span class="line">        <span class="comment"># Logistic激活函数层</span></span><br><span class="line">        self.act_fn1 = Logistic()</span><br><span class="line">        self.fc2 = Linear(hidden_size, output_size, name=<span class="string">&quot;fc2&quot;</span>)</span><br><span class="line">        self.act_fn2 = Logistic()</span><br><span class="line"></span><br><span class="line">        self.layers = [self.fc1, self.act_fn1, self.fc2, self.act_fn2]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.forward(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        z1 = self.fc1(X)</span><br><span class="line">        a1 = self.act_fn1(z1)</span><br><span class="line">        z2 = self.fc2(a1)</span><br><span class="line">        a2 = self.act_fn2(z2)</span><br><span class="line">        <span class="keyword">return</span> a2</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 反向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, loss_grad_a2</span>):</span><br><span class="line">        loss_grad_z2 = self.act_fn2.backward(loss_grad_a2)</span><br><span class="line">        loss_grad_a1 = self.fc2.backward(loss_grad_z2)</span><br><span class="line">        loss_grad_z1 = self.act_fn1.backward(loss_grad_a1)</span><br><span class="line">        loss_grad_inputs = self.fc1.backward(loss_grad_z1)</span><br></pre></td></tr></table></figure>
<h4 id="4-2-4-6-优化器"><a href="#4-2-4-6-优化器" class="headerlink" title="4.2.4.6 优化器"></a>4.2.4.6 优化器</h4><p>在计算好神经网络参数的梯度之后，我们将梯度下降法中参数的更新过程实现在优化器中。</p>
<p>与第3章中实现的梯度下降优化器<code>SimpleBatchGD</code>不同的是，此处的优化器需要遍历每层，对每层的参数分别做更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BatchGD</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_lr, model</span>):</span><br><span class="line">        <span class="built_in">super</span>(BatchGD, self).__init__(init_lr=init_lr, model=model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.model.layers: <span class="comment"># 遍历所有层</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer.params, <span class="built_in">dict</span>):</span><br><span class="line">                <span class="keyword">for</span> key <span class="keyword">in</span> layer.params.keys():</span><br><span class="line">                    layer.params[key] = layer.params[key] - self.init_lr * layer.grads[key]</span><br></pre></td></tr></table></figure>
<h3 id="4-2-5-完善Runner类：RunnerV2-1"><a href="#4-2-5-完善Runner类：RunnerV2-1" class="headerlink" title="4.2.5 完善Runner类：RunnerV2_1"></a>4.2.5 完善Runner类：RunnerV2_1</h3><p>基于3.1.6实现的 RunnerV2 类主要针对比较简单的模型。而在本章中，模型由多个算子组合而成，通常比较复杂，因此本节继续完善并实现一个改进版： <code>RunnerV2_1</code>类，其主要加入的功能有：</p>
<ol>
<li>支持自定义算子的梯度计算，在训练过程中调用<code>self.loss_fn.backward()</code>从损失函数开始反向计算梯度；</li>
<li>每层的模型保存和加载，将每一层的参数分别进行保存和加载。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RunnerV2_1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, optimizer, metric, loss_fn, **kwargs</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.metric = metric</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录训练过程中的评估指标变化情况</span></span><br><span class="line">        self.train_scores = []</span><br><span class="line">        self.dev_scores = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录训练过程中的评价指标变化情况</span></span><br><span class="line">        self.train_loss = []</span><br><span class="line">        self.dev_loss = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, train_set, dev_set, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 传入训练轮数，如果没有传入值则默认为0</span></span><br><span class="line">        num_epochs = kwargs.get(<span class="string">&quot;num_epochs&quot;</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 传入log打印频率，如果没有传入值则默认为100</span></span><br><span class="line">        log_epochs = kwargs.get(<span class="string">&quot;log_epochs&quot;</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 传入模型保存路径</span></span><br><span class="line">        save_dir = kwargs.get(<span class="string">&quot;save_dir&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 记录全局最优指标</span></span><br><span class="line">        best_score = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 进行num_epochs轮训练</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            X, y = train_set</span><br><span class="line">            <span class="comment"># 获取模型预测</span></span><br><span class="line">            logits = self.model(X)</span><br><span class="line">            <span class="comment"># 计算交叉熵损失</span></span><br><span class="line">            trn_loss = self.loss_fn(logits, y) <span class="comment"># return a tensor</span></span><br><span class="line">            </span><br><span class="line">            self.train_loss.append(trn_loss.item())</span><br><span class="line">            <span class="comment"># 计算评估指标</span></span><br><span class="line">            trn_score = self.metric(logits, y).item()</span><br><span class="line">            self.train_scores.append(trn_score)</span><br><span class="line"></span><br><span class="line">            self.loss_fn.backward()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 参数更新</span></span><br><span class="line">            self.optimizer.step()</span><br><span class="line">           </span><br><span class="line">            dev_score, dev_loss = self.evaluate(dev_set)</span><br><span class="line">            <span class="comment"># 如果当前指标为最优指标，保存该模型</span></span><br><span class="line">            <span class="keyword">if</span> dev_score &gt; best_score:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;[Evaluate] best accuracy performence has been updated: <span class="subst">&#123;best_score:<span class="number">.5</span>f&#125;</span> --&gt; <span class="subst">&#123;dev_score:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">                best_score = dev_score</span><br><span class="line">                <span class="keyword">if</span> save_dir:</span><br><span class="line">                    self.save_model(save_dir)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> log_epochs <span class="keyword">and</span> epoch % log_epochs == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;[Train] epoch: <span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>, loss: <span class="subst">&#123;trn_loss.item()&#125;</span>&quot;</span>)</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">self, data_set</span>):</span><br><span class="line">        X, y = data_set</span><br><span class="line">        <span class="comment"># 计算模型输出</span></span><br><span class="line">        logits = self.model(X)</span><br><span class="line">        <span class="comment"># 计算损失函数</span></span><br><span class="line">        loss = self.loss_fn(logits, y).item()</span><br><span class="line">        self.dev_loss.append(loss)</span><br><span class="line">        <span class="comment"># 计算评估指标</span></span><br><span class="line">        score = self.metric(logits, y).item()</span><br><span class="line">        self.dev_scores.append(score)</span><br><span class="line">        <span class="keyword">return</span> score, loss</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.model(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_model</span>(<span class="params">self, save_dir</span>):</span><br><span class="line">        <span class="comment"># 对模型每层参数分别进行保存，保存文件名称与该层名称相同</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.model.layers: <span class="comment"># 遍历所有层</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer.params, <span class="built_in">dict</span>):</span><br><span class="line">                paddle.save(layer.params, os.path.join(save_dir, layer.name+<span class="string">&quot;.pdparams&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_model</span>(<span class="params">self, model_dir</span>):</span><br><span class="line">        <span class="comment"># 获取所有层参数名称和保存路径之间的对应关系</span></span><br><span class="line">        model_file_names = os.listdir(model_dir)</span><br><span class="line">        name_file_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> file_name <span class="keyword">in</span> model_file_names:</span><br><span class="line">            name = file_name.replace(<span class="string">&quot;.pdparams&quot;</span>,<span class="string">&quot;&quot;</span>)</span><br><span class="line">            name_file_dict[name] = os.path.join(model_dir, file_name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载每层参数</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.model.layers: <span class="comment"># 遍历所有层</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer.params, <span class="built_in">dict</span>):</span><br><span class="line">                name = layer.name</span><br><span class="line">                file_path = name_file_dict[name]</span><br><span class="line">                layer.params = paddle.load(file_path)</span><br></pre></td></tr></table></figure>
<h3 id="4-2-6-模型训练"><a href="#4-2-6-模型训练" class="headerlink" title="4.2.6 模型训练"></a>4.2.6 模型训练</h3><p>基于<code>RunnerV2_1</code>，使用训练集和验证集进行模型训练，共训练2000个epoch。评价指标为第章介绍的<code>accuracy</code>。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nndl <span class="keyword">import</span> accuracy</span><br><span class="line">paddle.seed(<span class="number">123</span>)</span><br><span class="line">epoch_num = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">model_saved_dir = <span class="string">&quot;model&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入层维度为2</span></span><br><span class="line">input_size = <span class="number">2</span></span><br><span class="line"><span class="comment"># 隐藏层维度为5</span></span><br><span class="line">hidden_size = <span class="number">5</span></span><br><span class="line"><span class="comment"># 输出层维度为1</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络</span></span><br><span class="line">model = Model_MLP_L2(input_size=input_size, hidden_size=hidden_size, output_size=output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = BinaryCrossEntropyLoss(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line">optimizer = BatchGD(learning_rate, model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价方法</span></span><br><span class="line">metric = accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化RunnerV2_1类，并传入训练配置</span></span><br><span class="line">runner = RunnerV2_1(model, optimizer, metric, loss_fn)</span><br><span class="line"></span><br><span class="line">runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=epoch_num, log_epochs=<span class="number">50</span>, save_dir=model_saved_dir)</span><br></pre></td></tr></table></figure>
<pre><code>[Evaluate] best accuracy performence has been updated: 0.00000 --&gt; 0.58750
[Train] epoch: 0/1000, loss: 0.7420312762260437
[Evaluate] best accuracy performence has been updated: 0.58750 --&gt; 0.59375
[Evaluate] best accuracy performence has been updated: 0.59375 --&gt; 0.60000
[Evaluate] best accuracy performence has been updated: 0.60000 --&gt; 0.60625
[Evaluate] best accuracy performence has been updated: 0.60625 --&gt; 0.61875
[Evaluate] best accuracy performence has been updated: 0.61875 --&gt; 0.62500
[Evaluate] best accuracy performence has been updated: 0.62500 --&gt; 0.63125
[Evaluate] best accuracy performence has been updated: 0.63125 --&gt; 0.63750
[Evaluate] best accuracy performence has been updated: 0.63750 --&gt; 0.64375
[Train] epoch: 50/1000, loss: 0.6469818353652954
[Evaluate] best accuracy performence has been updated: 0.64375 --&gt; 0.65000
[Evaluate] best accuracy performence has been updated: 0.65000 --&gt; 0.65625
[Evaluate] best accuracy performence has been updated: 0.65625 --&gt; 0.66250
[Evaluate] best accuracy performence has been updated: 0.66250 --&gt; 0.66875
[Evaluate] best accuracy performence has been updated: 0.66875 --&gt; 0.68125
[Train] epoch: 100/1000, loss: 0.6152153015136719
[Evaluate] best accuracy performence has been updated: 0.68125 --&gt; 0.68750
[Evaluate] best accuracy performence has been updated: 0.68750 --&gt; 0.69375
[Evaluate] best accuracy performence has been updated: 0.69375 --&gt; 0.70000
[Evaluate] best accuracy performence has been updated: 0.70000 --&gt; 0.70625
[Evaluate] best accuracy performence has been updated: 0.70625 --&gt; 0.71250
[Evaluate] best accuracy performence has been updated: 0.71250 --&gt; 0.71875
[Evaluate] best accuracy performence has been updated: 0.71875 --&gt; 0.72500
[Train] epoch: 150/1000, loss: 0.5901204943656921
[Evaluate] best accuracy performence has been updated: 0.72500 --&gt; 0.73125
[Evaluate] best accuracy performence has been updated: 0.73125 --&gt; 0.73750
[Evaluate] best accuracy performence has been updated: 0.73750 --&gt; 0.74375
[Train] epoch: 200/1000, loss: 0.5668107867240906
[Evaluate] best accuracy performence has been updated: 0.74375 --&gt; 0.75000
[Evaluate] best accuracy performence has been updated: 0.75000 --&gt; 0.75625
[Train] epoch: 250/1000, loss: 0.5443627238273621
[Evaluate] best accuracy performence has been updated: 0.75625 --&gt; 0.76250
[Evaluate] best accuracy performence has been updated: 0.76250 --&gt; 0.76875
[Evaluate] best accuracy performence has been updated: 0.76875 --&gt; 0.77500
[Train] epoch: 300/1000, loss: 0.5232617259025574
[Train] epoch: 350/1000, loss: 0.5044774413108826
[Evaluate] best accuracy performence has been updated: 0.77500 --&gt; 0.78125
[Evaluate] best accuracy performence has been updated: 0.78125 --&gt; 0.78750
[Train] epoch: 400/1000, loss: 0.4887065887451172
[Train] epoch: 450/1000, loss: 0.47630712389945984
[Train] epoch: 500/1000, loss: 0.46706733107566833
[Train] epoch: 550/1000, loss: 0.46052178740501404
[Train] epoch: 600/1000, loss: 0.4560360014438629
[Evaluate] best accuracy performence has been updated: 0.78750 --&gt; 0.79375
[Evaluate] best accuracy performence has been updated: 0.79375 --&gt; 0.80000
[Train] epoch: 650/1000, loss: 0.45301899313926697
[Train] epoch: 700/1000, loss: 0.4510025084018707
[Train] epoch: 750/1000, loss: 0.449656218290329
[Train] epoch: 800/1000, loss: 0.4487312436103821
[Train] epoch: 850/1000, loss: 0.4480809271335602
[Train] epoch: 900/1000, loss: 0.4476102888584137
[Train] epoch: 950/1000, loss: 0.44723382592201233
</code></pre><p>可视化观察训练集与验证集的损失函数变化情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印训练集和验证集的损失</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(<span class="built_in">range</span>(epoch_num), runner.train_loss, color=<span class="string">&quot;#8E004D&quot;</span>, label=<span class="string">&quot;Train loss&quot;</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(epoch_num), runner.dev_loss, color=<span class="string">&quot;#E20079&quot;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&quot;Dev loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;epoch&quot;</span>, fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">plt.legend(fontsize=<span class="string">&#x27;large&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;fw-loss2.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>​<br><img src="/img-nndl/output_40_0.png" alt="png"><br>​    </p>
<h3 id="4-2-7-性能评价"><a href="#4-2-7-性能评价" class="headerlink" title="4.2.7 性能评价"></a>4.2.7 性能评价</h3><p>使用测试集对训练中的最优模型进行评价，观察模型的评价指标。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载训练好的模型</span></span><br><span class="line">runner.load_model(model_saved_dir)</span><br><span class="line"><span class="comment"># 在测试集上对模型进行评价</span></span><br><span class="line">score, loss = runner.evaluate([X_test, y_test])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(score, loss))</span><br></pre></td></tr></table></figure>
<pre><code>[Test] score/loss: 0.8150/0.4193
</code></pre><p>从结果来看，模型在测试集上取得了较高的准确率。</p>
<p>下面对结果进行可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># 均匀生成40000个数据点</span></span><br><span class="line">x1, x2 = paddle.meshgrid(paddle.linspace(-math.pi, math.pi, <span class="number">200</span>), paddle.linspace(-math.pi, math.pi, <span class="number">200</span>))</span><br><span class="line">x = paddle.stack([paddle.flatten(x1), paddle.flatten(x2)], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测对应类别</span></span><br><span class="line">y = runner.predict(x)</span><br><span class="line">y = paddle.squeeze(paddle.cast((y&gt;=<span class="number">0.5</span>),dtype=<span class="string">&#x27;float32&#x27;</span>),axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制类别区域</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>].tolist(), x[:,<span class="number">1</span>].tolist(), c=y.tolist(), cmap=plt.cm.Spectral)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_train[:, <span class="number">0</span>].tolist(), X_train[:, <span class="number">1</span>].tolist(), marker=<span class="string">&#x27;*&#x27;</span>, c=paddle.squeeze(y_train,axis=-<span class="number">1</span>).tolist())</span><br><span class="line">plt.scatter(X_dev[:, <span class="number">0</span>].tolist(), X_dev[:, <span class="number">1</span>].tolist(), marker=<span class="string">&#x27;*&#x27;</span>, c=paddle.squeeze(y_dev,axis=-<span class="number">1</span>).tolist())</span><br><span class="line">plt.scatter(X_test[:, <span class="number">0</span>].tolist(), X_test[:, <span class="number">1</span>].tolist(), marker=<span class="string">&#x27;*&#x27;</span>, c=paddle.squeeze(y_test,axis=-<span class="number">1</span>).tolist())</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x7f5a84204710&gt;
</code></pre><p>​<br><img src="/img-nndl/output_45_1.png" alt="png"><br>​    </p>
<h2 id="4-3-自动梯度计算和预定义算子"><a href="#4-3-自动梯度计算和预定义算子" class="headerlink" title="4.3 自动梯度计算和预定义算子"></a>4.3 自动梯度计算和预定义算子</h2><p>虽然我们能够通过模块化的方式比较好地对神经网络进行组装，但是每个模块的梯度计算过程仍然十分繁琐且容易出错。在深度学习框架中，已经封装了自动梯度计算的功能，我们只需要聚焦模型架构，不再需要耗费精力进行计算梯度。</p>
<p>飞桨提供了<code>paddle.nn.Layer</code>类，来方便快速的实现自己的层和模型。模型和层都可以基于<code>paddle.nn.Layer</code>扩充实现，模型只是一种特殊的层。</p>
<p>继承了<code>paddle.nn.Layer</code>类的算子中，可以在内部直接调用其它继承<code>paddle.nn.Layer</code>类的算子，飞桨框架会自动识别算子中内嵌的<code>paddle.nn.Layer</code>类算子，并自动计算它们的梯度，并在优化时更新它们的参数。</p>
<h3 id="4-3-1-利用预定义算子重新实现前馈神经网络"><a href="#4-3-1-利用预定义算子重新实现前馈神经网络" class="headerlink" title="4.3.1 利用预定义算子重新实现前馈神经网络"></a>4.3.1 利用预定义算子重新实现前馈神经网络</h3><p>下面我们使用Paddle的预定义算子来重新实现二分类任务。<br>主要使用到的预定义算子为<code>paddle.nn.Linear</code>：</p>
<p><code>class paddle.nn.Linear(in_features, out_features, weight_attr=None, bias_attr=None, name=None)</code></p>
<p><code>paddle.nn.Linear</code>算子可以接受一个形状为[batch_size,∗,in_features]的<strong>输入张量</strong>，其中”∗”表示张量中可以有任意的其它额外维度，并计算它与形状为[in_features, out_features]的<strong>权重矩阵</strong>的乘积，然后生成形状为[batch_size,∗,out_features]的<strong>输出张量</strong>。 <code>paddle.nn.Linear</code>算子默认有偏置参数，可以通过<code>bias_attr=False</code>设置不带偏置。</p>
<hr>
<p>paddle.nn 目录下包含飞桨框架支持的神经网络层和相关函数的相关API。paddle.nn.functional下都是一些函数实现。</p>
<hr>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> paddle.nn.initializer <span class="keyword">import</span> Constant, Normal, Uniform</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model_MLP_L2_V2</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model_MLP_L2_V2, self).__init__()</span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.nn.Linear&#x27;定义线性层。</span></span><br><span class="line">        <span class="comment"># 其中第一个参数（in_features）为线性层输入维度；第二个参数（out_features）为线性层输出维度</span></span><br><span class="line">        <span class="comment"># weight_attr为权重参数属性，这里使用&#x27;paddle.nn.initializer.Normal&#x27;进行随机高斯分布初始化</span></span><br><span class="line">        <span class="comment"># bias_attr为偏置参数属性，这里使用&#x27;paddle.nn.initializer.Constant&#x27;进行常量初始化</span></span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden_size,</span><br><span class="line">                                weight_attr=paddle.ParamAttr(initializer=Normal(mean=<span class="number">0.</span>, std=<span class="number">1.</span>)),</span><br><span class="line">                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="number">0.0</span>)))</span><br><span class="line">        self.fc2 = nn.Linear(hidden_size, output_size,</span><br><span class="line">                                weight_attr=paddle.ParamAttr(initializer=Normal(mean=<span class="number">0.</span>, std=<span class="number">1.</span>)),</span><br><span class="line">                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="number">0.0</span>)))</span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.nn.functional.sigmoid&#x27;定义 Logistic 激活函数</span></span><br><span class="line">        self.act_fn = F.sigmoid</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        z1 = self.fc1(inputs)</span><br><span class="line">        a1 = self.act_fn(z1)</span><br><span class="line">        z2 = self.fc2(a1)</span><br><span class="line">        a2 = self.act_fn(z2)</span><br><span class="line">        <span class="keyword">return</span> a2</span><br></pre></td></tr></table></figure>
<h3 id="4-3-2-完善Runner类"><a href="#4-3-2-完善Runner类" class="headerlink" title="4.3.2 完善Runner类"></a>4.3.2 完善Runner类</h3><p>基于上一节实现的 <code>RunnerV2_1</code> 类，本节的 RunnerV2_2 类在训练过程中使用自动梯度计算；模型保存时，使用<code>state_dict</code>方法获取模型参数；模型加载时，使用<code>set_state_dict</code>方法加载模型参数.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RunnerV2_2</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, optimizer, metric, loss_fn, **kwargs</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.metric = metric</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录训练过程中的评估指标变化情况</span></span><br><span class="line">        self.train_scores = []</span><br><span class="line">        self.dev_scores = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录训练过程中的评价指标变化情况</span></span><br><span class="line">        self.train_loss = []</span><br><span class="line">        self.dev_loss = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, train_set, dev_set, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 将模型切换为训练模式</span></span><br><span class="line">        self.model.train()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 传入训练轮数，如果没有传入值则默认为0</span></span><br><span class="line">        num_epochs = kwargs.get(<span class="string">&quot;num_epochs&quot;</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 传入log打印频率，如果没有传入值则默认为100</span></span><br><span class="line">        log_epochs = kwargs.get(<span class="string">&quot;log_epochs&quot;</span>, <span class="number">100</span>)</span><br><span class="line">        <span class="comment"># 传入模型保存路径，如果没有传入值则默认为&quot;best_model.pdparams&quot;</span></span><br><span class="line">        save_path = kwargs.get(<span class="string">&quot;save_path&quot;</span>, <span class="string">&quot;best_model.pdparams&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># log打印函数，如果没有传入则默认为&quot;None&quot;</span></span><br><span class="line">        custom_print_log = kwargs.get(<span class="string">&quot;custom_print_log&quot;</span>, <span class="literal">None</span>) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 记录全局最优指标</span></span><br><span class="line">        best_score = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 进行num_epochs轮训练</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            X, y = train_set</span><br><span class="line">            <span class="comment"># 获取模型预测</span></span><br><span class="line">            logits = self.model(X)</span><br><span class="line">            <span class="comment"># 计算交叉熵损失</span></span><br><span class="line">            trn_loss = self.loss_fn(logits, y)</span><br><span class="line">            self.train_loss.append(trn_loss.item())</span><br><span class="line">            <span class="comment"># 计算评估指标</span></span><br><span class="line">            trn_score = self.metric(logits, y).item()</span><br><span class="line">            self.train_scores.append(trn_score)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 自动计算参数梯度</span></span><br><span class="line">            trn_loss.backward()</span><br><span class="line">            <span class="keyword">if</span> custom_print_log <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 打印每一层的梯度</span></span><br><span class="line">                custom_print_log(self)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 参数更新</span></span><br><span class="line">            self.optimizer.step()</span><br><span class="line">            <span class="comment"># 清空梯度</span></span><br><span class="line">            self.optimizer.clear_grad()</span><br><span class="line"></span><br><span class="line">            dev_score, dev_loss = self.evaluate(dev_set)</span><br><span class="line">            <span class="comment"># 如果当前指标为最优指标，保存该模型</span></span><br><span class="line">            <span class="keyword">if</span> dev_score &gt; best_score:</span><br><span class="line">                self.save_model(save_path)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;[Evaluate] best accuracy performence has been updated: <span class="subst">&#123;best_score:<span class="number">.5</span>f&#125;</span> --&gt; <span class="subst">&#123;dev_score:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">                best_score = dev_score</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> log_epochs <span class="keyword">and</span> epoch % log_epochs == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;[Train] epoch: <span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>, loss: <span class="subst">&#123;trn_loss.item()&#125;</span>&quot;</span>)</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># 模型评估阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span></span><br><span class="line"><span class="meta">    @paddle.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">self, data_set</span>):</span><br><span class="line">        <span class="comment"># 将模型切换为评估模式</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">        X, y = data_set</span><br><span class="line">        <span class="comment"># 计算模型输出</span></span><br><span class="line">        logits = self.model(X)</span><br><span class="line">        <span class="comment"># 计算损失函数</span></span><br><span class="line">        loss = self.loss_fn(logits, y).item()</span><br><span class="line">        self.dev_loss.append(loss)</span><br><span class="line">        <span class="comment"># 计算评估指标</span></span><br><span class="line">        score = self.metric(logits, y).item()</span><br><span class="line">        self.dev_scores.append(score)</span><br><span class="line">        <span class="keyword">return</span> score, loss</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型测试阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span></span><br><span class="line"><span class="meta">    @paddle.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 将模型切换为评估模式</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">return</span> self.model(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用&#x27;model.state_dict()&#x27;获取模型参数，并进行保存</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_model</span>(<span class="params">self, saved_path</span>):</span><br><span class="line">        paddle.save(self.model.state_dict(), saved_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用&#x27;model.set_state_dict&#x27;加载模型参数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_model</span>(<span class="params">self, model_path</span>):</span><br><span class="line">        state_dict = paddle.load(model_path)</span><br><span class="line">        self.model.set_state_dict(state_dict)</span><br></pre></td></tr></table></figure>
<h3 id="4-3-3-模型训练"><a href="#4-3-3-模型训练" class="headerlink" title="4.3.3 模型训练"></a>4.3.3 模型训练</h3><p>实例化RunnerV2类，并传入训练配置，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置模型</span></span><br><span class="line">input_size = <span class="number">2</span></span><br><span class="line">hidden_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line">model = Model_MLP_L2_V2(input_size=input_size, hidden_size=hidden_size, output_size=output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置损失函数</span></span><br><span class="line">loss_fn = F.binary_cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置优化器</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line">optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置评价指标</span></span><br><span class="line">metric = accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他参数</span></span><br><span class="line">epoch_num = <span class="number">1000</span></span><br><span class="line">saved_path = <span class="string">&#x27;best_model.pdparams&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化RunnerV2类，并传入训练配置</span></span><br><span class="line">runner = RunnerV2_2(model, optimizer, metric, loss_fn)</span><br><span class="line"></span><br><span class="line">runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=epoch_num, log_epochs=<span class="number">50</span>, save_path=<span class="string">&quot;best_model.pdparams&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[Evaluate] best accuracy performence has been updated: 0.00000 --&gt; 0.49375
[Train] epoch: 0/1000, loss: 0.7797883152961731
[Evaluate] best accuracy performence has been updated: 0.49375 --&gt; 0.50625
[Evaluate] best accuracy performence has been updated: 0.50625 --&gt; 0.53750
[Evaluate] best accuracy performence has been updated: 0.53750 --&gt; 0.55625
[Evaluate] best accuracy performence has been updated: 0.55625 --&gt; 0.57500
[Evaluate] best accuracy performence has been updated: 0.57500 --&gt; 0.60000
[Evaluate] best accuracy performence has been updated: 0.60000 --&gt; 0.63125
[Evaluate] best accuracy performence has been updated: 0.63125 --&gt; 0.66250
[Evaluate] best accuracy performence has been updated: 0.66250 --&gt; 0.68750
[Evaluate] best accuracy performence has been updated: 0.68750 --&gt; 0.72500
[Evaluate] best accuracy performence has been updated: 0.72500 --&gt; 0.73125
[Evaluate] best accuracy performence has been updated: 0.73125 --&gt; 0.73750
[Evaluate] best accuracy performence has been updated: 0.73750 --&gt; 0.75625
[Evaluate] best accuracy performence has been updated: 0.75625 --&gt; 0.76250
[Evaluate] best accuracy performence has been updated: 0.76250 --&gt; 0.76875
[Evaluate] best accuracy performence has been updated: 0.76875 --&gt; 0.77500
[Evaluate] best accuracy performence has been updated: 0.77500 --&gt; 0.78125
[Evaluate] best accuracy performence has been updated: 0.78125 --&gt; 0.78750
[Train] epoch: 50/1000, loss: 0.5038431286811829
[Train] epoch: 100/1000, loss: 0.4717639088630676
[Train] epoch: 150/1000, loss: 0.45760929584503174
[Evaluate] best accuracy performence has been updated: 0.78750 --&gt; 0.79375
[Train] epoch: 200/1000, loss: 0.45105433464050293
[Train] epoch: 250/1000, loss: 0.4478422999382019
[Evaluate] best accuracy performence has been updated: 0.79375 --&gt; 0.80000
[Train] epoch: 300/1000, loss: 0.44619712233543396
[Train] epoch: 350/1000, loss: 0.44532302021980286
[Train] epoch: 400/1000, loss: 0.4448436498641968
[Train] epoch: 450/1000, loss: 0.4445667862892151
[Train] epoch: 500/1000, loss: 0.44439566135406494
[Train] epoch: 550/1000, loss: 0.4442830979824066
[Train] epoch: 600/1000, loss: 0.4441978931427002
[Train] epoch: 650/1000, loss: 0.4441308379173279
[Train] epoch: 700/1000, loss: 0.4440736174583435
[Train] epoch: 750/1000, loss: 0.44402214884757996
[Train] epoch: 800/1000, loss: 0.443974107503891
[Train] epoch: 850/1000, loss: 0.44392937421798706
[Train] epoch: 900/1000, loss: 0.4438859522342682
[Train] epoch: 950/1000, loss: 0.44384509325027466
</code></pre><p>将训练过程中训练集与验证集的准确率变化情况进行可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化观察训练集与验证集的指标变化情况</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">runner, fig_name</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">    epochs = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(runner.train_scores))]</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, runner.train_loss, color=<span class="string">&#x27;#8E004D&#x27;</span>, label=<span class="string">&quot;Train loss&quot;</span>)</span><br><span class="line">    plt.plot(epochs, runner.dev_loss, color=<span class="string">&#x27;#E20079&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&quot;Dev loss&quot;</span>)</span><br><span class="line">    <span class="comment"># 绘制坐标轴和图例</span></span><br><span class="line">    plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>, fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>, fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, runner.train_scores, color=<span class="string">&#x27;#8E004D&#x27;</span>, label=<span class="string">&quot;Train accuracy&quot;</span>)</span><br><span class="line">    plt.plot(epochs, runner.dev_scores, color=<span class="string">&#x27;#E20079&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&quot;Dev accuracy&quot;</span>)</span><br><span class="line">    <span class="comment"># 绘制坐标轴和图例</span></span><br><span class="line">    plt.ylabel(<span class="string">&quot;score&quot;</span>, fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>, fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>, fontsize=<span class="string">&#x27;x-large&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    plt.savefig(fig_name)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot(runner, <span class="string">&#x27;fw-acc.pdf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>​<br><img src="/img-nndl/output_53_0.png" alt="png"><br>​    </p>
<h3 id="4-3-4-性能评价"><a href="#4-3-4-性能评价" class="headerlink" title="4.3.4 性能评价"></a>4.3.4 性能评价</h3><p>使用测试数据对训练完成后的最优模型进行评价，观察模型在测试集上的准确率以及loss情况。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line">runner.load_model(<span class="string">&quot;best_model.pdparams&quot;</span>)</span><br><span class="line">score, loss = runner.evaluate([X_test, y_test])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(score, loss))</span><br></pre></td></tr></table></figure>
<pre><code>[Test] score/loss: 0.8250/0.4122
</code></pre><p>从结果来看，模型在测试集上取得了较高的准确率。</p>
<h2 id="4-4-优化问题"><a href="#4-4-优化问题" class="headerlink" title="4.4 优化问题"></a>4.4 优化问题</h2><p>在本节中，我们通过实践来发现神经网络模型的优化问题，并思考如何改进。</p>
<h3 id="4-4-1-参数初始化"><a href="#4-4-1-参数初始化" class="headerlink" title="4.4.1 参数初始化"></a>4.4.1 参数初始化</h3><p>实现一个神经网络前，需要先初始化模型参数。如果对每一层的权重和偏置都用0初始化，那么通过第一遍前向计算，所有隐藏层神经元的激活值都相同；在反向传播时，所有权重的更新也都相同，这样会导致隐藏层神经元没有差异性，出现<strong>对称权重现象</strong>。</p>
<p>接下来，将模型参数全都初始化为0，看实验结果。这里重新定义了一个类<code>TwoLayerNet_Zeros</code>，两个线性层的参数全都初始化为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> paddle.nn.initializer <span class="keyword">import</span> Constant, Normal, Uniform</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model_MLP_L2_V4</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model_MLP_L2_V4, self).__init__()</span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.nn.Linear&#x27;定义线性层。</span></span><br><span class="line">        <span class="comment"># 其中in_features为线性层输入维度；out_features为线性层输出维度</span></span><br><span class="line">        <span class="comment"># weight_attr为权重参数属性</span></span><br><span class="line">        <span class="comment"># bias_attr为偏置参数属性</span></span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden_size,</span><br><span class="line">                                weight_attr=paddle.ParamAttr(initializer=Constant(value=<span class="number">0.0</span>)),</span><br><span class="line">                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="number">0.0</span>)))</span><br><span class="line">        self.fc2 = nn.Linear(hidden_size, output_size,</span><br><span class="line">                                weight_attr=paddle.ParamAttr(initializer=Constant(value=<span class="number">0.0</span>)),</span><br><span class="line">                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="number">0.0</span>)))</span><br><span class="line">        <span class="comment"># 使用&#x27;paddle.nn.functional.sigmoid&#x27;定义 Logistic 激活函数</span></span><br><span class="line">        self.act_fn = F.sigmoid</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        z1 = self.fc1(inputs)</span><br><span class="line">        a1 = self.act_fn(z1)</span><br><span class="line">        z2 = self.fc2(a1)</span><br><span class="line">        a2 = self.act_fn(z2)</span><br><span class="line">        <span class="keyword">return</span> a2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_weights</span>(<span class="params">runner</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;The weights of the Layers：&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> runner.model.sublayers():</span><br><span class="line">        <span class="built_in">print</span>(item.full_name())</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> item.parameters():</span><br><span class="line">            <span class="built_in">print</span>(param.numpy())</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<p>利用Runner类训练模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置模型</span></span><br><span class="line">input_size = <span class="number">2</span></span><br><span class="line">hidden_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line">model = Model_MLP_L2_V4(input_size=input_size, hidden_size=hidden_size, output_size=output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置损失函数</span></span><br><span class="line">loss_fn = F.binary_cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置优化器</span></span><br><span class="line">learning_rate = <span class="number">0.2</span> <span class="comment">#5e-2</span></span><br><span class="line">optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置评价指标</span></span><br><span class="line">metric = accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他参数</span></span><br><span class="line">epoch = <span class="number">2000</span></span><br><span class="line">saved_path = <span class="string">&#x27;best_model.pdparams&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化RunnerV2类，并传入训练配置</span></span><br><span class="line">runner = RunnerV2_2(model, optimizer, metric, loss_fn)</span><br><span class="line"></span><br><span class="line">runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="number">5</span>, log_epochs=<span class="number">50</span>, save_path=<span class="string">&quot;best_model.pdparams&quot;</span>,custom_print_log=print_weights)</span><br></pre></td></tr></table></figure>
<pre><code>The weights of the Layers：
linear_26
[[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0.]
linear_27
[[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
[0.]
[Evaluate] best accuracy performence has been updated: 0.00000 --&gt; 0.48750
[Train] epoch: 0/5, loss: 0.6931471824645996
The weights of the Layers：
linear_26
[[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]]
[0. 0. 0. 0. 0.]
linear_27
[[0.00124969]
 [0.00124969]
 [0.00124969]
 [0.00124969]
 [0.00124969]]
[0.0025]
The weights of the Layers：
linear_26
[[ 1.48579265e-05  1.48579265e-05  1.48579265e-05  1.48579265e-05
   1.48579265e-05]
 [-1.15139528e-05 -1.15139528e-05 -1.15139528e-05 -1.15139528e-05
  -1.15139528e-05]]
[6.950849e-07 6.950849e-07 6.950849e-07 6.950849e-07 6.950849e-07]
linear_27
[[0.00236244]
 [0.00236244]
 [0.00236244]
 [0.00236244]
 [0.00236244]]
[0.00471878]
The weights of the Layers：
linear_26
[[ 4.2887910e-05  4.2887910e-05  4.2887910e-05  4.2887910e-05
   4.2887910e-05]
 [-3.3327407e-05 -3.3327407e-05 -3.3327407e-05 -3.3327407e-05
  -3.3327407e-05]]
[1.8656712e-06 1.8656712e-06 1.8656712e-06 1.8656712e-06 1.8656712e-06]
linear_27
[[0.00335312]
 [0.00335312]
 [0.00335312]
 [0.00335312]
 [0.00335312]]
[0.00668755]
The weights of the Layers：
linear_26
[[ 8.256304e-05  8.256304e-05  8.256304e-05  8.256304e-05  8.256304e-05]
 [-6.431999e-05 -6.431999e-05 -6.431999e-05 -6.431999e-05 -6.431999e-05]]
[3.3219417e-06 3.3219417e-06 3.3219417e-06 3.3219417e-06 3.3219417e-06]
linear_27
[[0.00422173]
 [0.00422173]
 [0.00422173]
 [0.00422173]
 [0.00422173]]
[0.00843404]
</code></pre><p>可视化训练和验证集上的主准确率和loss变化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(runner, <span class="string">&quot;fw-zero.pdf&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>​<br><img src="/img-nndl/output_64_0.png" alt="png"><br>​    </p>
<p>从输出结果看，二分类准确率为50%左右，说明模型没有学到任何内容。训练和验证loss几乎没有怎么下降。</p>
<p>为了避免对称权重现象，可以使用高斯分布或均匀分布初始化神经网络的参数。</p>
<p>高斯分布和均匀分布采样的实现和可视化代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用&#x27;paddle.normal&#x27;实现高斯分布采样，其中&#x27;mean&#x27;为高斯分布的均值，&#x27;std&#x27;为高斯分布的标准差，&#x27;shape&#x27;为输出形状</span></span><br><span class="line">gausian_weights = paddle.normal(mean=<span class="number">0.0</span>, std=<span class="number">1.0</span>, shape=[<span class="number">10000</span>])</span><br><span class="line"><span class="comment"># 使用&#x27;paddle.uniform&#x27;实现在[min,max)范围内的均匀分布采样，其中&#x27;shape&#x27;为输出形状</span></span><br><span class="line">uniform_weights = paddle.uniform(shape=[<span class="number">10000</span>], <span class="built_in">min</span>=- <span class="number">1.0</span>, <span class="built_in">max</span>=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制两种参数分布</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Gausian Distribution&#x27;</span>)</span><br><span class="line">plt.hist(gausian_weights, bins=<span class="number">200</span>, density=<span class="literal">True</span>, color=<span class="string">&#x27;#E20079&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Uniform Distribution&#x27;</span>)</span><br><span class="line">plt.hist(uniform_weights, bins=<span class="number">200</span>, density=<span class="literal">True</span>, color=<span class="string">&#x27;#8E004D&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;fw-gausian-uniform.pdf&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>​<br><img src="/img-nndl/output_67_0.png" alt="png"><br>​    </p>
<h3 id="4-4-2-梯度消失问题"><a href="#4-4-2-梯度消失问题" class="headerlink" title="4.4.2 梯度消失问题"></a>4.4.2 梯度消失问题</h3><p>在神经网络的构建过程中，随着网络层数的增加，理论上网络的拟合能力也应该是越来越好的。但是随着网络变深，参数学习更加困难，容易出现梯度消失问题。</p>
<p>由于Sigmoid型函数的饱和性，饱和区的导数更接近于0，误差经过每一层传递都会不断衰减。当网络层数很深时，梯度就会不停衰减，甚至消失，使得整个网络很难训练，这就是所谓的梯度消失问题。<br>在深度神经网络中，减轻梯度消失问题的方法有很多种，一种简单有效的方式就是使用导数比较大的激活函数，如：ReLU。</p>
<p>下面通过一个简单的实验观察前馈神经网络的梯度消失现象和改进方法。</p>
<h4 id="4-4-2-1-模型构建"><a href="#4-4-2-1-模型构建" class="headerlink" title="4.4.2.1 模型构建"></a>4.4.2.1 模型构建</h4><p>定义一个前馈神经网络，包含4个隐藏层和1个输出层，通过传入的参数指定激活函数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义多层前馈神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model_MLP_L5</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, output_size, act=<span class="string">&#x27;sigmoid&#x27;</span>, w_init=Normal(<span class="params">mean=<span class="number">0.0</span>, std=<span class="number">0.01</span></span>), b_init=Constant(<span class="params">value=<span class="number">1.0</span></span>)</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model_MLP_L5, self).__init__()</span><br><span class="line">        self.fc1 = paddle.nn.Linear(input_size, <span class="number">3</span>)</span><br><span class="line">        self.fc2 = paddle.nn.Linear(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        self.fc3 = paddle.nn.Linear(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        self.fc4 = paddle.nn.Linear(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        self.fc5 = paddle.nn.Linear(<span class="number">3</span>, output_size)</span><br><span class="line">        <span class="comment"># 定义网络使用的激活函数</span></span><br><span class="line">        <span class="keyword">if</span> act == <span class="string">&#x27;sigmoid&#x27;</span>:</span><br><span class="line">            self.act = F.sigmoid</span><br><span class="line">        <span class="keyword">elif</span> act == <span class="string">&#x27;relu&#x27;</span>:</span><br><span class="line">            self.act = F.relu</span><br><span class="line">        <span class="keyword">elif</span> act == <span class="string">&#x27;lrelu&#x27;</span>:</span><br><span class="line">            self.act = F.leaky_relu</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Please enter sigmoid relu or lrelu!&quot;</span>)</span><br><span class="line">        <span class="comment"># 初始化线性层权重和偏置参数</span></span><br><span class="line">        self.init_weights(w_init, b_init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化线性层权重和偏置参数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">self, w_init, b_init</span>):</span><br><span class="line">        <span class="comment"># 使用&#x27;named_sublayers&#x27;遍历所有网络层</span></span><br><span class="line">        <span class="keyword">for</span> n, m <span class="keyword">in</span> self.named_sublayers():</span><br><span class="line">            <span class="comment"># 如果是线性层，则使用指定方式进行参数初始化</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                w_init(m.weight)</span><br><span class="line">                b_init(m.bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        outputs = self.fc1(inputs)</span><br><span class="line">        outputs = self.act(outputs)</span><br><span class="line">        outputs = self.fc2(outputs)</span><br><span class="line">        outputs = self.act(outputs)</span><br><span class="line">        outputs = self.fc3(outputs)</span><br><span class="line">        outputs = self.act(outputs)</span><br><span class="line">        outputs = self.fc4(outputs)</span><br><span class="line">        outputs = self.act(outputs)</span><br><span class="line">        outputs = self.fc5(outputs)</span><br><span class="line">        outputs = F.sigmoid(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h4 id="4-4-2-2-使用Sigmoid型函数进行训练"><a href="#4-4-2-2-使用Sigmoid型函数进行训练" class="headerlink" title="4.4.2.2 使用Sigmoid型函数进行训练"></a>4.4.2.2 使用Sigmoid型函数进行训练</h4><p>使用Sigmoid型函数作为激活函数，为了便于观察梯度消失现象，只进行一轮网络优化。代码实现如下：</p>
<p>定义梯度打印函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_grads</span>(<span class="params">runner</span>):</span><br><span class="line">    <span class="comment"># 打印每一层的权重的模</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;The gradient of the Layers：&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> runner.model.sublayers():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(item.parameters())==<span class="number">2</span>:</span><br><span class="line">            <span class="built_in">print</span>(item.full_name(), paddle.norm(item.parameters()[<span class="number">0</span>].grad, p=<span class="number">2.</span>).numpy()[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">102</span>)</span><br><span class="line"><span class="comment"># 学习率大小</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络，激活函数使用sigmoid</span></span><br><span class="line">model =  Model_MLP_L5(input_size=<span class="number">2</span>, output_size=<span class="number">1</span>, act=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数，使用交叉熵损失函数</span></span><br><span class="line">loss_fn = F.binary_cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义评价指标</span></span><br><span class="line">metric = accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定梯度打印函数</span></span><br><span class="line">custom_print_log=print_grads</span><br></pre></td></tr></table></figure>
<p>实例化RunnerV2_2类，并传入训练配置。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化Runner类</span></span><br><span class="line">runner = RunnerV2_2(model, optimizer, metric, loss_fn)</span><br></pre></td></tr></table></figure>
<p>模型训练，打印网络每层梯度值的$\ell_2$范数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动训练</span></span><br><span class="line">runner.train([X_train, y_train], [X_dev, y_dev], </span><br><span class="line">            num_epochs=<span class="number">1</span>, log_epochs=<span class="literal">None</span>, </span><br><span class="line">            save_path=<span class="string">&quot;best_model.pdparams&quot;</span>, </span><br><span class="line">            custom_print_log=custom_print_log)</span><br></pre></td></tr></table></figure>
<pre><code>The gradient of the Layers：
linear_28 1.7168893e-11
linear_29 4.0600314e-09
linear_30 2.6041837e-06
linear_31 0.0012919077
linear_32 0.2817244
[Evaluate] best accuracy performence has been updated: 0.00000 --&gt; 0.48750
</code></pre><p>观察实验结果可以发现，梯度经过每一个神经层的传递都会不断衰减，最终传递到第一个神经层时，梯度几乎完全消失。</p>
<h4 id="4-4-2-3-使用ReLU函数进行模型训练"><a href="#4-4-2-3-使用ReLU函数进行模型训练" class="headerlink" title="4.4.2.3 使用ReLU函数进行模型训练"></a>4.4.2.3 使用ReLU函数进行模型训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">paddle.seed(<span class="number">102</span>)</span><br><span class="line">lr = <span class="number">0.01</span>  <span class="comment"># 学习率大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络，激活函数使用relu</span></span><br><span class="line">model =  Model_MLP_L5(input_size=<span class="number">2</span>, output_size=<span class="number">1</span>, act=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="comment"># 定义损失函数，这里使用交叉熵损失函数</span></span><br><span class="line">loss_fn = F.binary_cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义评估指标</span></span><br><span class="line">metric = accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化Runner</span></span><br><span class="line">runner = RunnerV2_2(model, optimizer, metric, loss_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动训练</span></span><br><span class="line">runner.train([X_train, y_train], [X_dev, y_dev], </span><br><span class="line">            num_epochs=<span class="number">1</span>, log_epochs=<span class="literal">None</span>, </span><br><span class="line">            save_path=<span class="string">&quot;best_model.pdparams&quot;</span>, </span><br><span class="line">            custom_print_log=custom_print_log)</span><br></pre></td></tr></table></figure>
<pre><code>The gradient of the Layers：
linear_33 1.1487313e-08
linear_34 7.364981e-07
linear_35 9.2768496e-05
linear_36 0.00907571
linear_37 0.39227772
[Evaluate] best accuracy performence has been updated: 0.00000 --&gt; 0.48750
</code></pre><p><strong>图4.4</strong> 展示了使用不同激活函数时，网络每层梯度值的$\ell_2$范数情况。从结果可以看到，5层的全连接前馈神经网络使用Sigmoid型函数作为激活函数时，梯度经过每一个神经层的传递都会不断衰减，最终传递到第一个神经层时，梯度几乎完全消失。改为ReLU激活函数后，梯度消失现象得到了缓解，每一层的参数都具有梯度值。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/d1fe3501bdbe47c2be309151c4aa2cf8419b561a64184e468e89c7d3de4f480d" width="600"  ></center>
<center><br>图4.4：网络每层梯度的L2范数变化趋势</br></center>

<p><br></br> </p>
<h3 id="4-4-3-死亡-ReLU-问题"><a href="#4-4-3-死亡-ReLU-问题" class="headerlink" title="4.4.3  死亡 ReLU 问题"></a>4.4.3  死亡 ReLU 问题</h3><p>ReLU激活函数可以一定程度上改善梯度消失问题，但是ReLU函数在某些情况下容易出现死亡 ReLU问题，使得网络难以训练。这是由于当$x&lt;0$时，ReLU函数的输出恒为0。在训练过程中，如果参数在一次不恰当的更新后，某个ReLU神经元在所有训练数据上都不能被激活（即输出为0），那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远都不能被激活。而一种简单有效的优化方式就是将激活函数更换为Leaky ReLU、ELU等ReLU的变种。</p>
<h4 id="4-4-3-1-使用ReLU进行模型训练"><a href="#4-4-3-1-使用ReLU进行模型训练" class="headerlink" title="4.4.3.1 使用ReLU进行模型训练"></a>4.4.3.1 使用ReLU进行模型训练</h4><p>使用第4.4.2节中定义的多层全连接前馈网络进行实验，使用ReLU作为激活函数，观察死亡ReLU现象和优化方法。当神经层的偏置被初始化为一个相对于权重较大的负值时，可以想像，输入经过神经层的处理，最终的输出会为负值，从而导致死亡ReLU现象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义网络，并使用较大的负值来初始化偏置</span></span><br><span class="line">model =  Model_MLP_L5(input_size=<span class="number">2</span>, output_size=<span class="number">1</span>, act=<span class="string">&#x27;relu&#x27;</span>, b_init=Constant(value=-<span class="number">8.0</span>))</span><br></pre></td></tr></table></figure>
<p>实例化RunnerV2类，启动模型训练，打印网络每层梯度值的$\ell_2$范数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化Runner类</span></span><br><span class="line">runner = RunnerV2_2(model, optimizer, metric, loss_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动训练</span></span><br><span class="line">runner.train([X_train, y_train], [X_dev, y_dev], </span><br><span class="line">            num_epochs=<span class="number">1</span>, log_epochs=<span class="number">0</span>, </span><br><span class="line">            save_path=<span class="string">&quot;best_model.pdparams&quot;</span>, </span><br><span class="line">            custom_print_log=custom_print_log)</span><br></pre></td></tr></table></figure>
<pre><code>The gradient of the Layers：
linear_38 0.0
linear_39 0.0
linear_40 0.0
linear_41 0.0
linear_42 0.0
[Evaluate] best accuracy performence has been updated: 0.00000 --&gt; 0.51250
</code></pre><p>从输出结果可以发现，使用 ReLU 作为激活函数，当满足条件时，会发生死亡ReLU问题，网络训练过程中 ReLU 神经元的梯度始终为0，参数无法更新。</p>
<p>针对死亡ReLU问题，一种简单有效的优化方式就是将激活函数更换为Leaky ReLU、ELU等ReLU 的变种。接下来，观察将激活函数更换为 Leaky ReLU时的梯度情况。</p>
<h4 id="4-4-3-2-使用Leaky-ReLU进行模型训练"><a href="#4-4-3-2-使用Leaky-ReLU进行模型训练" class="headerlink" title="4.4.3.2 使用Leaky ReLU进行模型训练"></a>4.4.3.2 使用Leaky ReLU进行模型训练</h4><p>将激活函数更换为Leaky ReLU进行模型训练，观察梯度情况。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新定义网络，使用Leaky ReLU激活函数</span></span><br><span class="line">model =  Model_MLP_L5(input_size=<span class="number">2</span>, output_size=<span class="number">1</span>, act=<span class="string">&#x27;lrelu&#x27;</span>, b_init=Constant(value=-<span class="number">8.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化Runner类</span></span><br><span class="line">runner = RunnerV2_2(model, optimizer, metric, loss_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动训练</span></span><br><span class="line">runner.train([X_train, y_train], [X_dev, y_dev], </span><br><span class="line">            num_epochs=<span class="number">1</span>, log_epochps=<span class="literal">None</span>, </span><br><span class="line">            save_path=<span class="string">&quot;best_model.pdparams&quot;</span>, </span><br><span class="line">            custom_print_log=custom_print_log)</span><br></pre></td></tr></table></figure>
<pre><code>The gradient of the Layers：
linear_43 7.37096e-17
linear_44 8.791596e-14
linear_45 5.3190563e-10
linear_46 1.9863977e-05
linear_47 0.07098922
[Evaluate] best accuracy performence has been updated: 0.00000 --&gt; 0.51250
[Train] epoch: 0/1, loss: 4.099949836730957
</code></pre><p>从输出结果可以看到，将激活函数更换为Leaky ReLU后，死亡ReLU问题得到了改善，梯度恢复正常，参数也可以正常更新。但是由于 Leaky ReLU 中，$\mathcal{x&lt;0}$ 时的斜率默认只有0.01，所以反向传播时，随着网络层数的加深，梯度值越来越小。如果想要改善这一现象，将 Leaky ReLU 中，$\mathcal{x&lt;0}$ 时的斜率调大即可。</p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/08/12/nndl/chapter4B/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          第4章（下）：基于前馈神经网络完成鸢尾花分类任务
        
      </div>
    </a>
  
  
    <a href="/2022/08/12/nndl/chapter3B/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">第3章（下）：基于Softmax回归完成鸢尾花分类任务</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2022 宝可梦训练师
      </div>
        <div class="footer-right">
          <a href="https://space.bilibili.com/782159" target="_blank">访问我的哔哩哔哩</a> 
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: ,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>